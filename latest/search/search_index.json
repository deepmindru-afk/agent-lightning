{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Agent Lightning","text":"<p>Agent Lightning is the absolute trainer to light up AI agents.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Turn your agent into an optimizable beast with ZERO CODE CHANGE (almost)! \ud83d\udca4</li> <li>Build with ANY agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, ...); or even WITHOUT agent framework (Python OpenAI). You name it! \ud83e\udd16</li> <li>Selectively optimize one or more agents in a multi-agent system. \ud83c\udfaf</li> <li>Embraces Reinforcement Learning, Automatic Prompt Optimization and more algorithms. \ud83e\udd17</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation - Get started with Agent Lightning</li> <li>Quickstart - Learn the fundamentals of Agent Lightning</li> <li>Train SQL Agent with RL - A practical example of training a SQL agent</li> <li>API Reference - Complete API documentation</li> <li>Join our Discord community - Connect with other users and contributors</li> </ul>"},{"location":"#resources","title":"Resources","text":"<ul> <li>8/11/2025 Training AI Agents to Write and Self-correct SQL with Reinforcement Learning Medium.</li> <li>8/5/2025 Agent Lightning: Train ANY AI Agents with Reinforcement Learning arXiv paper.</li> <li>7/26/2025 We discovered an approach to train any AI agent with RL, with (almost) zero code changes. Reddit.</li> <li>6/6/2025 Agent Lightning - Microsoft Research Project page.</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you find Agent Lightning useful in your research or projects, please cite our paper:</p> <pre><code>@misc{luo2025agentlightningtrainai,\n      title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning},\n      author={Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang},\n      year={2025},\n      eprint={2508.03680},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2508.03680},\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>See the LICENSE file for details.</p>"},{"location":"deep-dive/server-client-architecture/","title":"Server-client Architecture","text":"<p>Article to be written.</p> <pre><code>sequenceDiagram\n    participant RL as RL Framework\n    participant TS as Training Server\n    participant AC as Agent Client\n    participant AG as Agent\n\n    AC-&gt;&gt;TS: Upload Dataset (1)\n    RL-&gt;&gt;TS: Start RL Server (2)\n    TS-&gt;&gt;RL: Latest Model (3)\n\n    loop for each batch of tasks\n        loop for each task in the batch\n            AC-&gt;&gt;TS: Request Task (4)\n            TS-&gt;&gt;AC: Send Task &amp; Model API (5)\n            AC-&gt;&gt;AG: Run Agent with Task &amp; Model API (6)\n            loop for each LLM call\n                AC-&gt;&gt;AG: Prompt (7)\n                AG-&gt;&gt;AC: Response (8)\n            end\n            AG-&gt;&gt;AC: Rewarded Trace (9)\n            AC-&gt;&gt;TS: Send Rewarded Trace (10)\n        end\n        TS-&gt;&gt;RL: Send Batch of Traces (11)\n        RL-&gt;&gt;TS: Return Updated Model (12)\n    end</code></pre>"},{"location":"how-to/train-sql-agent/","title":"SQL Agent with Agent Lightning","text":"<p>This tutorial is tested with <code>verl==0.5.0</code> and <code>vllm==0.10.0</code>.</p> <p>This example demonstrates how to build and train a self-correcting SQL agent. It leverages Agent Lightning and the <code>verl</code> framework for Reinforcement Learning (RL) based training, and LangGraph to define the agent's complex, cyclical reasoning workflow. The goal is to fine-tune a Large Language Model (LLM) to accurately convert natural language questions into executable SQL queries.</p>"},{"location":"how-to/train-sql-agent/#sql-agent-implementation","title":"SQL Agent Implementation","text":"<p>The design of Agent-lightning allows flexible integration with various agent frameworks, including AutoGen, CrewAI, OpenAI Agent SDK, LangGraph, and more. It can also work without agent frameworks, allowing you to train an agent built from scratch with Python code. See our example gallery for more details.</p> <p>The core of the agent is a state machine built with LangGraph, which allows for a robust and transparent workflow. The agent's logic, as visualized below, starts by writing a query, executes it, and then enters a refinement loop where it checks and rewrites the query until it is deemed correct or a turn limit is reached.</p> <pre><code>---\nconfig:\n  flowchart:\n    curve: linear\n---\ngraph LR;\n        __start__([&lt;p&gt;__start__&lt;/p&gt;]):::first\n        write_query(write_query)\n        execute_query(execute_query)\n        check_query(check_query)\n        rewrite_query(rewrite_query)\n        __end__([&lt;p&gt;__end__&lt;/p&gt;]):::last\n        __start__ --&gt; write_query;\n        check_query -.-&gt; __end__;\n        check_query -.-&gt; rewrite_query;\n        execute_query --&gt; check_query;\n        rewrite_query --&gt; execute_query;\n        write_query --&gt; execute_query;\n        classDef default fill:#f2f2f2,line-height:1.2\n        classDef first fill-opacity:0\n        classDef last fill:#cccccc</code></pre> <p>This workflow is implemented in the <code>SQLAgent</code> class within <code>sql_agent.py</code>. It consists of the following key steps:</p> <ol> <li>write_query: Given a user's question and database schema, the agent makes an initial attempt to write a SQL query.</li> <li>execute_query: The generated query is run against the target database.</li> <li>check_query: The agent analyzes the original query and its execution result (or error) to check for mistakes. It uses a specific prompt (<code>CHECK_QUERY_PROMPT</code>) to determine if the query is correct.</li> <li>rewrite_query: If the <code>check_query</code> step finds errors, the agent enters this step. It uses the feedback from the previous step to generate a corrected SQL query. The process then loops back to <code>check_query</code> for re-evaluation.</li> <li>END: The loop terminates when <code>check_query</code> confirms the query is correct or the maximum number of turns (<code>max_turns</code>) is exceeded. One turn corresponds to a complete cycle of <code>write_query</code> (if first round), <code>execute_query</code>, <code>check_query</code>, and potentially <code>rewrite_query</code>.</li> </ol> <p>We aim to train write_query and rewrite_query step in the setup of this example. The check_query step is not trained but will share the same LLM weights as the other steps.</p>"},{"location":"how-to/train-sql-agent/#client-server-training-with-agent-lightning","title":"Client-Server Training with Agent Lightning","text":"<p>The training process uses a distributed client-server architecture designed by Agent Lightning to efficiently fine-tune the underlying LLM. This separation allows for scalable data generation across multiple clients while centralizing the computationally intensive model training on a dedicated server with GPUs, and also provides opportunities for customizing algorithms and training strategies (like prompt optimization) with minimal code changes.</p> <ul> <li>Training Server (<code>agentlightning.verl</code>): The server, launched with the first command below, manages the core training loop. It runs an RL algorithm (with <code>verl</code> of course) and hosts an OpenAI-compatible LLM endpoint (with <code>verl</code>'s async server). The server's sole purpose is to receive interaction data from clients and update the LLM's weights to improve its performance. This link points to the implementation of the server, which is built upon <code>verl</code>.</li> <li>Agent Clients (<code>sql_agent.py</code>): The clients run the LangGraph agent logic described above. They connect to the server to fetch tasks (natural language questions) and use the server's OpenAI-compatible endpoint for all generation steps (<code>write_query</code>, <code>check_query</code>, <code>rewrite_query</code>). After completing a task, the client exports its interaction traces (traced by AgentOps and filtered by trace hierarchy), evaluates its correctness to calculate a reward, and sends the entire interaction history (the \"trajectory\") back to the server for training. To adapt any agent to an \"agent client\", you do not need to change the agent logic, but only need to invoke the client's <code>run</code> method with <code>agentlightning.trainer</code>.</li> </ul> <p></p>"},{"location":"how-to/train-sql-agent/#running-the-example","title":"Running the Example","text":"<ol> <li> <p>Prepare the dataset: download from here and unzip it to the <code>data</code> folder. It's basically a Spider V1 dataset converted to Parquet format. The dataset contains about 8000 training samples and about 2000 test samples, from which we sampled 500 samples for evaluation.    <pre><code>pip install gdown\ngdown --fuzzy https://drive.google.com/file/d/1oi9J1jZP9TyM35L85CL3qeGWl2jqlnL6/view\nunzip -q spider-data.zip -d data\nrm spider-data.zip\n</code></pre></p> </li> <li> <p>Install the required dependencies:    <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Launch the training server:    <code>bash    python -m agentlightning.verl \\        agentlightning.port=9997 \\        algorithm.adv_estimator=grpo \\        data.train_files=data/train_spider.parquet \\        data.val_files=data/test_dev_500.parquet \\        actor_rollout_ref.rollout.tensor_model_parallel_size=1 \\        trainer.n_gpus_per_node=1 \\        data.train_batch_size=32 \\        actor_rollout_ref.rollout.n=4 \\        actor_rollout_ref.actor.ppo_mini_batch_size=32 \\        actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \\        actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=4 \\        actor_rollout_ref.rollout.multi_turn.format=hermes \\        actor_rollout_ref.model.path=meta-llama/Llama-3.2-3B-Instruct \\        data.max_prompt_length=4096 \\        data.max_response_length=2048 \\        data.truncation='error' \\        trainer.val_before_train=True \\        actor_rollout_ref.actor.optim.lr=1e-6 \\        actor_rollout_ref.model.use_remove_padding=True \\        actor_rollout_ref.actor.use_kl_loss=False \\        actor_rollout_ref.actor.kl_loss_coef=0.000 \\        actor_rollout_ref.actor.entropy_coeff=0 \\        actor_rollout_ref.actor.clip_ratio_low=0.2 \\        actor_rollout_ref.actor.clip_ratio_high=0.3 \\        actor_rollout_ref.model.enable_gradient_checkpointing=True \\        actor_rollout_ref.actor.fsdp_config.param_offload=True \\        actor_rollout_ref.actor.fsdp_config.optimizer_offload=True \\        actor_rollout_ref.rollout.name=vllm \\        actor_rollout_ref.rollout.gpu_memory_utilization=0.8 \\        actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=8 \\        actor_rollout_ref.ref.fsdp_config.param_offload=True \\        algorithm.use_kl_in_reward=False \\        trainer.critic_warmup=0 \\        trainer.logger=['console','wandb'] \\        trainer.project_name=AgentLightning \\        trainer.experiment_name=train_sql_agent \\        trainer.nnodes=1 \\        trainer.save_freq=256 \\        trainer.test_freq=32 \\        trainer.total_epochs=2</code></p> </li> <li> <p>Launch agent clients that connect with the server:    <pre><code>export VERL_API_BASE=http://localhost:9997/  # Same as the server port. This is used for receiving tasks and sending results.\npython sql_agent.py \\\n    --litsqlagent.trained-agents write \\  # Will only train the write and rewrite agent.\n    --trainer.n-workers 16 \\\n    --litsqlagent.val-temperature 0\n</code></pre></p> </li> </ol> <p>There is no hard requirement in the launching order of the server and clients. But remember to kill the long-running agent clients after the training is done.</p>"},{"location":"how-to/train-sql-agent/#debug-the-agent-without-verl","title":"Debug the Agent without verl","text":"<p>You can run the agent client alone without the <code>verl</code> server. This is useful for debugging the agent logic and SQL execution.</p> <ol> <li> <p>Copy <code>.env.example</code> to <code>.env</code> and fill in your OpenAI API key. <code>VERL_API_BASE</code> does not really matter here because you are not connecting to the server end.</p> </li> <li> <p>Run the agent client:    <pre><code>dotenv run python sql_agent.py \\\n    --litsqlagent.trained-agents write \\  # Will only select the trajectories related to write and rewrite.\n    --trainer.n-workers 1 \\  # For debug, use single process.\n    --trainer.dev true  # Enable the dev debug mode.\n</code></pre></p> </li> </ol>"},{"location":"how-to/train-sql-agent/#evaluation","title":"Evaluation","text":"<p>The example is evaluated using Llama-3.2-Instruct models. The models are trained on the Spider dataset for 2 epochs, with evaluation performed on a randomly selected subset of 500 test samples to compute held-out accuracy. The default setup for running agent clients during evaluation is as follows:</p> <pre><code>python sql_agent.py \\\n   --litsqlagent.trained-agents write \\\n   --trainer.n-workers 16 \\\n   --trainer.daemon true \\\n   --litsqlagent.val-temperature 0 \\\n   --litsqlagent.max-turns 3 \\\n   --litsqlagent.table-info-truncate 2048 \\\n   --litsqlagent.execution-truncate 2048\n</code></pre> <p>The setup of training server is the same as the command above.</p>"},{"location":"how-to/train-sql-agent/#wb-report","title":"W&amp;B Report","text":"<p>link</p>"},{"location":"how-to/train-sql-agent/#performance-metrics","title":"Performance Metrics","text":"Model Size Context Max Turns Agents Acc (Initial) Acc (Final) Transitions Prompt Length Response Length Llama3.2 1B 2048 3 write|rewrite 21 49.6 2.87 \u2192 3.08 821.2 319.2 \u2192 249.4 Llama3.2 3B 2048 3 write|rewrite 51.8 66.4 2.20 \u2192 2.72 865.6 116.2 \u2192 314.3 <p>Notes:</p> <ol> <li>Context Length: Controlled via <code>--litsqlagent.table-info-truncate &lt;context-length&gt;</code> and <code>--litsqlagent.execution-truncate &lt;context-length&gt;</code></li> <li>Max Turns: Set using <code>--litsqlagent.max-turns &lt;max-turns&gt;</code></li> <li>Agents: Specified with <code>--litsqlagent.agents &lt;regex&gt;</code> (defaults to <code>write</code>, which matches both write and rewrite agents)</li> <li>Transitions: Represents the number of prompt-response pairs traced (collected) during each rollout. Note that this differs from the turn count in the SQL agent workflow, where one turn may encompass 2-3 transitions in the check-rewrite cycle. The number of transitions is also related to which agents get involved in the training.</li> <li>Prompt/Response Length: Average token count per traced prompt/transition response.</li> </ol>"},{"location":"how-to/train-sql-agent/#efficiency-metrics","title":"Efficiency Metrics","text":"Model Size Context Max Turns Agents # GPUs # Steps Time (h) Time/Step (s) Rollout Time (%) Update Actor Time (%) Llama3.2 1B 2048 3 write|rewrite 1 436 13.06 98.9 66.7 25.2 Llama3.2 3B 2048 3 write|rewrite 2 436 10.3 181.3 63.9 27.9"},{"location":"quickstart/getting-started/","title":"Getting Started","text":"<p>This guide walks you through building your first Agent Lightning application - a simple prompt optimization system that finds the best system prompt for an AI agent.</p>"},{"location":"quickstart/getting-started/#what-youll-build","title":"What You'll Build","text":"<p>You'll create a distributed training system with a server that manages optimization algorithms and tasks, a client with multiple workers that execute tasks in parallel, and built-in telemetry for monitoring and debugging.</p> <p>Before starting, ensure you have Python 3.10 or later, Agent Lightning installed (<code>pip install agentlightning</code>), and an OpenAI API key. The complete code is available in the examples/apo directory.</p>"},{"location":"quickstart/getting-started/#part-1-building-your-agent","title":"Part 1: Building Your Agent","text":"<p>Let's start by creating a simple agent that can answer questions using OpenAI's API. Your agent needs to inherit from <code>LitAgent</code> and implement a <code>training_rollout</code> method.</p>"},{"location":"quickstart/getting-started/#step-1-create-your-agent-class","title":"Step 1: Create Your Agent Class","text":"<p>First, import the necessary dependencies and create your agent class:</p> <pre><code>from agentlightning.litagent import LitAgent\n\nclass SimpleAgent(LitAgent):\n    def training_rollout(self, task, rollout_id, resources):\n        \"\"\"Execute a single training rollout.\"\"\"\n</code></pre> <p>The <code>training_rollout</code> method is the heart of your agent. It receives three parameters: a <code>task</code> dictionary containing the work to do (like \"What is the capital of France?\"), a unique <code>rollout_id</code> for tracking this execution, and <code>resources</code> from the server - in our case, the system prompt we're testing.</p>"},{"location":"quickstart/getting-started/#step-2-execute-the-task","title":"Step 2: Execute the Task","text":"<p>Inside the training_rollout method, extract the system prompt from resources and use it to complete the task:</p> <pre><code># Extract the system prompt being tested\nsystem_prompt = resources[\"system_prompt\"].template\n\n# Call OpenAI with this prompt\nresult = openai.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": task[\"prompt\"]},\n    ],\n)\n</code></pre> <p>The server sends different prompts to test, and your agent uses each one to answer the same question. This lets us compare which prompt works best.</p>"},{"location":"quickstart/getting-started/#step-3-return-a-reward-score","title":"Step 3: Return a Reward Score","text":"<p>After executing the task, return a reward score between 0 and 1:</p> <pre><code># In real scenarios, calculate based on response quality\nreturn random.uniform(0, 1)\n</code></pre> <p>Higher rewards mean better performance. In a real system, you'd calculate this with rules, or even an LLM as a judge. For now, we're using random values to demonstrate the flow.</p>"},{"location":"quickstart/getting-started/#step-4-set-up-the-trainer","title":"Step 4: Set Up the Trainer","text":"<p>To run your agent with multiple workers in parallel:</p> <pre><code>from agentlightning.trainer import Trainer\n\nagent = SimpleAgent()\ntrainer = Trainer(n_workers=2)  # Create 2 parallel workers\ntrainer.fit(agent, \"http://127.0.0.1:9997\")\n</code></pre> <p>The trainer creates separate processes for each worker, allowing them to execute tasks independently. This parallelization significantly speeds up the optimization process - with 2 workers, you can test prompts twice as fast.</p>"},{"location":"quickstart/getting-started/#part-2-building-the-optimization-server","title":"Part 2: Building the Optimization Server","text":"<p>The server coordinates the training process and implements your optimization algorithm. It manages resources, distributes tasks, and collects results.</p>"},{"location":"quickstart/getting-started/#step-1-initialize-the-server","title":"Step 1: Initialize the Server","text":"<p>Create an async function to run your optimization:</p> <pre><code>import asyncio\nfrom agentlightning.server import AgentLightningServer\nfrom agentlightning.types import PromptTemplate\n\nasync def prompt_optimization():\n    server = AgentLightningServer(host=\"127.0.0.1\", port=9997)\n    await server.start()\n</code></pre> <p>We use async/await because the server handles multiple clients simultaneously. This allows it to queue tasks without blocking and process results as they arrive from different workers.</p>"},{"location":"quickstart/getting-started/#step-2-test-different-prompts","title":"Step 2: Test Different Prompts","text":"<p>Define the prompts you want to test and iterate through them:</p> <pre><code>prompt_candidates = [\n    \"You are a helpful assistant.\",\n    \"You are a knowledgeable AI.\",\n    \"You are a friendly chatbot.\",\n]\n\nfor prompt in prompt_candidates:\n    # Send this prompt to all connected clients\n    resources = {\n        \"system_prompt\": PromptTemplate(template=prompt, engine=\"f-string\")\n    }\n    await server.update_resources(resources)\n</code></pre> <p>When you update resources, all connected clients immediately receive the new system prompt. The format of resources can be arbitrary. We use the key <code>\"system_prompt\"</code> here as an example. The resources here are exactly you would expect at the client side, who will use this prompt for the next task they process.</p>"},{"location":"quickstart/getting-started/#step-3-queue-tasks-and-collect-results","title":"Step 3: Queue Tasks and Collect Results","text":"<p>For each prompt, queue a task and wait for results. The <code>{\"prompt\": ...}</code> format here is exact what you would expect from the client side code.</p> <pre><code># Queue a task for clients to process\ntask_id = await server.queue_task(\n    sample={\"prompt\": \"What is the capital of France?\"},\n    mode=\"train\"\n)\n\n# Wait for a client to complete it (30 second timeout)\nrollout = await server.poll_completed_rollout(task_id, timeout=30)\n\n# Extract and store the reward (this comes from the return value of the client side)\nreward = rollout.final_reward\n</code></pre> <p>The server queues the same question for each prompt. The rollout object contains not just the reward, but also detailed telemetry and trace information for debugging and optimization.</p>"},{"location":"quickstart/getting-started/#step-4-find-the-best-prompt","title":"Step 4: Find the Best Prompt","text":"<p>After testing all candidates, identify the winner:</p> <pre><code>best_prompt = max(prompt_and_rewards, key=lambda x: x[1])\nprint(f\"Best prompt: '{best_prompt[0]}' with reward {best_prompt[1]:.3f}\")\n</code></pre>"},{"location":"quickstart/getting-started/#running-your-system","title":"Running Your System","text":"<p>The Complete example code can be found on the GitHub repository. To run it:</p> <p>Create a <code>.env</code> file with your API credentials:</p> <pre><code>OPENAI_API_KEY=your-api-key-here\nOPENAI_API_BASE=https://api.openai.com/v1  # Optional\n</code></pre> <p>Start the server first in one terminal: <pre><code>python server.py\n</code></pre></p> <p>Then start the client in another terminal: <pre><code>python client.py\n</code></pre></p>"},{"location":"quickstart/getting-started/#understanding-the-output","title":"Understanding the Output","text":"<p>When you run the system, you'll see detailed logs from both the client and server. Understanding these logs helps you debug issues and optimize performance.</p>"},{"location":"quickstart/getting-started/#client-output-explained","title":"Client Output Explained","text":"<p><pre><code>2025-08-10 12:59:38,224 [INFO] Initializing Trainer...\n</code></pre> The trainer is starting up and preparing to create workers.</p> <p><pre><code>[INFO] Starting AgentOps local server on port 52081...\n</code></pre> A local telemetry server starts to collect metrics and traces. You can access this at <code>http://localhost:52081</code> to see detailed execution traces.</p> <p><pre><code>[INFO] Starting worker process 0...\n[INFO] Starting worker process 1...\n</code></pre> Two separate processes are created. Each can execute tasks independently, doubling your throughput.</p> <p><pre><code>[INFO] [Task 1 Received] ID: rollout-c1eb987b...\nResources: {'system_prompt': PromptTemplate(...)}\n</code></pre> A worker receives a task from the server along with the current prompt to test.</p> <p><pre><code>[INFO] [Worker 0 | Rollout] Completed in 1.09s. Reward: 0.631\n</code></pre> Worker 0 finished executing the task in 1.09 seconds and calculated a reward of 0.631. This tells you both performance (execution time) and quality (reward score).</p>"},{"location":"quickstart/getting-started/#server-output-explained","title":"Server Output Explained","text":"<p><pre><code>[Algo] Testing prompt: 'You are a helpful assistant.'\n</code></pre> The optimization algorithm selects the next prompt to test.</p> <p><pre><code>[Algo] Task 'rollout-c1eb987b...' is now available for clients.\n</code></pre> The task is queued and waiting for an available worker to pick it up.</p> <p><pre><code>[Algo] Received reward: 0.631\n</code></pre> A client completed the task and returned a performance score. The server uses this to compare prompts.</p> <p><pre><code>[Algo] Best prompt: 'You are a knowledgeable AI.' (reward: 0.925)\n</code></pre> After testing all prompts, the server identifies which one performed best.</p>"},{"location":"quickstart/getting-started/#whats-happening-behind-the-scenes","title":"What's Happening Behind the Scenes","text":"<p>Agent Lightning handles several complex operations automatically. Multiple workers process tasks simultaneously. Every API call and execution is tracked through the telemetry tracing system, providing detailed traces for debugging and optimization. If a worker fails, others continue processing, ensuring your optimization doesn't stop.</p> <p>The AgentOps tracer, enabled by default, collects comprehensive data about each execution, including API calls, timing information, token usage, and error traces. The data is sent to the server and can be accessed via <code>rollout.triplets</code> and <code>rollout.traces</code> at the server side to build more advanced automatic optimization algorithms.</p>"},{"location":"quickstart/getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have a working system, how about:</p> <ul> <li>Replacing the random reward with actual quality metrics based on real response accuracy?</li> <li>Testing the system prompt on a batch of different questions to see how it performs across various tasks?</li> <li>Making the algorithm automatically improve the system prompt based on the best-performing ones?</li> <li>Setting up a real agent system that consists of multiple prompts, and optimizing them together?</li> </ul>"},{"location":"quickstart/installation/","title":"Installation","text":""},{"location":"quickstart/installation/#install-from-pypi","title":"Install from PyPI","text":""},{"location":"quickstart/installation/#set-up-your-environment","title":"Set Up Your Environment","text":"<p>We strongly recommend creating a new virtual environment to avoid conflicts with other packages. You can use either <code>conda</code> or <code>venv</code>. Python 3.10 or later is recommended.</p>"},{"location":"quickstart/installation/#install-core-training-dependencies-optional","title":"Install Core Training Dependencies (Optional)","text":"<p>If you are running RL with Agent-Lightning, the next step is to install the essential packages: <code>PyTorch</code>, <code>FlashAttention</code>, <code>vLLM</code> and <code>VERL</code>. The following versions and installation order have been tested and are confirmed to work.</p> <pre><code>pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128\npip install flash-attn --no-build-isolation\npip install vllm==0.9.2\npip install verl==0.5.0\n</code></pre> <p>See this script for a full installation script.</p>"},{"location":"quickstart/installation/#install-agent-lightning","title":"Install Agent Lightning","text":"<p>Now, you're ready to install Agent Lightning itself.</p> <pre><code>pip install agentlightning\n</code></pre>"},{"location":"quickstart/installation/#install-agent-frameworks-optional","title":"Install Agent Frameworks (Optional)","text":"<p>If you plan to use other agent frameworks, you can install them with the following commands. If you don't need these, feel free to skip this step. We recommend doing this as the final step to avoid dependency versions being overwritten by mistake.</p> <pre><code># AutoGen (Recommended to install first)\npip install \"autogen-agentchat\" \"autogen-ext[openai]\"\n\n# LiteLLM\npip install \"litellm[proxy]\"\n\n# MCP\npip install mcp\n\n# UV\npip install uv\n\n# OpenAI Agents\npip install openai-agents\n\n# LangChain\npip install langgraph \"langchain[openai]\" langchain-community langchain-text-splitters\n\n# SQL-related dependencies\npip install sqlparse nltk\n</code></pre>"},{"location":"quickstart/installation/#shortcuts-for-installing-extra-dependencies","title":"Shortcuts for installing Extra Dependencies","text":"<p>For development: <pre><code>pip install agentlightning[dev]\n</code></pre></p> <p>For agent support: <pre><code>pip install agentlightning[agent]\n</code></pre></p>"},{"location":"quickstart/installation/#install-from-source","title":"Install from Source","text":"<pre><code>git clone https://github.com/microsoft/agent-lightning\ncd agent-lightning\npip install -e .[dev]\n</code></pre> <p>Please run pre-commit hooks before checking in code:</p> <pre><code>pre-commit install\npre-commit run --all-files --show-diff-on-failure --color=always\n</code></pre>"},{"location":"reference/core/","title":"Agent Lightning Core","text":""},{"location":"reference/core/#client-side","title":"Client Side","text":""},{"location":"reference/core/#agentlightning.litagent","title":"<code>agentlightning.litagent</code>","text":""},{"location":"reference/core/#agentlightning.litagent.LitAgent","title":"<code>LitAgent</code>","text":"<p>               Bases: <code>Generic[T]</code></p> <p>Base class for the training and validation logic of an agent.</p> <p>Developers should subclass this class and implement the rollout methods to define the agent's behavior for a single task. The agent's logic is completely decoupled from the server communication and training infrastructure.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>class LitAgent(Generic[T]):\n    \"\"\"Base class for the training and validation logic of an agent.\n\n    Developers should subclass this class and implement the rollout methods\n    to define the agent's behavior for a single task. The agent's logic\n    is completely decoupled from the server communication and training\n    infrastructure.\n    \"\"\"\n\n    def __init__(self, *, trained_agents: Optional[str] = None) -&gt; None:  # FIXME: str | None won't work for cli\n        \"\"\"\n        Initialize the LitAgent.\n\n        Args:\n            trained_agents: Optional string representing the trained agents.\n                            This can be used to track which agents have been trained by this instance.\n        \"\"\"\n        self.trained_agents = trained_agents\n        self._trainer_ref: weakref.ReferenceType[Trainer] | None = None\n        self._runner_ref: weakref.ReferenceType[AgentRunner] | None = None\n\n    @property\n    def is_async(self) -&gt; bool:\n        \"\"\"\n        Check if the agent implements asynchronous rollout methods.\n        Override this property for customized async detection logic.\n\n        Returns:\n            True if the agent has custom async rollout methods, False otherwise.\n        \"\"\"\n        return (\n            (\n                hasattr(self, \"training_rollout_async\")\n                and self.__class__.training_rollout_async is not LitAgent.training_rollout_async  # type: ignore\n            )\n            or (\n                hasattr(self, \"validation_rollout_async\")\n                and self.__class__.validation_rollout_async is not LitAgent.validation_rollout_async  # type: ignore\n            )\n            or (hasattr(self, \"rollout_async\") and self.__class__.rollout_async is not LitAgent.rollout_async)  # type: ignore\n        )\n\n    def set_trainer(self, trainer: Trainer) -&gt; None:\n        \"\"\"\n        Set the trainer for this agent.\n\n        Args:\n            trainer: The Trainer instance that will handle training and validation.\n        \"\"\"\n        self._trainer_ref = weakref.ref(trainer)\n\n    @property\n    def trainer(self) -&gt; Trainer:\n        \"\"\"\n        Get the trainer for this agent.\n\n        Returns:\n            The Trainer instance associated with this agent.\n        \"\"\"\n        if self._trainer_ref is None:\n            raise ValueError(\"Trainer has not been set for this agent.\")\n        trainer = self._trainer_ref()\n        if trainer is None:\n            raise ValueError(\"Trainer reference is no longer valid (object has been garbage collected).\")\n        return trainer\n\n    @property\n    def tracer(self) -&gt; BaseTracer:\n        \"\"\"\n        Get the tracer for this agent.\n\n        Returns:\n            The BaseTracer instance associated with this agent.\n        \"\"\"\n        return self.trainer.tracer\n\n    def set_runner(self, runner: AgentRunner) -&gt; None:\n        \"\"\"\n        Set the runner for this agent.\n\n        Args:\n            runner: The AgentRunner instance that will handle the execution of rollouts.\n        \"\"\"\n        self._runner_ref = weakref.ref(runner)\n\n    @property\n    def runner(self) -&gt; AgentRunner:\n        \"\"\"\n        Get the runner for this agent.\n\n        Returns:\n            The AgentRunner instance associated with this agent.\n        \"\"\"\n        if self._runner_ref is None:\n            raise ValueError(\"Runner has not been set for this agent.\")\n        runner = self._runner_ref()\n        if runner is None:\n            raise ValueError(\"Runner reference is no longer valid (object has been garbage collected).\")\n        return runner\n\n    def on_rollout_start(self, task: Task, runner: AgentRunner, tracer: BaseTracer) -&gt; None:\n        \"\"\"Hook called immediately before a rollout begins.\n\n        Args:\n            task: The :class:`Task` object that will be processed.\n            runner: The :class:`AgentRunner` managing the rollout.\n            tracer: The tracer instance associated with the runner.\n\n        Subclasses can override this method to implement custom logic such as\n        logging, metric collection, or resource setup. By default, this is a\n        no-op.\n        \"\"\"\n\n    def on_rollout_end(self, task: Task, rollout: Rollout, runner: AgentRunner, tracer: BaseTracer) -&gt; None:\n        \"\"\"Hook called after a rollout completes.\n\n        Args:\n            task: The :class:`Task` object that was processed.\n            rollout: The resulting :class:`Rollout` object.\n            runner: The :class:`AgentRunner` managing the rollout.\n            tracer: The tracer instance associated with the runner.\n\n        Subclasses can override this method for cleanup or additional\n        logging. By default, this is a no-op.\n        \"\"\"\n\n    def rollout(self, task: T, resources: NamedResources, rollout: Rollout) -&gt; RolloutRawResult:\n        \"\"\"Main entry point for executing a rollout.\n\n        This method determines whether to call the synchronous or\n        asynchronous rollout method based on the agent's implementation.\n\n        If you don't wish to implement both training rollout and validation\n        rollout separately, you can just implement `rollout` which will work for both.\n\n        Args:\n            task: The task object received from the server, containing the\n                  input data and metadata.\n            resources: A dictionary of named resources (e.g., LLMs, prompt\n                       templates) for the agent to use.\n            rollout: The full rollout object, please avoid from directly modifying it.\n                     Most agents should only use `task` and `resources`. Use `rollout`\n                     only if you need to access metadata like `rollout_id`.\n\n        Returns:\n            The result of the rollout, which can be one of:\n            - None. The tracing should be handled by the agent runner.\n            - A float representing the final reward.\n            - A list of `Triplet` objects for detailed, step-by-step feedback.\n            - A list of `ReadableSpan` objects for OpenTelemetry tracing.\n            - A list of dictionaries for any trace spans.\n            - A complete `Rollout` object for full control over reporting.\n        \"\"\"\n        raise NotImplementedError(\"Agents must implement the `rollout` method.\")\n\n    async def rollout_async(self, task: T, resources: NamedResources, rollout: Rollout) -&gt; RolloutRawResult:\n        \"\"\"Asynchronous version of the main rollout method.\n\n        This method determines whether to call the synchronous or\n        asynchronous rollout method based on the agent's implementation.\n\n        Args:\n            task: The task object received from the server, containing the\n                  input data and metadata.\n            resources: A dictionary of named resources (e.g., LLMs, prompt\n                       templates) for the agent to use.\n            rollout: The full rollout object, please avoid from directly modifying it.\n                     Most agents should only use `task` and `resources`. Use `rollout`\n                     only if you need to access metadata like `rollout_id`.\n\n        Returns:\n            The result of the rollout, which can be one of:\n            - None. The tracing should be handled by the agent runner.\n            - A float representing the final reward.\n            - A list of `Triplet` objects for detailed, step-by-step feedback.\n            - A list of `ReadableSpan` objects for OpenTelemetry tracing.\n            - A list of dictionaries for any trace spans.\n            - A complete `Rollout` object for full control over reporting.\n        \"\"\"\n        raise NotImplementedError(\"Agents must implement the `rollout_async` method for async operations.\")\n\n    def training_rollout(self, task: T, resources: NamedResources, rollout: Rollout) -&gt; RolloutRawResult:\n        \"\"\"Defines the agent's behavior for a single training task.\n\n        This method should contain the logic for how the agent processes an\n        input, uses the provided resources (like LLMs or prompts), and\n        produces a result.\n\n        Args:\n            task: The task object received from the server, containing the\n                  input data and metadata.\n            resources: A dictionary of named resources (e.g., LLMs, prompt\n                       templates) for the agent to use.\n            rollout: The full rollout object, please avoid from directly modifying it.\n        \"\"\"\n        return self.rollout(task, resources, rollout)\n\n    def validation_rollout(self, task: T, resources: NamedResources, rollout: Rollout) -&gt; RolloutRawResult:\n        \"\"\"Defines the agent's behavior for a single validation task.\n\n        By default, this method redirects to `training_rollout`. Override it\n        if the agent should behave differently during validation.\n\n        Args:\n            task: The task object received from the server, containing the\n                  input data and metadata.\n            resources: A dictionary of named resources for the agent to use.\n            rollout: The full rollout object, avoid from modifying it.\n\n        Returns:\n            The result of the validation rollout. See `rollout` for\n            possible return types.\n        \"\"\"\n        return self.rollout(task, resources, rollout)\n\n    async def training_rollout_async(self, task: T, resources: NamedResources, rollout: Rollout) -&gt; RolloutRawResult:\n        \"\"\"Asynchronous version of `training_rollout`.\n\n        This method should be implemented by agents that perform asynchronous\n        operations (e.g., non-blocking I/O, concurrent API calls).\n\n        Args:\n            task: The task object received from the server.\n            resources: A dictionary of named resources for the agent to use.\n            rollout: The full rollout object, avoid from modifying it.\n\n        Returns:\n            The result of the asynchronous training rollout. See `rollout` for\n            possible return types.\n        \"\"\"\n        return await self.rollout_async(task, resources, rollout)\n\n    async def validation_rollout_async(self, task: T, resources: NamedResources, rollout: Rollout) -&gt; RolloutRawResult:\n        \"\"\"Asynchronous version of `validation_rollout`.\n\n        By default, this method redirects to `training_rollout_async`.\n        Override it for different asynchronous validation behavior.\n\n        Args:\n            task: The task object received from the server.\n            resources: A dictionary of named resources for the agent to use.\n            rollout: The full rollout object, avoid from modifying it.\n\n        Returns:\n            The result of the asynchronous validation rollout. See `rollout` for\n            possible return types.\n        \"\"\"\n        return await self.rollout_async(task, resources, rollout)\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.is_async","title":"<code>is_async</code>  <code>property</code>","text":"<p>Check if the agent implements asynchronous rollout methods. Override this property for customized async detection logic.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the agent has custom async rollout methods, False otherwise.</p>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.runner","title":"<code>runner</code>  <code>property</code>","text":"<p>Get the runner for this agent.</p> <p>Returns:</p> Type Description <code>AgentRunner</code> <p>The AgentRunner instance associated with this agent.</p>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.tracer","title":"<code>tracer</code>  <code>property</code>","text":"<p>Get the tracer for this agent.</p> <p>Returns:</p> Type Description <code>BaseTracer</code> <p>The BaseTracer instance associated with this agent.</p>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.trainer","title":"<code>trainer</code>  <code>property</code>","text":"<p>Get the trainer for this agent.</p> <p>Returns:</p> Type Description <code>Trainer</code> <p>The Trainer instance associated with this agent.</p>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.__init__","title":"<code>__init__(*, trained_agents=None)</code>","text":"<p>Initialize the LitAgent.</p> <p>Parameters:</p> Name Type Description Default <code>trained_agents</code> <code>Optional[str]</code> <p>Optional string representing the trained agents.             This can be used to track which agents have been trained by this instance.</p> <code>None</code> Source code in <code>agentlightning/litagent.py</code> <pre><code>def __init__(self, *, trained_agents: Optional[str] = None) -&gt; None:  # FIXME: str | None won't work for cli\n    \"\"\"\n    Initialize the LitAgent.\n\n    Args:\n        trained_agents: Optional string representing the trained agents.\n                        This can be used to track which agents have been trained by this instance.\n    \"\"\"\n    self.trained_agents = trained_agents\n    self._trainer_ref: weakref.ReferenceType[Trainer] | None = None\n    self._runner_ref: weakref.ReferenceType[AgentRunner] | None = None\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.on_rollout_end","title":"<code>on_rollout_end(task, rollout, runner, tracer)</code>","text":"<p>Hook called after a rollout completes.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>The :class:<code>Task</code> object that was processed.</p> required <code>rollout</code> <code>Rollout</code> <p>The resulting :class:<code>Rollout</code> object.</p> required <code>runner</code> <code>AgentRunner</code> <p>The :class:<code>AgentRunner</code> managing the rollout.</p> required <code>tracer</code> <code>BaseTracer</code> <p>The tracer instance associated with the runner.</p> required <p>Subclasses can override this method for cleanup or additional logging. By default, this is a no-op.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>def on_rollout_end(self, task: Task, rollout: Rollout, runner: AgentRunner, tracer: BaseTracer) -&gt; None:\n    \"\"\"Hook called after a rollout completes.\n\n    Args:\n        task: The :class:`Task` object that was processed.\n        rollout: The resulting :class:`Rollout` object.\n        runner: The :class:`AgentRunner` managing the rollout.\n        tracer: The tracer instance associated with the runner.\n\n    Subclasses can override this method for cleanup or additional\n    logging. By default, this is a no-op.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.on_rollout_start","title":"<code>on_rollout_start(task, runner, tracer)</code>","text":"<p>Hook called immediately before a rollout begins.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>The :class:<code>Task</code> object that will be processed.</p> required <code>runner</code> <code>AgentRunner</code> <p>The :class:<code>AgentRunner</code> managing the rollout.</p> required <code>tracer</code> <code>BaseTracer</code> <p>The tracer instance associated with the runner.</p> required <p>Subclasses can override this method to implement custom logic such as logging, metric collection, or resource setup. By default, this is a no-op.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>def on_rollout_start(self, task: Task, runner: AgentRunner, tracer: BaseTracer) -&gt; None:\n    \"\"\"Hook called immediately before a rollout begins.\n\n    Args:\n        task: The :class:`Task` object that will be processed.\n        runner: The :class:`AgentRunner` managing the rollout.\n        tracer: The tracer instance associated with the runner.\n\n    Subclasses can override this method to implement custom logic such as\n    logging, metric collection, or resource setup. By default, this is a\n    no-op.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.rollout","title":"<code>rollout(task, resources, rollout)</code>","text":"<p>Main entry point for executing a rollout.</p> <p>This method determines whether to call the synchronous or asynchronous rollout method based on the agent's implementation.</p> <p>If you don't wish to implement both training rollout and validation rollout separately, you can just implement <code>rollout</code> which will work for both.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>T</code> <p>The task object received from the server, containing the   input data and metadata.</p> required <code>resources</code> <code>NamedResources</code> <p>A dictionary of named resources (e.g., LLMs, prompt        templates) for the agent to use.</p> required <code>rollout</code> <code>Rollout</code> <p>The full rollout object, please avoid from directly modifying it.      Most agents should only use <code>task</code> and <code>resources</code>. Use <code>rollout</code>      only if you need to access metadata like <code>rollout_id</code>.</p> required <p>Returns:</p> Type Description <code>RolloutRawResult</code> <p>The result of the rollout, which can be one of:</p> <code>RolloutRawResult</code> <ul> <li>None. The tracing should be handled by the agent runner.</li> </ul> <code>RolloutRawResult</code> <ul> <li>A float representing the final reward.</li> </ul> <code>RolloutRawResult</code> <ul> <li>A list of <code>Triplet</code> objects for detailed, step-by-step feedback.</li> </ul> <code>RolloutRawResult</code> <ul> <li>A list of <code>ReadableSpan</code> objects for OpenTelemetry tracing.</li> </ul> <code>RolloutRawResult</code> <ul> <li>A list of dictionaries for any trace spans.</li> </ul> <code>RolloutRawResult</code> <ul> <li>A complete <code>Rollout</code> object for full control over reporting.</li> </ul> Source code in <code>agentlightning/litagent.py</code> <pre><code>def rollout(self, task: T, resources: NamedResources, rollout: Rollout) -&gt; RolloutRawResult:\n    \"\"\"Main entry point for executing a rollout.\n\n    This method determines whether to call the synchronous or\n    asynchronous rollout method based on the agent's implementation.\n\n    If you don't wish to implement both training rollout and validation\n    rollout separately, you can just implement `rollout` which will work for both.\n\n    Args:\n        task: The task object received from the server, containing the\n              input data and metadata.\n        resources: A dictionary of named resources (e.g., LLMs, prompt\n                   templates) for the agent to use.\n        rollout: The full rollout object, please avoid from directly modifying it.\n                 Most agents should only use `task` and `resources`. Use `rollout`\n                 only if you need to access metadata like `rollout_id`.\n\n    Returns:\n        The result of the rollout, which can be one of:\n        - None. The tracing should be handled by the agent runner.\n        - A float representing the final reward.\n        - A list of `Triplet` objects for detailed, step-by-step feedback.\n        - A list of `ReadableSpan` objects for OpenTelemetry tracing.\n        - A list of dictionaries for any trace spans.\n        - A complete `Rollout` object for full control over reporting.\n    \"\"\"\n    raise NotImplementedError(\"Agents must implement the `rollout` method.\")\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.rollout_async","title":"<code>rollout_async(task, resources, rollout)</code>  <code>async</code>","text":"<p>Asynchronous version of the main rollout method.</p> <p>This method determines whether to call the synchronous or asynchronous rollout method based on the agent's implementation.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>T</code> <p>The task object received from the server, containing the   input data and metadata.</p> required <code>resources</code> <code>NamedResources</code> <p>A dictionary of named resources (e.g., LLMs, prompt        templates) for the agent to use.</p> required <code>rollout</code> <code>Rollout</code> <p>The full rollout object, please avoid from directly modifying it.      Most agents should only use <code>task</code> and <code>resources</code>. Use <code>rollout</code>      only if you need to access metadata like <code>rollout_id</code>.</p> required <p>Returns:</p> Type Description <code>RolloutRawResult</code> <p>The result of the rollout, which can be one of:</p> <code>RolloutRawResult</code> <ul> <li>None. The tracing should be handled by the agent runner.</li> </ul> <code>RolloutRawResult</code> <ul> <li>A float representing the final reward.</li> </ul> <code>RolloutRawResult</code> <ul> <li>A list of <code>Triplet</code> objects for detailed, step-by-step feedback.</li> </ul> <code>RolloutRawResult</code> <ul> <li>A list of <code>ReadableSpan</code> objects for OpenTelemetry tracing.</li> </ul> <code>RolloutRawResult</code> <ul> <li>A list of dictionaries for any trace spans.</li> </ul> <code>RolloutRawResult</code> <ul> <li>A complete <code>Rollout</code> object for full control over reporting.</li> </ul> Source code in <code>agentlightning/litagent.py</code> <pre><code>async def rollout_async(self, task: T, resources: NamedResources, rollout: Rollout) -&gt; RolloutRawResult:\n    \"\"\"Asynchronous version of the main rollout method.\n\n    This method determines whether to call the synchronous or\n    asynchronous rollout method based on the agent's implementation.\n\n    Args:\n        task: The task object received from the server, containing the\n              input data and metadata.\n        resources: A dictionary of named resources (e.g., LLMs, prompt\n                   templates) for the agent to use.\n        rollout: The full rollout object, please avoid from directly modifying it.\n                 Most agents should only use `task` and `resources`. Use `rollout`\n                 only if you need to access metadata like `rollout_id`.\n\n    Returns:\n        The result of the rollout, which can be one of:\n        - None. The tracing should be handled by the agent runner.\n        - A float representing the final reward.\n        - A list of `Triplet` objects for detailed, step-by-step feedback.\n        - A list of `ReadableSpan` objects for OpenTelemetry tracing.\n        - A list of dictionaries for any trace spans.\n        - A complete `Rollout` object for full control over reporting.\n    \"\"\"\n    raise NotImplementedError(\"Agents must implement the `rollout_async` method for async operations.\")\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.set_runner","title":"<code>set_runner(runner)</code>","text":"<p>Set the runner for this agent.</p> <p>Parameters:</p> Name Type Description Default <code>runner</code> <code>AgentRunner</code> <p>The AgentRunner instance that will handle the execution of rollouts.</p> required Source code in <code>agentlightning/litagent.py</code> <pre><code>def set_runner(self, runner: AgentRunner) -&gt; None:\n    \"\"\"\n    Set the runner for this agent.\n\n    Args:\n        runner: The AgentRunner instance that will handle the execution of rollouts.\n    \"\"\"\n    self._runner_ref = weakref.ref(runner)\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.set_trainer","title":"<code>set_trainer(trainer)</code>","text":"<p>Set the trainer for this agent.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The Trainer instance that will handle training and validation.</p> required Source code in <code>agentlightning/litagent.py</code> <pre><code>def set_trainer(self, trainer: Trainer) -&gt; None:\n    \"\"\"\n    Set the trainer for this agent.\n\n    Args:\n        trainer: The Trainer instance that will handle training and validation.\n    \"\"\"\n    self._trainer_ref = weakref.ref(trainer)\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.training_rollout","title":"<code>training_rollout(task, resources, rollout)</code>","text":"<p>Defines the agent's behavior for a single training task.</p> <p>This method should contain the logic for how the agent processes an input, uses the provided resources (like LLMs or prompts), and produces a result.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>T</code> <p>The task object received from the server, containing the   input data and metadata.</p> required <code>resources</code> <code>NamedResources</code> <p>A dictionary of named resources (e.g., LLMs, prompt        templates) for the agent to use.</p> required <code>rollout</code> <code>Rollout</code> <p>The full rollout object, please avoid from directly modifying it.</p> required Source code in <code>agentlightning/litagent.py</code> <pre><code>def training_rollout(self, task: T, resources: NamedResources, rollout: Rollout) -&gt; RolloutRawResult:\n    \"\"\"Defines the agent's behavior for a single training task.\n\n    This method should contain the logic for how the agent processes an\n    input, uses the provided resources (like LLMs or prompts), and\n    produces a result.\n\n    Args:\n        task: The task object received from the server, containing the\n              input data and metadata.\n        resources: A dictionary of named resources (e.g., LLMs, prompt\n                   templates) for the agent to use.\n        rollout: The full rollout object, please avoid from directly modifying it.\n    \"\"\"\n    return self.rollout(task, resources, rollout)\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.training_rollout_async","title":"<code>training_rollout_async(task, resources, rollout)</code>  <code>async</code>","text":"<p>Asynchronous version of <code>training_rollout</code>.</p> <p>This method should be implemented by agents that perform asynchronous operations (e.g., non-blocking I/O, concurrent API calls).</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>T</code> <p>The task object received from the server.</p> required <code>resources</code> <code>NamedResources</code> <p>A dictionary of named resources for the agent to use.</p> required <code>rollout</code> <code>Rollout</code> <p>The full rollout object, avoid from modifying it.</p> required <p>Returns:</p> Type Description <code>RolloutRawResult</code> <p>The result of the asynchronous training rollout. See <code>rollout</code> for</p> <code>RolloutRawResult</code> <p>possible return types.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>async def training_rollout_async(self, task: T, resources: NamedResources, rollout: Rollout) -&gt; RolloutRawResult:\n    \"\"\"Asynchronous version of `training_rollout`.\n\n    This method should be implemented by agents that perform asynchronous\n    operations (e.g., non-blocking I/O, concurrent API calls).\n\n    Args:\n        task: The task object received from the server.\n        resources: A dictionary of named resources for the agent to use.\n        rollout: The full rollout object, avoid from modifying it.\n\n    Returns:\n        The result of the asynchronous training rollout. See `rollout` for\n        possible return types.\n    \"\"\"\n    return await self.rollout_async(task, resources, rollout)\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.validation_rollout","title":"<code>validation_rollout(task, resources, rollout)</code>","text":"<p>Defines the agent's behavior for a single validation task.</p> <p>By default, this method redirects to <code>training_rollout</code>. Override it if the agent should behave differently during validation.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>T</code> <p>The task object received from the server, containing the   input data and metadata.</p> required <code>resources</code> <code>NamedResources</code> <p>A dictionary of named resources for the agent to use.</p> required <code>rollout</code> <code>Rollout</code> <p>The full rollout object, avoid from modifying it.</p> required <p>Returns:</p> Type Description <code>RolloutRawResult</code> <p>The result of the validation rollout. See <code>rollout</code> for</p> <code>RolloutRawResult</code> <p>possible return types.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>def validation_rollout(self, task: T, resources: NamedResources, rollout: Rollout) -&gt; RolloutRawResult:\n    \"\"\"Defines the agent's behavior for a single validation task.\n\n    By default, this method redirects to `training_rollout`. Override it\n    if the agent should behave differently during validation.\n\n    Args:\n        task: The task object received from the server, containing the\n              input data and metadata.\n        resources: A dictionary of named resources for the agent to use.\n        rollout: The full rollout object, avoid from modifying it.\n\n    Returns:\n        The result of the validation rollout. See `rollout` for\n        possible return types.\n    \"\"\"\n    return self.rollout(task, resources, rollout)\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.validation_rollout_async","title":"<code>validation_rollout_async(task, resources, rollout)</code>  <code>async</code>","text":"<p>Asynchronous version of <code>validation_rollout</code>.</p> <p>By default, this method redirects to <code>training_rollout_async</code>. Override it for different asynchronous validation behavior.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>T</code> <p>The task object received from the server.</p> required <code>resources</code> <code>NamedResources</code> <p>A dictionary of named resources for the agent to use.</p> required <code>rollout</code> <code>Rollout</code> <p>The full rollout object, avoid from modifying it.</p> required <p>Returns:</p> Type Description <code>RolloutRawResult</code> <p>The result of the asynchronous validation rollout. See <code>rollout</code> for</p> <code>RolloutRawResult</code> <p>possible return types.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>async def validation_rollout_async(self, task: T, resources: NamedResources, rollout: Rollout) -&gt; RolloutRawResult:\n    \"\"\"Asynchronous version of `validation_rollout`.\n\n    By default, this method redirects to `training_rollout_async`.\n    Override it for different asynchronous validation behavior.\n\n    Args:\n        task: The task object received from the server.\n        resources: A dictionary of named resources for the agent to use.\n        rollout: The full rollout object, avoid from modifying it.\n\n    Returns:\n        The result of the asynchronous validation rollout. See `rollout` for\n        possible return types.\n    \"\"\"\n    return await self.rollout_async(task, resources, rollout)\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgentLLM","title":"<code>LitAgentLLM</code>","text":"<p>               Bases: <code>LitAgent[T]</code></p> <p>A specialized LitAgent that wraps a function-based rollout that accepts dynamically a task input and a configured LLM.</p> <p>This class allows users to define agent behavior using a simple function that takes task input and an LLM resource, rather than implementing a full LitAgent subclass.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>class LitAgentLLM(LitAgent[T]):\n    \"\"\"A specialized LitAgent that wraps a function-based rollout that accepts\n    dynamically a task input and a configured LLM.\n\n    This class allows users to define agent behavior using a simple function\n    that takes task input and an LLM resource, rather than implementing a full\n    LitAgent subclass.\n    \"\"\"\n\n    def __init__(self, llm_rollout_func: LlmRolloutFunc[T], *, trained_agents: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Initialize the LitAgentLLM with an LLM rollout function.\n\n        Args:\n            llm_rollout_func: A function that defines the agent's behavior.\n                              Can be sync or async, and can optionally accept a Rollout parameter.\n            trained_agents: Optional string representing the trained agents.\n                            This can be used to track which agents have been trained by this instance.\n        \"\"\"\n        super().__init__(trained_agents=trained_agents)\n        self.llm_rollout_func = llm_rollout_func\n        self._is_async = inspect.iscoroutinefunction(llm_rollout_func)\n        self._accepts_rollout = \"rollout\" in inspect.signature(llm_rollout_func).parameters\n\n        # Copy function metadata to preserve type hints and other attributes\n        functools.update_wrapper(self, llm_rollout_func)  # type: ignore\n\n    def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Make the agent instance callable, preserving the original function behavior.\"\"\"\n        return self.llm_rollout_func(*args, **kwargs)\n\n    @property\n    def is_async(self) -&gt; bool:\n        return self._is_async\n\n    def rollout(self, task: T, resources: NamedResources, rollout: Rollout) -&gt; RolloutRawResult:\n        \"\"\"Execute a synchronous rollout using the wrapped function.\n\n        Args:\n            task: The task input data.\n            resources: Dictionary of named resources including LLMs.\n            rollout: The rollout object with metadata.\n\n        Returns:\n            The result from the wrapped rollout function.\n        \"\"\"\n        if self._is_async:\n            raise RuntimeError(\"This LitAgentLLM uses an async function. Use rollout_async instead.\")\n\n        # Find the first LLM resource\n        llm = self._get_llm_resource(resources)\n\n        if self._accepts_rollout:\n            return self.llm_rollout_func(task, llm=llm, rollout=rollout)  # type: ignore\n        else:\n            return self.llm_rollout_func(task, llm=llm)  # type: ignore\n\n    async def rollout_async(self, task: T, resources: NamedResources, rollout: Rollout) -&gt; RolloutRawResult:\n        \"\"\"Execute an asynchronous rollout using the wrapped function.\n\n        Args:\n            task: The task input data.\n            resources: Dictionary of named resources including LLMs.\n            rollout: The rollout object with metadata.\n\n        Returns:\n            The result from the wrapped rollout function.\n        \"\"\"\n        if not self._is_async:\n            raise RuntimeError(\"This LitAgentLLM uses a sync function. Use rollout instead.\")\n\n        # Find the first LLM resource\n        llm = self._get_llm_resource(resources)\n\n        if self._accepts_rollout:\n            return await self.llm_rollout_func(task, llm=llm, rollout=rollout)  # type: ignore\n        else:\n            return await self.llm_rollout_func(task, llm=llm)  # type: ignore\n\n    def _get_llm_resource(self, resources: NamedResources) -&gt; LLM:\n        \"\"\"Extract the first LLM resource from the resources dictionary.\n\n        Args:\n            resources: Dictionary of named resources.\n\n        Returns:\n            The first LLM resource found.\n\n        Raises:\n            ValueError: If no LLM resource is found.\n        \"\"\"\n        resource_found: LLM | None = None\n        for name, resource in resources.items():\n            if isinstance(resource, LLM):\n                if resource_found is not None:\n                    logger.warning(f\"Multiple LLM resources found in resources. Using the first one: '{name}'.\")\n                    break\n                resource_found = resource\n\n        if resource_found is None:\n            raise ValueError(\"No LLM resource found in the provided resources.\")\n        return resource_found\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgentLLM.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Make the agent instance callable, preserving the original function behavior.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Make the agent instance callable, preserving the original function behavior.\"\"\"\n    return self.llm_rollout_func(*args, **kwargs)\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgentLLM.__init__","title":"<code>__init__(llm_rollout_func, *, trained_agents=None)</code>","text":"<p>Initialize the LitAgentLLM with an LLM rollout function.</p> <p>Parameters:</p> Name Type Description Default <code>llm_rollout_func</code> <code>LlmRolloutFunc[T]</code> <p>A function that defines the agent's behavior.               Can be sync or async, and can optionally accept a Rollout parameter.</p> required <code>trained_agents</code> <code>Optional[str]</code> <p>Optional string representing the trained agents.             This can be used to track which agents have been trained by this instance.</p> <code>None</code> Source code in <code>agentlightning/litagent.py</code> <pre><code>def __init__(self, llm_rollout_func: LlmRolloutFunc[T], *, trained_agents: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Initialize the LitAgentLLM with an LLM rollout function.\n\n    Args:\n        llm_rollout_func: A function that defines the agent's behavior.\n                          Can be sync or async, and can optionally accept a Rollout parameter.\n        trained_agents: Optional string representing the trained agents.\n                        This can be used to track which agents have been trained by this instance.\n    \"\"\"\n    super().__init__(trained_agents=trained_agents)\n    self.llm_rollout_func = llm_rollout_func\n    self._is_async = inspect.iscoroutinefunction(llm_rollout_func)\n    self._accepts_rollout = \"rollout\" in inspect.signature(llm_rollout_func).parameters\n\n    # Copy function metadata to preserve type hints and other attributes\n    functools.update_wrapper(self, llm_rollout_func)  # type: ignore\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgentLLM.rollout","title":"<code>rollout(task, resources, rollout)</code>","text":"<p>Execute a synchronous rollout using the wrapped function.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>T</code> <p>The task input data.</p> required <code>resources</code> <code>NamedResources</code> <p>Dictionary of named resources including LLMs.</p> required <code>rollout</code> <code>Rollout</code> <p>The rollout object with metadata.</p> required <p>Returns:</p> Type Description <code>RolloutRawResult</code> <p>The result from the wrapped rollout function.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>def rollout(self, task: T, resources: NamedResources, rollout: Rollout) -&gt; RolloutRawResult:\n    \"\"\"Execute a synchronous rollout using the wrapped function.\n\n    Args:\n        task: The task input data.\n        resources: Dictionary of named resources including LLMs.\n        rollout: The rollout object with metadata.\n\n    Returns:\n        The result from the wrapped rollout function.\n    \"\"\"\n    if self._is_async:\n        raise RuntimeError(\"This LitAgentLLM uses an async function. Use rollout_async instead.\")\n\n    # Find the first LLM resource\n    llm = self._get_llm_resource(resources)\n\n    if self._accepts_rollout:\n        return self.llm_rollout_func(task, llm=llm, rollout=rollout)  # type: ignore\n    else:\n        return self.llm_rollout_func(task, llm=llm)  # type: ignore\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgentLLM.rollout_async","title":"<code>rollout_async(task, resources, rollout)</code>  <code>async</code>","text":"<p>Execute an asynchronous rollout using the wrapped function.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>T</code> <p>The task input data.</p> required <code>resources</code> <code>NamedResources</code> <p>Dictionary of named resources including LLMs.</p> required <code>rollout</code> <code>Rollout</code> <p>The rollout object with metadata.</p> required <p>Returns:</p> Type Description <code>RolloutRawResult</code> <p>The result from the wrapped rollout function.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>async def rollout_async(self, task: T, resources: NamedResources, rollout: Rollout) -&gt; RolloutRawResult:\n    \"\"\"Execute an asynchronous rollout using the wrapped function.\n\n    Args:\n        task: The task input data.\n        resources: Dictionary of named resources including LLMs.\n        rollout: The rollout object with metadata.\n\n    Returns:\n        The result from the wrapped rollout function.\n    \"\"\"\n    if not self._is_async:\n        raise RuntimeError(\"This LitAgentLLM uses a sync function. Use rollout instead.\")\n\n    # Find the first LLM resource\n    llm = self._get_llm_resource(resources)\n\n    if self._accepts_rollout:\n        return await self.llm_rollout_func(task, llm=llm, rollout=rollout)  # type: ignore\n    else:\n        return await self.llm_rollout_func(task, llm=llm)  # type: ignore\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.is_v0_1_rollout_api","title":"<code>is_v0_1_rollout_api(func)</code>","text":"<p>Check if the rollout API is v0.1. Inspect the function signature to see if it has a rollout_id parameter.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>The function to check.</p> required Source code in <code>agentlightning/litagent.py</code> <pre><code>def is_v0_1_rollout_api(func: Callable[..., Any]) -&gt; bool:\n    \"\"\"Check if the rollout API is v0.1.\n    Inspect the function signature to see if it has a rollout_id parameter.\n\n    Args:\n        func: The function to check.\n    \"\"\"\n    return \"rollout_id\" in inspect.signature(func).parameters\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.llm_rollout","title":"<code>llm_rollout(func, *, trained_agents=None)</code>","text":"<p>Create a LitAgentLLM from a function that takes (task, llm[, rollout]).</p> <p>This decorator allows you to define an agent using a simple function instead of creating a full LitAgent subclass. The returned LitAgentLLM instance is callable, preserving the original function's behavior.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>LlmRolloutFunc[T]</code> <p>A function that defines the agent's behavior. Can be:   - sync: (task, llm) -&gt; result   - sync with rollout: (task, llm, rollout) -&gt; result   - async: async (task, llm) -&gt; result   - async with rollout: async (task, llm, rollout) -&gt; result</p> required <code>trained_agents</code> <code>Optional[str]</code> <p>Optional string representing trained agents.</p> <code>None</code> <p>Returns:</p> Type Description <code>LitAgentLLM[T]</code> <p>A callable LitAgentLLM instance that preserves the original function's</p> <code>LitAgentLLM[T]</code> <p>type hints and behavior while providing all agent functionality.</p> Example <p>@llm_rollout def my_agent(task, llm):     # Agent logic here     return response</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>def llm_rollout(func: LlmRolloutFunc[T], *, trained_agents: Optional[str] = None) -&gt; LitAgentLLM[T]:\n    \"\"\"Create a LitAgentLLM from a function that takes (task, llm[, rollout]).\n\n    This decorator allows you to define an agent using a simple function\n    instead of creating a full LitAgent subclass. The returned LitAgentLLM\n    instance is callable, preserving the original function's behavior.\n\n    Args:\n        func: A function that defines the agent's behavior. Can be:\n              - sync: (task, llm) -&gt; result\n              - sync with rollout: (task, llm, rollout) -&gt; result\n              - async: async (task, llm) -&gt; result\n              - async with rollout: async (task, llm, rollout) -&gt; result\n        trained_agents: Optional string representing trained agents.\n\n    Returns:\n        A callable LitAgentLLM instance that preserves the original function's\n        type hints and behavior while providing all agent functionality.\n\n    Example:\n        @llm_rollout\n        def my_agent(task, llm):\n            # Agent logic here\n            return response\n\n        # Function is still callable with original behavior\n        result = my_agent(task, llm)\n\n        # Agent methods are also available\n        result = my_agent.rollout(task, resources, rollout)\n    \"\"\"\n    return LitAgentLLM(func, trained_agents=trained_agents)\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.llm_rollout--function-is-still-callable-with-original-behavior","title":"Function is still callable with original behavior","text":"<p>result = my_agent(task, llm)</p>"},{"location":"reference/core/#agentlightning.litagent.llm_rollout--agent-methods-are-also-available","title":"Agent methods are also available","text":"<p>result = my_agent.rollout(task, resources, rollout)</p>"},{"location":"reference/core/#agentlightning.litagent.rollout","title":"<code>rollout(func, *, trained_agents=None)</code>","text":"<p>Create a LitAgent from a function, automatically detecting the appropriate type.</p> <p>This function inspects the provided callable and creates the appropriate agent type based on its signature. The returned agent instance is callable, preserving the original function's behavior and type hints.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Union[LlmRolloutFunc[T], Callable[..., Any]]</code> <p>A function that defines the agent's behavior.</p> required <code>trained_agents</code> <code>Optional[str]</code> <p>Optional string representing trained agents.</p> <code>None</code> <p>Returns:</p> Type Description <code>LitAgent[T]</code> <p>A callable LitAgent subclass instance that preserves the original function's</p> <code>LitAgent[T]</code> <p>type hints and behavior while providing all agent functionality.</p> Example <p>@rollout def my_agent(task, llm):     client = OpenAI(base_url=llm.endpoint)     response = client.chat.completions.create(         model=llm.model,         messages=[{\"role\": \"user\", \"content\": task.input}],     )</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the function signature doesn't match any known patterns.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>def rollout(func: Union[LlmRolloutFunc[T], Callable[..., Any]], *, trained_agents: Optional[str] = None) -&gt; LitAgent[T]:\n    \"\"\"Create a LitAgent from a function, automatically detecting the appropriate type.\n\n    This function inspects the provided callable and creates the appropriate\n    agent type based on its signature. The returned agent instance is callable,\n    preserving the original function's behavior and type hints.\n\n    Args:\n        func: A function that defines the agent's behavior.\n        trained_agents: Optional string representing trained agents.\n\n    Returns:\n        A callable LitAgent subclass instance that preserves the original function's\n        type hints and behavior while providing all agent functionality.\n\n    Example:\n        @rollout\n        def my_agent(task, llm):\n            client = OpenAI(base_url=llm.endpoint)\n            response = client.chat.completions.create(\n                model=llm.model,\n                messages=[{\"role\": \"user\", \"content\": task.input}],\n            )\n\n        # Function is still callable with original behavior\n        result = my_agent(task, llm)\n\n        # Agent methods are also available\n        result = my_agent.rollout(task, resources, rollout)\n\n    Raises:\n        NotImplementedError: If the function signature doesn't match any known patterns.\n    \"\"\"\n    sig = inspect.signature(func)\n    params = list(sig.parameters.keys())\n\n    # Check if it matches the LLM rollout API pattern\n    # Should have at least 2 params, with the second one being 'llm' or typed as LLM\n    if len(params) &gt;= 2:\n        second_param = sig.parameters[params[1]]\n        # Check if the second parameter is named 'llm' or has LLM type annotation\n        if second_param.name == \"llm\" or (\n            second_param.annotation != inspect.Parameter.empty\n            and (second_param.annotation == LLM or str(second_param.annotation).endswith(\"LLM\"))\n        ):\n            return llm_rollout(func, trained_agents=trained_agents)\n\n    raise NotImplementedError(\n        f\"Function signature {sig} does not match any known agent patterns. \"\n        \"Expected signatures: (task, llm[, rollout]) or async (task, llm[, rollout])\"\n    )\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.rollout--function-is-still-callable-with-original-behavior","title":"Function is still callable with original behavior","text":"<p>result = my_agent(task, llm)</p>"},{"location":"reference/core/#agentlightning.litagent.rollout--agent-methods-are-also-available","title":"Agent methods are also available","text":"<p>result = my_agent.rollout(task, resources, rollout)</p>"},{"location":"reference/core/#agentlightning.client","title":"<code>agentlightning.client</code>","text":""},{"location":"reference/core/#agentlightning.client.AgentLightningClient","title":"<code>AgentLightningClient</code>","text":"<p>Client for interacting with a version-aware Agent Lightning Server.</p> <p>This client handles polling for tasks, fetching specific versions of resources (like model configurations), and posting completed rollouts back to the server. It provides both synchronous and asynchronous methods for these operations and includes a cache for resources.</p> Source code in <code>agentlightning/client.py</code> <pre><code>class AgentLightningClient:\n    \"\"\"\n    Client for interacting with a version-aware Agent Lightning Server.\n\n    This client handles polling for tasks, fetching specific versions of resources\n    (like model configurations), and posting completed rollouts back to the server.\n    It provides both synchronous and asynchronous methods for these operations and\n    includes a cache for resources.\n    \"\"\"\n\n    _next_task_uri = \"/task\"\n    _resources_uri = \"/resources\"\n    _latest_resources_uri = \"/resources/latest\"\n    _report_rollout_uri = \"/rollout\"\n\n    def __init__(self, endpoint: str, poll_interval: float = 5.0, timeout: float = 10.0):\n        \"\"\"Initializes the AgentLightningClient.\n\n        Args:\n            endpoint: The root URL of the Agent Lightning server.\n            poll_interval: The interval in seconds to wait between polling for new tasks.\n            timeout: The timeout in seconds for HTTP requests.\n        \"\"\"\n        self.endpoint = endpoint\n        self.task_count = 0\n        self.poll_interval = poll_interval\n        self.timeout = timeout\n        self._resource_cache: Dict[str, ResourcesUpdate] = {}  # TODO: mechanism to evict cache\n        self._default_headers = {\"X-AgentLightning-Client\": \"true\"}\n\n    async def _request_json_async(self, url: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Makes an async GET request to the specified URL and returns the JSON response.\n\n        Args:\n            url: The URL to request.\n\n        Returns:\n            The JSON response as a dictionary or None if the request fails.\n        \"\"\"\n        timeout = aiohttp.ClientTimeout(total=self.timeout)\n        async with aiohttp.ClientSession(timeout=timeout) as session:\n            try:\n                async with session.get(url, headers=self._default_headers) as resp:\n                    resp.raise_for_status()\n                    return await resp.json()\n            except Exception as e:\n                logger.debug(f\"Async GET request failed for {url}: {e}\")\n                return None\n\n    async def _post_json_async(self, url: str, payload: Dict[str, Any]) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Makes an async POST request with a JSON payload.\n\n        Args:\n            url: The URL to post to.\n            payload: The dictionary data to send as JSON.\n\n        Returns:\n            The JSON response as a dictionary or None if the request fails.\n        \"\"\"\n        timeout = aiohttp.ClientTimeout(total=self.timeout)\n        async with aiohttp.ClientSession(timeout=timeout) as session:\n            try:\n                async with session.post(url, json=payload, headers=self._default_headers) as resp:\n                    resp.raise_for_status()\n                    return await resp.json()\n            except Exception as e:\n                logger.debug(f\"Async POST request failed for {url}: {e}\")\n                return None\n\n    async def poll_next_task_async(self) -&gt; Optional[Task]:\n        \"\"\"Polls the server asynchronously for the next task until one is available.\n\n        Returns:\n            A Task object containing the task details.\n        \"\"\"\n        url = urllib.parse.urljoin(self.endpoint, self._next_task_uri)\n        while True:\n            response = await self._request_json_async(url)\n            if response:\n                task_if_any = TaskIfAny.model_validate(response)\n                if task_if_any.is_available and task_if_any.task:\n                    self.task_count += 1\n                    logger.info(f\"[Task {self.task_count} Received] ID: {task_if_any.task.rollout_id}\")\n                    return task_if_any.task\n            logger.debug(f\"No task available yet. Retrying in {self.poll_interval} seconds...\")\n            await asyncio.sleep(self.poll_interval)\n\n    async def get_resources_by_id_async(self, resource_id: str) -&gt; Optional[ResourcesUpdate]:\n        \"\"\"Fetches a specific version of resources by its ID, using a cache.\n\n        Args:\n            resource_id: The ID of the resources to fetch, usually from a Task's metadata.\n\n        Returns:\n            A ResourcesUpdate object containing the versioned resources, or None if not found.\n        \"\"\"\n        if resource_id in self._resource_cache:\n            logger.debug(f\"Found resources '{resource_id}' in cache.\")\n            return self._resource_cache[resource_id]\n\n        url = urllib.parse.urljoin(self.endpoint, f\"{self._resources_uri}/{resource_id}\")\n        response = await self._request_json_async(url)\n        if response:\n            resources_update = ResourcesUpdate.model_validate(response)\n            self._resource_cache[resource_id] = resources_update\n            logger.info(f\"Fetched and cached resources for ID: {resource_id}\")\n            return resources_update\n        return None\n\n    async def get_latest_resources_async(self) -&gt; Optional[ResourcesUpdate]:\n        \"\"\"Fetches the latest available resources from the server.\n\n        Returns:\n            A ResourcesUpdate object containing the latest resources.\n        \"\"\"\n        url = urllib.parse.urljoin(self.endpoint, self._latest_resources_uri)\n        response = await self._request_json_async(url)\n        if response:\n            resources_update = ResourcesUpdate.model_validate(response)\n            # Cache this result as well\n            self._resource_cache[resources_update.resources_id] = resources_update\n            return resources_update\n        return None\n\n    async def post_rollout_async(self, rollout: Rollout) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Posts a completed rollout to the server asynchronously.\n\n        Args:\n            rollout: A Rollout object containing the results of a task.\n\n        Returns:\n            The server's JSON response as a dictionary.\n        \"\"\"\n        url = urllib.parse.urljoin(self.endpoint, self._report_rollout_uri)\n        payload = rollout.model_dump(mode=\"json\")\n        return await self._post_json_async(url, payload)\n\n    def _request_json(self, url: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Makes a sync GET request to the specified URL and returns the JSON response.\n\n        Args:\n            url: The URL to request.\n\n        Returns:\n            The JSON response as a dictionary or None if the request fails.\n        \"\"\"\n        try:\n            response = requests.get(url, timeout=self.timeout, headers=self._default_headers)\n            response.raise_for_status()\n            return response.json()\n        except requests.exceptions.RequestException as e:\n            logger.debug(f\"Sync GET request failed for {url}: {e}\")\n            return None\n\n    def _post_json(self, url: str, payload: Dict[str, Any]) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Makes a sync POST request with a JSON payload.\n\n        Args:\n            url: The URL to post to.\n            payload: The dictionary data to send as JSON.\n\n        Returns:\n            The JSON response as a dictionary or None if the request fails.\n        \"\"\"\n        try:\n            response = requests.post(url, json=payload, timeout=self.timeout, headers=self._default_headers)\n            response.raise_for_status()\n            return response.json()\n        except requests.exceptions.RequestException as e:\n            logger.debug(f\"Sync POST request failed for {url}: {e}\")\n            return None\n\n    def poll_next_task(self) -&gt; Optional[Task]:\n        \"\"\"Polls the server synchronously for the next task until one is available.\n\n        Returns:\n            A Task object containing the task details, including the required `resources_id`.\n        \"\"\"\n        url = urllib.parse.urljoin(self.endpoint, self._next_task_uri)\n        while True:\n            response = self._request_json(url)\n            if response:\n                task_if_any = TaskIfAny.model_validate(response)\n                if task_if_any.is_available and task_if_any.task:\n                    self.task_count += 1\n                    logger.info(f\"[Task {self.task_count} Received] ID: {task_if_any.task.rollout_id}\")\n                    return task_if_any.task\n            logger.debug(f\"No task available yet. Retrying in {self.poll_interval} seconds...\")\n            time.sleep(self.poll_interval)\n\n    def get_resources_by_id(self, resource_id: str) -&gt; Optional[ResourcesUpdate]:\n        \"\"\"Fetches a specific version of resources by its ID synchronously, using a cache.\n\n        Args:\n            resource_id: The ID of the resources to fetch, usually from a Task's metadata.\n\n        Returns:\n            A ResourcesUpdate object containing the versioned resources, or None if not found.\n        \"\"\"\n        if resource_id in self._resource_cache:\n            logger.debug(f\"Found resources '{resource_id}' in cache.\")\n            return self._resource_cache[resource_id]\n\n        url = urllib.parse.urljoin(self.endpoint, f\"{self._resources_uri}/{resource_id}\")\n        response = self._request_json(url)\n        if response:\n            resources_update = ResourcesUpdate.model_validate(response)\n            self._resource_cache[resource_id] = resources_update\n            logger.info(f\"Fetched and cached resources for ID: {resource_id}\")\n            return resources_update\n        return None\n\n    def get_latest_resources(self) -&gt; Optional[ResourcesUpdate]:\n        \"\"\"Fetches the latest available resources from the server synchronously.\n\n        Returns:\n            A ResourcesUpdate object containing the latest resources.\n        \"\"\"\n        url = urllib.parse.urljoin(self.endpoint, self._latest_resources_uri)\n        response = self._request_json(url)\n        if response:\n            resources_update = ResourcesUpdate.model_validate(response)\n            self._resource_cache[resources_update.resources_id] = resources_update\n            return resources_update\n        return None\n\n    def post_rollout(self, rollout: Rollout) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Posts a completed rollout to the server synchronously.\n\n        Args:\n            rollout: A Rollout object containing the results of a task.\n\n        Returns:\n            The server's JSON response as a dictionary.\n        \"\"\"\n        url = urllib.parse.urljoin(self.endpoint, self._report_rollout_uri)\n        payload = rollout.model_dump(mode=\"json\")\n        return self._post_json(url, payload)\n</code></pre>"},{"location":"reference/core/#agentlightning.client.AgentLightningClient.__init__","title":"<code>__init__(endpoint, poll_interval=5.0, timeout=10.0)</code>","text":"<p>Initializes the AgentLightningClient.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The root URL of the Agent Lightning server.</p> required <code>poll_interval</code> <code>float</code> <p>The interval in seconds to wait between polling for new tasks.</p> <code>5.0</code> <code>timeout</code> <code>float</code> <p>The timeout in seconds for HTTP requests.</p> <code>10.0</code> Source code in <code>agentlightning/client.py</code> <pre><code>def __init__(self, endpoint: str, poll_interval: float = 5.0, timeout: float = 10.0):\n    \"\"\"Initializes the AgentLightningClient.\n\n    Args:\n        endpoint: The root URL of the Agent Lightning server.\n        poll_interval: The interval in seconds to wait between polling for new tasks.\n        timeout: The timeout in seconds for HTTP requests.\n    \"\"\"\n    self.endpoint = endpoint\n    self.task_count = 0\n    self.poll_interval = poll_interval\n    self.timeout = timeout\n    self._resource_cache: Dict[str, ResourcesUpdate] = {}  # TODO: mechanism to evict cache\n    self._default_headers = {\"X-AgentLightning-Client\": \"true\"}\n</code></pre>"},{"location":"reference/core/#agentlightning.client.AgentLightningClient.get_latest_resources","title":"<code>get_latest_resources()</code>","text":"<p>Fetches the latest available resources from the server synchronously.</p> <p>Returns:</p> Type Description <code>Optional[ResourcesUpdate]</code> <p>A ResourcesUpdate object containing the latest resources.</p> Source code in <code>agentlightning/client.py</code> <pre><code>def get_latest_resources(self) -&gt; Optional[ResourcesUpdate]:\n    \"\"\"Fetches the latest available resources from the server synchronously.\n\n    Returns:\n        A ResourcesUpdate object containing the latest resources.\n    \"\"\"\n    url = urllib.parse.urljoin(self.endpoint, self._latest_resources_uri)\n    response = self._request_json(url)\n    if response:\n        resources_update = ResourcesUpdate.model_validate(response)\n        self._resource_cache[resources_update.resources_id] = resources_update\n        return resources_update\n    return None\n</code></pre>"},{"location":"reference/core/#agentlightning.client.AgentLightningClient.get_latest_resources_async","title":"<code>get_latest_resources_async()</code>  <code>async</code>","text":"<p>Fetches the latest available resources from the server.</p> <p>Returns:</p> Type Description <code>Optional[ResourcesUpdate]</code> <p>A ResourcesUpdate object containing the latest resources.</p> Source code in <code>agentlightning/client.py</code> <pre><code>async def get_latest_resources_async(self) -&gt; Optional[ResourcesUpdate]:\n    \"\"\"Fetches the latest available resources from the server.\n\n    Returns:\n        A ResourcesUpdate object containing the latest resources.\n    \"\"\"\n    url = urllib.parse.urljoin(self.endpoint, self._latest_resources_uri)\n    response = await self._request_json_async(url)\n    if response:\n        resources_update = ResourcesUpdate.model_validate(response)\n        # Cache this result as well\n        self._resource_cache[resources_update.resources_id] = resources_update\n        return resources_update\n    return None\n</code></pre>"},{"location":"reference/core/#agentlightning.client.AgentLightningClient.get_resources_by_id","title":"<code>get_resources_by_id(resource_id)</code>","text":"<p>Fetches a specific version of resources by its ID synchronously, using a cache.</p> <p>Parameters:</p> Name Type Description Default <code>resource_id</code> <code>str</code> <p>The ID of the resources to fetch, usually from a Task's metadata.</p> required <p>Returns:</p> Type Description <code>Optional[ResourcesUpdate]</code> <p>A ResourcesUpdate object containing the versioned resources, or None if not found.</p> Source code in <code>agentlightning/client.py</code> <pre><code>def get_resources_by_id(self, resource_id: str) -&gt; Optional[ResourcesUpdate]:\n    \"\"\"Fetches a specific version of resources by its ID synchronously, using a cache.\n\n    Args:\n        resource_id: The ID of the resources to fetch, usually from a Task's metadata.\n\n    Returns:\n        A ResourcesUpdate object containing the versioned resources, or None if not found.\n    \"\"\"\n    if resource_id in self._resource_cache:\n        logger.debug(f\"Found resources '{resource_id}' in cache.\")\n        return self._resource_cache[resource_id]\n\n    url = urllib.parse.urljoin(self.endpoint, f\"{self._resources_uri}/{resource_id}\")\n    response = self._request_json(url)\n    if response:\n        resources_update = ResourcesUpdate.model_validate(response)\n        self._resource_cache[resource_id] = resources_update\n        logger.info(f\"Fetched and cached resources for ID: {resource_id}\")\n        return resources_update\n    return None\n</code></pre>"},{"location":"reference/core/#agentlightning.client.AgentLightningClient.get_resources_by_id_async","title":"<code>get_resources_by_id_async(resource_id)</code>  <code>async</code>","text":"<p>Fetches a specific version of resources by its ID, using a cache.</p> <p>Parameters:</p> Name Type Description Default <code>resource_id</code> <code>str</code> <p>The ID of the resources to fetch, usually from a Task's metadata.</p> required <p>Returns:</p> Type Description <code>Optional[ResourcesUpdate]</code> <p>A ResourcesUpdate object containing the versioned resources, or None if not found.</p> Source code in <code>agentlightning/client.py</code> <pre><code>async def get_resources_by_id_async(self, resource_id: str) -&gt; Optional[ResourcesUpdate]:\n    \"\"\"Fetches a specific version of resources by its ID, using a cache.\n\n    Args:\n        resource_id: The ID of the resources to fetch, usually from a Task's metadata.\n\n    Returns:\n        A ResourcesUpdate object containing the versioned resources, or None if not found.\n    \"\"\"\n    if resource_id in self._resource_cache:\n        logger.debug(f\"Found resources '{resource_id}' in cache.\")\n        return self._resource_cache[resource_id]\n\n    url = urllib.parse.urljoin(self.endpoint, f\"{self._resources_uri}/{resource_id}\")\n    response = await self._request_json_async(url)\n    if response:\n        resources_update = ResourcesUpdate.model_validate(response)\n        self._resource_cache[resource_id] = resources_update\n        logger.info(f\"Fetched and cached resources for ID: {resource_id}\")\n        return resources_update\n    return None\n</code></pre>"},{"location":"reference/core/#agentlightning.client.AgentLightningClient.poll_next_task","title":"<code>poll_next_task()</code>","text":"<p>Polls the server synchronously for the next task until one is available.</p> <p>Returns:</p> Type Description <code>Optional[Task]</code> <p>A Task object containing the task details, including the required <code>resources_id</code>.</p> Source code in <code>agentlightning/client.py</code> <pre><code>def poll_next_task(self) -&gt; Optional[Task]:\n    \"\"\"Polls the server synchronously for the next task until one is available.\n\n    Returns:\n        A Task object containing the task details, including the required `resources_id`.\n    \"\"\"\n    url = urllib.parse.urljoin(self.endpoint, self._next_task_uri)\n    while True:\n        response = self._request_json(url)\n        if response:\n            task_if_any = TaskIfAny.model_validate(response)\n            if task_if_any.is_available and task_if_any.task:\n                self.task_count += 1\n                logger.info(f\"[Task {self.task_count} Received] ID: {task_if_any.task.rollout_id}\")\n                return task_if_any.task\n        logger.debug(f\"No task available yet. Retrying in {self.poll_interval} seconds...\")\n        time.sleep(self.poll_interval)\n</code></pre>"},{"location":"reference/core/#agentlightning.client.AgentLightningClient.poll_next_task_async","title":"<code>poll_next_task_async()</code>  <code>async</code>","text":"<p>Polls the server asynchronously for the next task until one is available.</p> <p>Returns:</p> Type Description <code>Optional[Task]</code> <p>A Task object containing the task details.</p> Source code in <code>agentlightning/client.py</code> <pre><code>async def poll_next_task_async(self) -&gt; Optional[Task]:\n    \"\"\"Polls the server asynchronously for the next task until one is available.\n\n    Returns:\n        A Task object containing the task details.\n    \"\"\"\n    url = urllib.parse.urljoin(self.endpoint, self._next_task_uri)\n    while True:\n        response = await self._request_json_async(url)\n        if response:\n            task_if_any = TaskIfAny.model_validate(response)\n            if task_if_any.is_available and task_if_any.task:\n                self.task_count += 1\n                logger.info(f\"[Task {self.task_count} Received] ID: {task_if_any.task.rollout_id}\")\n                return task_if_any.task\n        logger.debug(f\"No task available yet. Retrying in {self.poll_interval} seconds...\")\n        await asyncio.sleep(self.poll_interval)\n</code></pre>"},{"location":"reference/core/#agentlightning.client.AgentLightningClient.post_rollout","title":"<code>post_rollout(rollout)</code>","text":"<p>Posts a completed rollout to the server synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>rollout</code> <code>Rollout</code> <p>A Rollout object containing the results of a task.</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>The server's JSON response as a dictionary.</p> Source code in <code>agentlightning/client.py</code> <pre><code>def post_rollout(self, rollout: Rollout) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Posts a completed rollout to the server synchronously.\n\n    Args:\n        rollout: A Rollout object containing the results of a task.\n\n    Returns:\n        The server's JSON response as a dictionary.\n    \"\"\"\n    url = urllib.parse.urljoin(self.endpoint, self._report_rollout_uri)\n    payload = rollout.model_dump(mode=\"json\")\n    return self._post_json(url, payload)\n</code></pre>"},{"location":"reference/core/#agentlightning.client.AgentLightningClient.post_rollout_async","title":"<code>post_rollout_async(rollout)</code>  <code>async</code>","text":"<p>Posts a completed rollout to the server asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>rollout</code> <code>Rollout</code> <p>A Rollout object containing the results of a task.</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>The server's JSON response as a dictionary.</p> Source code in <code>agentlightning/client.py</code> <pre><code>async def post_rollout_async(self, rollout: Rollout) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Posts a completed rollout to the server asynchronously.\n\n    Args:\n        rollout: A Rollout object containing the results of a task.\n\n    Returns:\n        The server's JSON response as a dictionary.\n    \"\"\"\n    url = urllib.parse.urljoin(self.endpoint, self._report_rollout_uri)\n    payload = rollout.model_dump(mode=\"json\")\n    return await self._post_json_async(url, payload)\n</code></pre>"},{"location":"reference/core/#agentlightning.client.DevTaskLoader","title":"<code>DevTaskLoader</code>","text":"<p>               Bases: <code>AgentLightningClient</code></p> <p>A local task manager for development that provides sample tasks and resources.</p> <p>This client mocks the server APIs by maintaining a local queue of tasks and resources within the same process. It's designed for development, testing, and scenarios where a full Agent Lightning server is not needed.</p> <p>The DevTaskLoader overrides the polling and resource fetching methods to return data from local collections instead of making HTTP requests to a remote server.</p> Source code in <code>agentlightning/client.py</code> <pre><code>class DevTaskLoader(AgentLightningClient):\n    \"\"\"A local task manager for development that provides sample tasks and resources.\n\n    This client mocks the server APIs by maintaining a local queue of tasks and resources\n    within the same process. It's designed for development, testing, and scenarios where\n    a full Agent Lightning server is not needed.\n\n    The DevTaskLoader overrides the polling and resource fetching methods to return data\n    from local collections instead of making HTTP requests to a remote server.\n    \"\"\"\n\n    def __init__(\n        self,\n        tasks: Union[List[TaskInput], List[Task]],\n        resources: Union[NamedResources, ResourcesUpdate],\n        **kwargs: Any,\n    ):\n        \"\"\"Initializes the DevTaskLoader with pre-defined tasks and resources.\n\n        Args:\n            tasks: Either a List of TaskInput objects or a List of Task objects.\n            resources: Either NamedResources or ResourcesUpdate object.\n            **kwargs: Additional arguments passed to the parent AgentLightningClient.\n        \"\"\"\n        super().__init__(endpoint=\"local://\", **kwargs)\n        self._tasks = tasks.copy()\n        if len(self._tasks) == 0:\n            raise ValueError(\"DevTaskLoader requires at least one task to be provided.\")\n\n        # Check if tasks are mixture of TaskInput and Task\n        if any(isinstance(task, Task) for task in self._tasks):\n            if not all(isinstance(task, Task) for task in self._tasks):\n                raise ValueError(\"All tasks must be either Task or TaskInput objects.\")\n\n        self._task_index = 0\n\n        if isinstance(resources, ResourcesUpdate):\n            self._resources_update = resources\n        else:\n            self._resources_update = ResourcesUpdate(resources_id=\"local\", resources=resources)\n\n        # Store rollouts posted back to the loader for easy debugging of local runs\n        self._rollouts: List[Rollout] = []\n\n    @property\n    def rollouts(self) -&gt; List[Rollout]:\n        \"\"\"Return rollouts that have been posted back to the loader.\"\"\"\n        return self._rollouts\n\n    def poll_next_task(self) -&gt; Optional[Task]:\n        \"\"\"Returns the next task from the local queue.\n\n        If tasks are TaskInput objects, assembles them into Task objects.\n        If tasks are already Task objects, returns them directly.\n\n        Returns:\n            The next Task object from the local task list.\n        \"\"\"\n        if self._task_index &gt;= len(self._tasks):\n            self._task_index = 0\n\n        task_or_input = self._tasks[self._task_index]\n\n        if isinstance(task_or_input, Task):\n            task = task_or_input\n        else:\n            rollout_id = f\"local_task_{self._task_index + 1:03d}\"\n            task = Task(\n                rollout_id=rollout_id,\n                input=task_or_input,\n                resources_id=self._resources_update.resources_id,\n                create_time=time.time(),\n            )\n\n        self._task_index += 1\n        self.task_count += 1\n        logger.info(f\"[Task {self.task_count} Received] Task ID: {task.rollout_id}\")\n        return task\n\n    def get_resources_by_id(self, resource_id: str) -&gt; Optional[ResourcesUpdate]:\n        logger.debug(f\"DevTaskLoader checking resources for ID: {resource_id}\")\n        if resource_id != self._resources_update.resources_id:\n            raise ValueError(\n                f\"Resource ID '{resource_id}' not found. Only '{self._resources_update.resources_id}' is available.\"\n            )\n        return self._resources_update\n\n    def get_latest_resources(self) -&gt; Optional[ResourcesUpdate]:\n        logger.debug(\"DevTaskLoader returning latest resources.\")\n        return self._resources_update\n\n    def post_rollout(self, rollout: Rollout) -&gt; Optional[Dict[str, Any]]:\n        logger.debug(f\"DevTaskLoader received rollout for task: {rollout.rollout_id}\")\n        self._rollouts.append(rollout)\n        return {\"status\": \"received\", \"rollout_id\": rollout.rollout_id}\n\n    async def poll_next_task_async(self) -&gt; Optional[Task]:\n        return self.poll_next_task()\n\n    async def get_resources_by_id_async(self, resource_id: str) -&gt; Optional[ResourcesUpdate]:\n        return self.get_resources_by_id(resource_id)\n\n    async def get_latest_resources_async(self) -&gt; Optional[ResourcesUpdate]:\n        return self.get_latest_resources()\n\n    async def post_rollout_async(self, rollout: Rollout) -&gt; Optional[Dict[str, Any]]:\n        return self.post_rollout(rollout)\n\n    def __repr__(self):\n        return f\"DevTaskLoader(num_tasks={len(self._tasks)}, resources={self._resources_update.resources})\"\n</code></pre>"},{"location":"reference/core/#agentlightning.client.DevTaskLoader.rollouts","title":"<code>rollouts</code>  <code>property</code>","text":"<p>Return rollouts that have been posted back to the loader.</p>"},{"location":"reference/core/#agentlightning.client.DevTaskLoader.__init__","title":"<code>__init__(tasks, resources, **kwargs)</code>","text":"<p>Initializes the DevTaskLoader with pre-defined tasks and resources.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>Union[List[TaskInput], List[Task]]</code> <p>Either a List of TaskInput objects or a List of Task objects.</p> required <code>resources</code> <code>Union[NamedResources, ResourcesUpdate]</code> <p>Either NamedResources or ResourcesUpdate object.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to the parent AgentLightningClient.</p> <code>{}</code> Source code in <code>agentlightning/client.py</code> <pre><code>def __init__(\n    self,\n    tasks: Union[List[TaskInput], List[Task]],\n    resources: Union[NamedResources, ResourcesUpdate],\n    **kwargs: Any,\n):\n    \"\"\"Initializes the DevTaskLoader with pre-defined tasks and resources.\n\n    Args:\n        tasks: Either a List of TaskInput objects or a List of Task objects.\n        resources: Either NamedResources or ResourcesUpdate object.\n        **kwargs: Additional arguments passed to the parent AgentLightningClient.\n    \"\"\"\n    super().__init__(endpoint=\"local://\", **kwargs)\n    self._tasks = tasks.copy()\n    if len(self._tasks) == 0:\n        raise ValueError(\"DevTaskLoader requires at least one task to be provided.\")\n\n    # Check if tasks are mixture of TaskInput and Task\n    if any(isinstance(task, Task) for task in self._tasks):\n        if not all(isinstance(task, Task) for task in self._tasks):\n            raise ValueError(\"All tasks must be either Task or TaskInput objects.\")\n\n    self._task_index = 0\n\n    if isinstance(resources, ResourcesUpdate):\n        self._resources_update = resources\n    else:\n        self._resources_update = ResourcesUpdate(resources_id=\"local\", resources=resources)\n\n    # Store rollouts posted back to the loader for easy debugging of local runs\n    self._rollouts: List[Rollout] = []\n</code></pre>"},{"location":"reference/core/#agentlightning.client.DevTaskLoader.poll_next_task","title":"<code>poll_next_task()</code>","text":"<p>Returns the next task from the local queue.</p> <p>If tasks are TaskInput objects, assembles them into Task objects. If tasks are already Task objects, returns them directly.</p> <p>Returns:</p> Type Description <code>Optional[Task]</code> <p>The next Task object from the local task list.</p> Source code in <code>agentlightning/client.py</code> <pre><code>def poll_next_task(self) -&gt; Optional[Task]:\n    \"\"\"Returns the next task from the local queue.\n\n    If tasks are TaskInput objects, assembles them into Task objects.\n    If tasks are already Task objects, returns them directly.\n\n    Returns:\n        The next Task object from the local task list.\n    \"\"\"\n    if self._task_index &gt;= len(self._tasks):\n        self._task_index = 0\n\n    task_or_input = self._tasks[self._task_index]\n\n    if isinstance(task_or_input, Task):\n        task = task_or_input\n    else:\n        rollout_id = f\"local_task_{self._task_index + 1:03d}\"\n        task = Task(\n            rollout_id=rollout_id,\n            input=task_or_input,\n            resources_id=self._resources_update.resources_id,\n            create_time=time.time(),\n        )\n\n    self._task_index += 1\n    self.task_count += 1\n    logger.info(f\"[Task {self.task_count} Received] Task ID: {task.rollout_id}\")\n    return task\n</code></pre>"},{"location":"reference/core/#agentlightning.runner","title":"<code>agentlightning.runner</code>","text":""},{"location":"reference/core/#agentlightning.runner.AgentRunner","title":"<code>AgentRunner</code>","text":"<p>               Bases: <code>ParallelWorkerBase</code></p> <p>Manages the agent's execution loop and integrates with AgentOps.</p> <p>This class orchestrates the interaction between the agent (<code>LitAgent</code>) and the server (<code>AgentLightningClient</code>). It handles polling for tasks, executing the agent's logic, and reporting results back to the server. If enabled, it will also automatically trace each rollout using AgentOps.</p> <p>Attributes:</p> Name Type Description <code>agent</code> <p>The <code>LitAgent</code> instance containing the agent's logic.</p> <code>client</code> <p>The <code>AgentLightningClient</code> for server communication.</p> <code>tracer</code> <p>The tracer instance for this runner/worker.</p> <code>worker_id</code> <p>An optional identifier for the worker process.</p> <code>max_tasks</code> <p>The maximum number of tasks to process before stopping.</p> Source code in <code>agentlightning/runner.py</code> <pre><code>class AgentRunner(ParallelWorkerBase):\n    \"\"\"Manages the agent's execution loop and integrates with AgentOps.\n\n    This class orchestrates the interaction between the agent (`LitAgent`) and\n    the server (`AgentLightningClient`). It handles polling for tasks, executing\n    the agent's logic, and reporting results back to the server. If enabled,\n    it will also automatically trace each rollout using AgentOps.\n\n    Attributes:\n        agent: The `LitAgent` instance containing the agent's logic.\n        client: The `AgentLightningClient` for server communication.\n        tracer: The tracer instance for this runner/worker.\n        worker_id: An optional identifier for the worker process.\n        max_tasks: The maximum number of tasks to process before stopping.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent: LitAgent[Any],\n        client: AgentLightningClient,\n        tracer: BaseTracer,\n        triplet_exporter: TraceTripletAdapter,\n        worker_id: Optional[int] = None,\n        max_tasks: Optional[int] = None,\n    ):\n        super().__init__()\n        self.agent = agent\n        self.client = client\n        self.tracer = tracer\n        self.triplet_exporter = triplet_exporter\n\n        # Worker-specific attributes\n        self.worker_id = worker_id\n        self.max_tasks = max_tasks\n\n    def _log_prefix(self, rollout_id: Optional[str] = None) -&gt; str:\n        \"\"\"Generates a standardized log prefix for the current worker.\"\"\"\n        if self.worker_id is not None:\n            if rollout_id:\n                return f\"[Worker {self.worker_id} | Rollout {rollout_id}]\"\n            else:\n                return f\"[Worker {self.worker_id}]\"\n        if rollout_id:\n            return f\"[Rollout {rollout_id}]\"\n        return \"[Default Worker]\"\n\n    def _to_rollout_object(\n        self,\n        result: RolloutRawResult,\n        rollout_id: str,\n    ) -&gt; Rollout:\n        \"\"\"Standardizes the agent's return value into a Rollout object.\n\n        Args:\n            result: The output from the agent's rollout method.\n            rollout_id: The unique identifier for the current task.\n\n        Returns:\n            A standardized `Rollout` object for reporting to the server.\n        \"\"\"\n        trace: Any = None\n        final_reward: Optional[float] = None\n        triplets: Optional[List[Triplet]] = None\n        trace_spans: Optional[List[ReadableSpan]] = None\n\n        # Handle different types of results from the agent\n        # Case 1: result is a float (final reward)\n        if isinstance(result, float):\n            final_reward = result\n        # Case 2: result is a list of Triplets\n        if isinstance(result, list) and all(isinstance(t, Triplet) for t in result):\n            triplets = result  # type: ignore\n        # Case 3: result is a list of ReadableSpan (OpenTelemetry spans)\n        if isinstance(result, list) and all(isinstance(t, ReadableSpan) for t in result):\n            trace_spans = result  # type: ignore\n            trace = [json.loads(readable_span.to_json()) for readable_span in trace_spans]  # type: ignore\n        # Case 4: result is a list of dict (trace JSON)\n        if isinstance(result, list) and all(isinstance(t, dict) for t in result):\n            trace = result\n        # Case 5: result is a Rollout object\n        if isinstance(result, Rollout):\n            final_reward = result.final_reward\n            triplets = result.triplets\n            trace = result.trace\n\n        # If the agent has tracing enabled, use the tracer's last trace if not already set\n        if self.tracer and (trace is None or trace_spans is None):\n            spans = self.tracer.get_last_trace()\n            if spans:\n                trace = [json.loads(readable_span.to_json()) for readable_span in spans]\n                trace_spans = spans\n\n        # Always extract triplets from the trace using TraceTripletAdapter\n        if trace_spans:\n            triplets = self.triplet_exporter(trace_spans)\n\n        # If the agent has triplets, use the last one for final reward if not set\n        if triplets and triplets[-1].reward is not None and final_reward is None:\n            final_reward = triplets[-1].reward\n\n        # Create the Rollout object with standardized fields\n        result_dict: Dict[str, Any] = {\n            \"rollout_id\": rollout_id,\n        }\n        if final_reward is not None:\n            result_dict[\"final_reward\"] = final_reward\n        if triplets is not None:\n            result_dict[\"triplets\"] = triplets\n        if trace is not None:\n            result_dict[\"trace\"] = trace\n\n        if isinstance(result, Rollout):\n            return result.model_copy(update=result_dict)\n        return Rollout(**result_dict)\n\n    def run(self) -&gt; bool:\n        \"\"\"Poll the task and rollout once synchronously.\"\"\"\n        self.agent.set_runner(self)  # Ensure the agent has a reference to this runner\n\n        task = self.client.poll_next_task()\n        if task is None:\n            logger.info(f\"{self._log_prefix()} Poll returned no task. Exiting.\")\n            return False\n        rollout_id = task.rollout_id\n\n        resources_id = task.resources_id\n        resources_update = None\n        if resources_id:\n            resources_update = self.client.get_resources_by_id(resources_id)\n        else:\n            logger.debug(f\"{self._log_prefix(rollout_id)} No 'resources_id'. Fetching latest resources.\")\n            resources_update = self.client.get_latest_resources()\n        if not resources_update:\n            logger.error(f\"{self._log_prefix(rollout_id)} Failed to fetch resources. Skipping.\")\n            return False\n\n        rollout_obj = Rollout(rollout_id=task.rollout_id, task=task)  # Default empty rollout\n\n        try:\n            try:\n                self.agent.on_rollout_start(task, self, self.tracer)\n            except Exception:\n                logger.exception(f\"{self._log_prefix(rollout_id)} Exception during on_rollout_start hook.\")\n\n            with self.tracer.trace_context(name=f\"rollout_{rollout_id}\"):\n                start_time = time.time()\n                rollout_method = self.agent.training_rollout if task.mode == \"train\" else self.agent.validation_rollout\n                # Pass the task input, not the whole task object\n                if is_v0_1_rollout_api(rollout_method):\n                    result = cast(\n                        RolloutRawResult,\n                        rollout_method(\n                            task.input, rollout_id=rollout_obj.rollout_id, resources=resources_update.resources  # type: ignore\n                        ),\n                    )  # type: ignore\n                else:\n                    result = rollout_method(task.input, resources=resources_update.resources, rollout=rollout_obj)\n                rollout_obj = self._to_rollout_object(result, task.rollout_id)\n                end_time = time.time()\n                logger.info(\n                    f\"{self._log_prefix(rollout_id)} Completed in \"\n                    f\"{end_time - start_time:.2f}s. Triplet length: \"\n                    f\"{len(rollout_obj.triplets) if rollout_obj.triplets is not None else 'N/A'}. \"\n                    f\"Reward: {rollout_obj.final_reward}\"\n                )\n\n        except Exception:\n            logger.exception(f\"{self._log_prefix(rollout_id)} Exception during rollout.\")\n        finally:\n            try:\n                self.agent.on_rollout_end(task, rollout_obj, self, self.tracer)\n            except Exception:\n                logger.exception(f\"{self._log_prefix(rollout_id)} Exception during on_rollout_end hook.\")\n            self.client.post_rollout(rollout_obj)\n\n        return True\n\n    def iter(self) -&gt; int:\n        \"\"\"Executes the synchronous polling and rollout loop.\"\"\"\n        num_tasks_processed = 0\n        logger.info(f\"{self._log_prefix()} Started sync rollouts (max: {self.max_tasks or 'unlimited'}).\")\n\n        while self.max_tasks is None or num_tasks_processed &lt; self.max_tasks:\n            if self.run():\n                num_tasks_processed += 1\n\n            if num_tasks_processed % 10 == 0 or num_tasks_processed == 1:\n                logger.info(f\"{self._log_prefix()} Progress: {num_tasks_processed}/{self.max_tasks or 'unlimited'}\")\n\n        logger.info(f\"{self._log_prefix()} Finished sync rollouts. Processed {num_tasks_processed} tasks.\")\n        return num_tasks_processed\n\n    async def run_async(self) -&gt; bool:\n        \"\"\"Poll the task and rollout once.\"\"\"\n        self.agent.set_runner(self)  # Ensure the agent has a reference to this runner\n\n        task = await self.client.poll_next_task_async()\n        if task is None:\n            logger.info(f\"{self._log_prefix()} Poll returned no task. Exiting.\")\n            return False\n        rollout_id = task.rollout_id\n\n        resources_id = task.resources_id\n        resources_update = None\n        if resources_id:\n            resources_update = await self.client.get_resources_by_id_async(resources_id)\n        else:\n            logger.debug(f\"{self._log_prefix(rollout_id)} No 'resources_id'. Fetching latest resources.\")\n            resources_update = await self.client.get_latest_resources_async()\n        if not resources_update:\n            logger.error(f\"{self._log_prefix(rollout_id)} Failed to fetch resources. Skipping.\")\n            return False\n\n        rollout_obj = Rollout(rollout_id=task.rollout_id, task=task)  # Default empty rollout\n\n        try:\n            try:\n                self.agent.on_rollout_start(task, self, self.tracer)\n            except Exception:\n                logger.exception(f\"{self._log_prefix(rollout_id)} Exception during on_rollout_start hook.\")\n\n            with self.tracer.trace_context(name=f\"rollout_{rollout_id}\"):\n                start_time = time.time()\n                rollout_method = (\n                    self.agent.training_rollout_async if task.mode == \"train\" else self.agent.validation_rollout_async\n                )\n                # Pass the task input, not the whole task object\n                if is_v0_1_rollout_api(rollout_method):\n                    result = cast(\n                        RolloutRawResult,\n                        await rollout_method(\n                            task.input, rollout_id=rollout_obj.rollout_id, resources=resources_update.resources  # type: ignore\n                        ),\n                    )  # type: ignore\n                else:\n                    result = await rollout_method(task.input, resources=resources_update.resources, rollout=rollout_obj)\n                rollout_obj = self._to_rollout_object(result, task.rollout_id)\n                end_time = time.time()\n                logger.info(\n                    f\"{self._log_prefix(rollout_id)} Completed in \"\n                    f\"{end_time - start_time:.2f}s. Triplet length: \"\n                    f\"{len(rollout_obj.triplets) if rollout_obj.triplets is not None else 'N/A'}. \"\n                    f\"Reward: {rollout_obj.final_reward}\"\n                )\n        except Exception:\n            logger.exception(f\"{self._log_prefix(rollout_id)} Exception during rollout.\")\n        finally:\n            try:\n                self.agent.on_rollout_end(task, rollout_obj, self, self.tracer)\n            except Exception:\n                logger.exception(f\"{self._log_prefix(rollout_id)} Exception during on_rollout_end hook.\")\n            await self.client.post_rollout_async(rollout_obj)\n\n        return True\n\n    async def iter_async(self) -&gt; int:\n        \"\"\"Executes the asynchronous polling and rollout loop.\"\"\"\n        num_tasks_processed = 0\n        logger.info(f\"{self._log_prefix()} Started async rollouts (max: {self.max_tasks or 'unlimited'}).\")\n\n        while self.max_tasks is None or num_tasks_processed &lt; self.max_tasks:\n            if await self.run_async():\n                num_tasks_processed += 1\n\n            if num_tasks_processed % 10 == 0 or num_tasks_processed == 1:\n                logger.info(f\"{self._log_prefix()} Progress: {num_tasks_processed}/{self.max_tasks or 'unlimited'}\")\n        logger.info(f\"{self._log_prefix()} Finished async rollouts. Processed {num_tasks_processed} tasks.\")\n        return num_tasks_processed\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.AgentRunner.iter","title":"<code>iter()</code>","text":"<p>Executes the synchronous polling and rollout loop.</p> Source code in <code>agentlightning/runner.py</code> <pre><code>def iter(self) -&gt; int:\n    \"\"\"Executes the synchronous polling and rollout loop.\"\"\"\n    num_tasks_processed = 0\n    logger.info(f\"{self._log_prefix()} Started sync rollouts (max: {self.max_tasks or 'unlimited'}).\")\n\n    while self.max_tasks is None or num_tasks_processed &lt; self.max_tasks:\n        if self.run():\n            num_tasks_processed += 1\n\n        if num_tasks_processed % 10 == 0 or num_tasks_processed == 1:\n            logger.info(f\"{self._log_prefix()} Progress: {num_tasks_processed}/{self.max_tasks or 'unlimited'}\")\n\n    logger.info(f\"{self._log_prefix()} Finished sync rollouts. Processed {num_tasks_processed} tasks.\")\n    return num_tasks_processed\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.AgentRunner.iter_async","title":"<code>iter_async()</code>  <code>async</code>","text":"<p>Executes the asynchronous polling and rollout loop.</p> Source code in <code>agentlightning/runner.py</code> <pre><code>async def iter_async(self) -&gt; int:\n    \"\"\"Executes the asynchronous polling and rollout loop.\"\"\"\n    num_tasks_processed = 0\n    logger.info(f\"{self._log_prefix()} Started async rollouts (max: {self.max_tasks or 'unlimited'}).\")\n\n    while self.max_tasks is None or num_tasks_processed &lt; self.max_tasks:\n        if await self.run_async():\n            num_tasks_processed += 1\n\n        if num_tasks_processed % 10 == 0 or num_tasks_processed == 1:\n            logger.info(f\"{self._log_prefix()} Progress: {num_tasks_processed}/{self.max_tasks or 'unlimited'}\")\n    logger.info(f\"{self._log_prefix()} Finished async rollouts. Processed {num_tasks_processed} tasks.\")\n    return num_tasks_processed\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.AgentRunner.run","title":"<code>run()</code>","text":"<p>Poll the task and rollout once synchronously.</p> Source code in <code>agentlightning/runner.py</code> <pre><code>def run(self) -&gt; bool:\n    \"\"\"Poll the task and rollout once synchronously.\"\"\"\n    self.agent.set_runner(self)  # Ensure the agent has a reference to this runner\n\n    task = self.client.poll_next_task()\n    if task is None:\n        logger.info(f\"{self._log_prefix()} Poll returned no task. Exiting.\")\n        return False\n    rollout_id = task.rollout_id\n\n    resources_id = task.resources_id\n    resources_update = None\n    if resources_id:\n        resources_update = self.client.get_resources_by_id(resources_id)\n    else:\n        logger.debug(f\"{self._log_prefix(rollout_id)} No 'resources_id'. Fetching latest resources.\")\n        resources_update = self.client.get_latest_resources()\n    if not resources_update:\n        logger.error(f\"{self._log_prefix(rollout_id)} Failed to fetch resources. Skipping.\")\n        return False\n\n    rollout_obj = Rollout(rollout_id=task.rollout_id, task=task)  # Default empty rollout\n\n    try:\n        try:\n            self.agent.on_rollout_start(task, self, self.tracer)\n        except Exception:\n            logger.exception(f\"{self._log_prefix(rollout_id)} Exception during on_rollout_start hook.\")\n\n        with self.tracer.trace_context(name=f\"rollout_{rollout_id}\"):\n            start_time = time.time()\n            rollout_method = self.agent.training_rollout if task.mode == \"train\" else self.agent.validation_rollout\n            # Pass the task input, not the whole task object\n            if is_v0_1_rollout_api(rollout_method):\n                result = cast(\n                    RolloutRawResult,\n                    rollout_method(\n                        task.input, rollout_id=rollout_obj.rollout_id, resources=resources_update.resources  # type: ignore\n                    ),\n                )  # type: ignore\n            else:\n                result = rollout_method(task.input, resources=resources_update.resources, rollout=rollout_obj)\n            rollout_obj = self._to_rollout_object(result, task.rollout_id)\n            end_time = time.time()\n            logger.info(\n                f\"{self._log_prefix(rollout_id)} Completed in \"\n                f\"{end_time - start_time:.2f}s. Triplet length: \"\n                f\"{len(rollout_obj.triplets) if rollout_obj.triplets is not None else 'N/A'}. \"\n                f\"Reward: {rollout_obj.final_reward}\"\n            )\n\n    except Exception:\n        logger.exception(f\"{self._log_prefix(rollout_id)} Exception during rollout.\")\n    finally:\n        try:\n            self.agent.on_rollout_end(task, rollout_obj, self, self.tracer)\n        except Exception:\n            logger.exception(f\"{self._log_prefix(rollout_id)} Exception during on_rollout_end hook.\")\n        self.client.post_rollout(rollout_obj)\n\n    return True\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.AgentRunner.run_async","title":"<code>run_async()</code>  <code>async</code>","text":"<p>Poll the task and rollout once.</p> Source code in <code>agentlightning/runner.py</code> <pre><code>async def run_async(self) -&gt; bool:\n    \"\"\"Poll the task and rollout once.\"\"\"\n    self.agent.set_runner(self)  # Ensure the agent has a reference to this runner\n\n    task = await self.client.poll_next_task_async()\n    if task is None:\n        logger.info(f\"{self._log_prefix()} Poll returned no task. Exiting.\")\n        return False\n    rollout_id = task.rollout_id\n\n    resources_id = task.resources_id\n    resources_update = None\n    if resources_id:\n        resources_update = await self.client.get_resources_by_id_async(resources_id)\n    else:\n        logger.debug(f\"{self._log_prefix(rollout_id)} No 'resources_id'. Fetching latest resources.\")\n        resources_update = await self.client.get_latest_resources_async()\n    if not resources_update:\n        logger.error(f\"{self._log_prefix(rollout_id)} Failed to fetch resources. Skipping.\")\n        return False\n\n    rollout_obj = Rollout(rollout_id=task.rollout_id, task=task)  # Default empty rollout\n\n    try:\n        try:\n            self.agent.on_rollout_start(task, self, self.tracer)\n        except Exception:\n            logger.exception(f\"{self._log_prefix(rollout_id)} Exception during on_rollout_start hook.\")\n\n        with self.tracer.trace_context(name=f\"rollout_{rollout_id}\"):\n            start_time = time.time()\n            rollout_method = (\n                self.agent.training_rollout_async if task.mode == \"train\" else self.agent.validation_rollout_async\n            )\n            # Pass the task input, not the whole task object\n            if is_v0_1_rollout_api(rollout_method):\n                result = cast(\n                    RolloutRawResult,\n                    await rollout_method(\n                        task.input, rollout_id=rollout_obj.rollout_id, resources=resources_update.resources  # type: ignore\n                    ),\n                )  # type: ignore\n            else:\n                result = await rollout_method(task.input, resources=resources_update.resources, rollout=rollout_obj)\n            rollout_obj = self._to_rollout_object(result, task.rollout_id)\n            end_time = time.time()\n            logger.info(\n                f\"{self._log_prefix(rollout_id)} Completed in \"\n                f\"{end_time - start_time:.2f}s. Triplet length: \"\n                f\"{len(rollout_obj.triplets) if rollout_obj.triplets is not None else 'N/A'}. \"\n                f\"Reward: {rollout_obj.final_reward}\"\n            )\n    except Exception:\n        logger.exception(f\"{self._log_prefix(rollout_id)} Exception during rollout.\")\n    finally:\n        try:\n            self.agent.on_rollout_end(task, rollout_obj, self, self.tracer)\n        except Exception:\n            logger.exception(f\"{self._log_prefix(rollout_id)} Exception during on_rollout_end hook.\")\n        await self.client.post_rollout_async(rollout_obj)\n\n    return True\n</code></pre>"},{"location":"reference/core/#agentlightning.trainer","title":"<code>agentlightning.trainer</code>","text":""},{"location":"reference/core/#agentlightning.trainer.Trainer","title":"<code>Trainer</code>","text":"<p>               Bases: <code>ParallelWorkerBase</code></p> <p>Orchestrates the distributed execution of agent rollouts.</p> <p>The Trainer is responsible for launching one or more worker processes that run the agent's execution loop. It manages multiprocessing, handles graceful shutdown, and serves as the main entry point for running a client-side agent fleet.</p> <p>Attributes:</p> Name Type Description <code>dev</code> <p>If True, rollouts are run against the dev endpoint provided in <code>fit</code>.</p> <code>n_workers</code> <p>Number of agent workers (processes) to run in parallel.</p> <code>max_tasks</code> <p>Maximum number of tasks to process per worker. If None,        workers run until no more tasks are available.</p> <code>daemon</code> <p>Whether worker processes should be daemons. Daemon processes     are terminated automatically when the main process exits.</p> <code>tracer</code> <p>A tracer instance, or a string pointing to the class full name or a dictionary with a 'type' key     that specifies the class full name and other initialization parameters.     If None, a default <code>AgentOpsTracer</code> will be created with the current settings.</p> <code>triplet_exporter</code> <p>An instance of <code>TraceTripletAdapter</code> to export triplets from traces,               or a dictionary with the initialization parameters for the exporter.</p> <code>algorithm</code> <p>An instance of <code>BaseAlgorithm</code> to use for training.</p> Source code in <code>agentlightning/trainer.py</code> <pre><code>class Trainer(ParallelWorkerBase):\n    \"\"\"Orchestrates the distributed execution of agent rollouts.\n\n    The Trainer is responsible for launching one or more worker processes\n    that run the agent's execution loop. It manages multiprocessing,\n    handles graceful shutdown, and serves as the main entry point for\n    running a client-side agent fleet.\n\n    Attributes:\n        dev: If True, rollouts are run against the dev endpoint provided in `fit`.\n        n_workers: Number of agent workers (processes) to run in parallel.\n        max_tasks: Maximum number of tasks to process per worker. If None,\n                   workers run until no more tasks are available.\n        daemon: Whether worker processes should be daemons. Daemon processes\n                are terminated automatically when the main process exits.\n        tracer: A tracer instance, or a string pointing to the class full name or a dictionary with a 'type' key\n                that specifies the class full name and other initialization parameters.\n                If None, a default `AgentOpsTracer` will be created with the current settings.\n        triplet_exporter: An instance of `TraceTripletAdapter` to export triplets from traces,\n                          or a dictionary with the initialization parameters for the exporter.\n        algorithm: An instance of `BaseAlgorithm` to use for training.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dev: bool = False,\n        n_workers: int = 1,\n        max_tasks: Optional[int] = None,\n        daemon: bool = True,\n        tracer: Union[BaseTracer, str, Dict[str, Any], None] = None,\n        triplet_exporter: Union[TraceTripletAdapter, Dict[str, Any], None] = None,\n        algorithm: Union[BaseAlgorithm, str, Dict[str, Any], None] = None,\n    ):\n        super().__init__()\n        self.n_workers = n_workers\n        self.max_tasks = max_tasks\n        self.daemon = daemon\n        self.dev = dev\n        self._client: AgentLightningClient | None = None  # Will be initialized in fit method\n\n        self.tracer = self._make_tracer(tracer)\n        if isinstance(triplet_exporter, TraceTripletAdapter):\n            self.triplet_exporter = triplet_exporter\n        elif isinstance(triplet_exporter, dict):\n            self.triplet_exporter = TraceTripletAdapter(**triplet_exporter)\n        elif triplet_exporter is None:\n            self.triplet_exporter = TraceTripletAdapter()\n        else:\n            raise ValueError(\n                f\"Invalid triplet_exporter type: {type(triplet_exporter)}. Expected TraceTripletAdapter, dict, or None.\"\n            )\n\n        self.algorithm = self._make_algorithm(algorithm)\n\n        if not self.daemon:\n            logger.warning(\n                \"daemon=False. Worker processes are non-daemonic. \"\n                \"The worker processes will NOT be terminated when the main process exits. \"\n                \"The cleanup must be handled manually.\"\n            )\n\n    def _make_tracer(self, tracer: Union[BaseTracer, str, Dict[str, Any], None]) -&gt; BaseTracer:\n        \"\"\"Creates a tracer instance based on the provided configuration.\"\"\"\n        if isinstance(tracer, BaseTracer):\n            return tracer\n        if isinstance(tracer, str):\n            module_name, class_name = tracer.rsplit(\".\", 1)\n            module = importlib.import_module(module_name)\n            tracer_cls = getattr(module, class_name)\n            return tracer_cls()\n        if isinstance(tracer, dict):\n            tracer_type = tracer.get(\"type\")\n            if tracer_type is None:\n                raise ValueError(\"tracer dict must have a 'type' key with the class full name\")\n            module_name, class_name = tracer_type.rsplit(\".\", 1)\n            module = importlib.import_module(module_name)\n            tracer_cls = getattr(module, class_name)\n            # Remove 'type' key and pass remaining keys as kwargs\n            tracer_kwargs = {k: v for k, v in tracer.items() if k != \"type\"}\n            return tracer_cls(**tracer_kwargs)\n        if tracer is None:\n            return AgentOpsTracer(agentops_managed=True, instrument_managed=True, daemon=self.daemon)\n        raise ValueError(f\"Invalid tracer type: {type(tracer)}. Expected BaseTracer, str, dict, or None.\")\n\n    def _make_algorithm(self, algorithm: Union[BaseAlgorithm, str, Dict[str, Any], None]) -&gt; Optional[BaseAlgorithm]:\n        \"\"\"Creates an algorithm instance based on the provided configuration.\"\"\"\n        if isinstance(algorithm, BaseAlgorithm):\n            return algorithm\n        if isinstance(algorithm, str):\n            module_name, class_name = algorithm.rsplit(\".\", 1)\n            module = importlib.import_module(module_name)\n            algorithm_cls = getattr(module, class_name)\n            return algorithm_cls()\n        if isinstance(algorithm, dict):\n            algorithm_type = algorithm.get(\"type\")\n            if algorithm_type is None:\n                raise ValueError(\"algorithm dict must have a 'type' key with the class full name\")\n            module_name, class_name = algorithm_type.rsplit(\".\", 1)\n            module = importlib.import_module(module_name)\n            algorithm_cls = getattr(module, class_name)\n            # Remove 'type' key and pass remaining keys as kwargs\n            algorithm_kwargs = {k: v for k, v in algorithm.items() if k != \"type\"}\n            return algorithm_cls(**algorithm_kwargs)\n        if algorithm is None:\n            return None\n        raise ValueError(f\"Invalid algorithm type: {type(algorithm)}. Expected BaseAlgorithm, str, dict, or None.\")\n\n    def _extract_client_from_data(\n        self, data: Union[str, AgentLightningClient, Dataset[Any]]\n    ) -&gt; Optional[AgentLightningClient]:\n        \"\"\"Extract client from data if it's a string URL or AgentLightningClient.\"\"\"\n        if isinstance(data, str):\n            if not data.startswith(\"http://\") and not data.startswith(\"https://\"):\n                raise ValueError(\"String data must be a valid URL starting with http:// or https://\")\n            return AgentLightningClient(endpoint=data)\n        elif isinstance(data, AgentLightningClient):\n            return data\n        return None\n\n    def _extract_dataset_from_data(\n        self, data: Union[str, AgentLightningClient, Dataset[Any]]\n    ) -&gt; Optional[Dataset[Any]]:\n        \"\"\"Extract dataset from data if it's a Dataset.\"\"\"\n        if isinstance(data, str) or isinstance(data, AgentLightningClient):\n            return None\n        return data\n\n    def _determine_backend(\n        self,\n        train_data: Union[str, AgentLightningClient, Dataset[Any]],\n        dev_data: Union[str, AgentLightningClient, Dataset[Any], None] = None,\n    ) -&gt; Union[str, AgentLightningClient]:\n        \"\"\"Determine which backend to use for initialization.\"\"\"\n        if self.dev:\n            if dev_data is None:\n                raise ValueError(\"dev_data must be provided when dev=True.\")\n            client = self._extract_client_from_data(dev_data)\n            if client is None:\n                raise ValueError(\"dev_data must be a string URL or AgentLightningClient when dev=True.\")\n            return client\n        else:\n            client = self._extract_client_from_data(train_data)\n            if client is None and self.algorithm is None:\n                raise ValueError(\n                    \"train_data must be a string URL or AgentLightningClient when no algorithm is provided.\"\n                )\n            elif client is None and self.algorithm is not None:\n                # Algorithm will be responsible for creating the client\n                client = self.algorithm.get_client()\n                logger.info(f\"Algorithm created client: {client}\")\n                return client\n            if client is None:\n                raise ValueError(\n                    \"train_data must be a string URL or AgentLightningClient when no algorithm is provided.\"\n                )\n            return client\n\n    def init(self, backend: Union[str, AgentLightningClient]) -&gt; None:\n        logger.info(f\"Initializing Trainer...\")\n\n        self._init_client(backend)\n\n        self.tracer.init()\n\n        logger.info(f\"Trainer main initialization complete.\")\n\n    def teardown(self) -&gt; None:\n        logger.info(f\"Cleaning up Trainer...\")\n        self.tracer.teardown()\n\n        self._client = None\n        logger.info(f\"Trainer main cleanup complete.\")\n\n    def client(self) -&gt; AgentLightningClient:\n        \"\"\"Returns the AgentLightningClient instance.\"\"\"\n        if self._client is None:\n            raise RuntimeError(\"AgentLightningClient has not been initialized. Call `init` first.\")\n        return self._client\n\n    def _init_client(self, backend: Union[str, AgentLightningClient]) -&gt; AgentLightningClient:\n        if self._client is None:\n            if isinstance(backend, AgentLightningClient):\n                logger.info(\"Using provided AgentLightningClient instance.\")\n                self._client = backend\n            else:\n                logger.info(f\"Initializing AgentLightningClient with endpoint: {backend}\")\n                if not isinstance(backend, str):  # type: ignore\n                    raise ValueError(\"backend must be a string URL or an AgentLightningClient instance.\")\n                if not backend.startswith(\"http://\") and not backend.startswith(\"https://\"):\n                    raise ValueError(\"backend must be a valid URL starting with http:// or https://\")\n                # Initialize the client with the provided backend URL\n                self._client = AgentLightningClient(endpoint=backend)\n        else:\n            logger.warning(\"AgentLightningClient already initialized. Returning existing instance.\")\n        return self._client\n\n    def _worker_main_loop(self, agent: LitAgent[Any], worker_id: int, is_async: bool):\n        \"\"\"The main function for each worker process.\n\n        This function initializes the client and the loop, then starts the\n        execution. It also configures process-specific settings like the\n        process title and signal handling.\n\n        Args:\n            agent: The `LitAgent` instance to run.\n            worker_id: The unique ID for this worker.\n            is_async: A boolean indicating if the async loop should be run.\n        \"\"\"\n        if self.n_workers &gt; 1:\n            import setproctitle\n\n            # Ignore Ctrl+C in worker processes; the main process handles it\n            signal.signal(signal.SIGINT, signal.SIG_IGN)\n            setproctitle.setproctitle(multiprocessing.current_process().name)\n\n        # Now we are in child processes, so we can safely set up the environment.\n        agent.set_trainer(self)\n        # TODO: this should be set elsewhere\n        if agent.trained_agents:\n            self.triplet_exporter.agent_match = agent.trained_agents\n        self._initialize_worker_env(worker_id)\n\n        mode = \"Async\" if is_async else \"Sync\"\n        logger.info(f\"[Worker {worker_id}] {mode} worker process started.\")\n\n        num_processed = 0\n\n        try:\n            client = self.client()\n            loop = AgentRunner(\n                agent=agent,\n                client=client,\n                tracer=self.tracer,\n                triplet_exporter=self.triplet_exporter,\n                max_tasks=self.max_tasks,\n                worker_id=worker_id,\n            )\n            loop.init_worker(worker_id)\n            if is_async:\n                num_processed = asyncio.run(loop.iter_async())\n            else:\n                num_processed = loop.iter()\n        except Exception:\n            logger.exception(f\"[Worker {worker_id}] Unhandled exception in worker loop.\")\n        finally:\n            self._teardown_worker_env(worker_id)\n\n        return num_processed\n\n    def _initialize_worker_env(self, worker_id: int):\n        logger.info(f\"[Worker {worker_id}] Setting up trainer environment...\")  # worker_id included in process name\n        self.tracer.init_worker(worker_id)\n\n    def _teardown_worker_env(self, worker_id: int):\n        logger.info(f\"[Worker {worker_id}] Cleaning up trainer environment...\")\n        self.tracer.teardown_worker(worker_id)\n        logger.info(f\"[Worker {worker_id}] Environment cleanup complete.\")\n\n    @staticmethod\n    def kill_orphaned_processes() -&gt; None:\n        \"\"\"\n        Kill any orphaned processes that may have been left behind by previous runs.\n        This is useful for cleaning up after crashes or unexpected exits.\n        \"\"\"\n        import psutil\n\n        for proc in psutil.process_iter():  # type: ignore\n            # check whether the process name matches\n            if proc.name().startswith(\"AgentLightning-\"):\n                proc.kill()\n\n    def _terminate_processes(self, processes: List[multiprocessing.Process]) -&gt; None:\n        if self.n_workers &gt; 1 and len(processes) &gt; 0:\n            for i, p in enumerate(processes):\n                if p.is_alive():\n                    logger.info(f\"Terminating worker {i} (name: {p.name}, PID: {p.pid})...\")\n                    p.terminate()\n                else:\n                    logger.info(f\"Worker {i} (name: {p.name}, PID: {p.pid}) is not alive or has already terminated.\")\n            for i, p in enumerate(processes):\n                if p.is_alive():\n                    p.join(timeout=10)  # Give some time to terminate\n                if p.is_alive():  # If still alive, kill\n                    logger.warning(\n                        f\"Worker {i} (name: {p.name}, PID: {p.pid}) did not terminate gracefully, killing...\"\n                    )\n                    p.kill()\n                    p.join(timeout=10)  # Ensure it's reaped\n\n    def fit(\n        self,\n        agent: LitAgent[T_co],\n        train_data: Union[str, AgentLightningClient, Dataset[T_co]],\n        *,\n        val_data: Union[str, AgentLightningClient, Dataset[T_co], None] = None,\n        dev_data: Union[str, AgentLightningClient, Dataset[T_co], None] = None,\n        dev_backend: Union[str, AgentLightningClient, None] = None,\n    ):\n        \"\"\"Train the agent using the provided data.\n\n        Each data argument can be a string URL connecting to a agent-lightning server,\n        or an AgentLightningClient instance connecting to a server (or mock server), or a dataset.\n        If no algorithm is provided when instantiating the trainer, the data must be\n        provided to connecting a server. Otherwise, dataset is also allowed and will be\n        passed to the algorithm.\n\n        If the algorithm is instantiated and there is no URL/client provided,\n        the algorithm will be responsible for creating a client that will connect to itself.\n        It can also create a mock client if the algorithm does not require a server.\n        \"\"\"\n\n        if dev_backend is not None:\n            warnings.warn(\"dev_backend is deprecated. Use dev_data instead.\")\n            if dev_data is not None:\n                raise ValueError(\"dev_data and dev_backend cannot be provided at the same time.\")\n            dev_data = dev_backend\n\n        # Extract datasets for algorithm if available\n        train_dataset = self._extract_dataset_from_data(train_data)\n        val_dataset = self._extract_dataset_from_data(val_data) if val_data else None\n        dev_dataset = self._extract_dataset_from_data(dev_data) if dev_data else None\n\n        # Initialize the algorithm with trainer if provided\n        if self.algorithm is not None:\n            self.algorithm.set_trainer(self)\n            # DO NOT RUN TRAINING HERE. Need to spawn the worker first.\n\n        # Determine the backend to use for client-server mode\n        backend = self._determine_backend(train_data, dev_data)\n\n        if self.dev:\n            logger.warning(f\"Running in dev mode. Using dev backend: {backend}\")\n        else:\n            logger.debug(f\"Running in non-dev mode. Using backend: {backend}\")\n\n        self.init(backend)\n\n        processes: List[multiprocessing.Process] = []\n\n        # Determine if the agent is asynchronous\n\n        mode = \"asynchronous\" if agent.is_async else \"synchronous\"\n\n        try:\n            if self.n_workers == 1:\n                logger.info(f\"Running with n_workers=1 ({mode} in main process).\")\n\n                # Warn if algorithm is set with single worker mode\n                if self.algorithm is not None:\n                    logger.warning(\n                        \"Algorithm is set but using single worker mode. Algorithm will never get the chance to run.\"\n                    )\n                    # Ideally the single worker should be run in a separate thread or process.\n\n                num_tasks = self._worker_main_loop(agent, 0, agent.is_async)\n                logger.info(f\"Single worker mode finished. Tasks processed: {num_tasks}\")\n\n                # If algorithm is provided and we have datasets, run algorithm after worker completes\n                if self.algorithm is not None and train_dataset is not None:\n                    logger.info(\"Running algorithm training after worker completion.\")\n                    self.algorithm.run(\n                        train_dataset=train_dataset,\n                        validation_dataset=val_dataset,\n                        dev_dataset=dev_dataset,\n                    )\n            else:\n                logger.info(f\"Running with n_workers={self.n_workers} ({mode} multiprocessing).\")\n                for i in range(self.n_workers):\n                    process_name = f\"AgentLightning-Worker-{i}\"\n                    p = multiprocessing.Process(\n                        target=self._worker_main_loop,\n                        args=(agent, i, agent.is_async),\n                        daemon=self.daemon,\n                        name=process_name,\n                    )\n                    processes.append(p)\n                    logger.info(f\"Starting worker process {i} (name: {process_name})...\")\n                    p.start()\n\n                if self.daemon:\n                    # If algorithm is provided and we have datasets, pass them to the algorithm\n                    if self.algorithm is not None:\n                        logger.info(\"All workers have been spawned. Running algorithm training with provided datasets.\")\n                        self.algorithm.run(\n                            train_dataset=train_dataset,\n                            validation_dataset=val_dataset,\n                            dev_dataset=dev_dataset,\n                        )\n                        logger.info(\"Algorithm exits. Killing the workers.\")\n                        self._terminate_processes(processes)\n\n                    for i, p in enumerate(processes):\n                        p.join()  # Wait for the process to complete\n                        logger.info(\n                            f\"Worker process {i} (name: {p.name}, PID: {p.pid}) joined with exit code {p.exitcode}.\"\n                        )\n                        if p.exitcode != 0:\n                            logger.warning(\n                                f\"Worker process {i} (name: {p.name}, PID: {p.pid}) exited with non-zero code: {p.exitcode}.\"\n                            )\n\n                    logger.info(f\"All {self.n_workers} worker processes have completed.\")\n                else:\n                    logger.info(\"All worker processes started. Main process will not wait.\")\n\n                    # A hack to stop the main process from waiting for child processes to finish.\n                    time.sleep(1)  # Give workers time to start\n                    import multiprocessing.process as multiprocessing_process\n\n                    multiprocessing_process._children.clear()  # type: ignore\n\n                    if self.algorithm is not None:\n                        logger.info(\"Main process continues to run algorithm.\")\n                        self.algorithm.run(\n                            train_dataset=train_dataset,\n                            validation_dataset=val_dataset,\n                            dev_dataset=dev_dataset,\n                        )\n                        logger.info(\"Algorithm exits. Killing the workers.\")\n                        self._terminate_processes(processes)\n\n        except KeyboardInterrupt:\n            logger.info(\"KeyboardInterrupt received. Killing the workers.\")\n            self._terminate_processes(processes)\n            logger.info(f\"Workers terminated or single worker interrupted.\")\n            raise\n        except Exception:\n            logger.exception(f\"Unhandled exception in fit method.\")\n            self._terminate_processes(processes)\n            logger.info(f\"Workers terminated or single worker interrupted.\")\n            raise\n        finally:\n            if self.daemon:\n                self.teardown()\n            else:\n                logger.info(\"Main process exiting. Please use Trainer.kill_orphaned_processes() for cleanup.\")\n</code></pre>"},{"location":"reference/core/#agentlightning.trainer.Trainer.client","title":"<code>client()</code>","text":"<p>Returns the AgentLightningClient instance.</p> Source code in <code>agentlightning/trainer.py</code> <pre><code>def client(self) -&gt; AgentLightningClient:\n    \"\"\"Returns the AgentLightningClient instance.\"\"\"\n    if self._client is None:\n        raise RuntimeError(\"AgentLightningClient has not been initialized. Call `init` first.\")\n    return self._client\n</code></pre>"},{"location":"reference/core/#agentlightning.trainer.Trainer.fit","title":"<code>fit(agent, train_data, *, val_data=None, dev_data=None, dev_backend=None)</code>","text":"<p>Train the agent using the provided data.</p> <p>Each data argument can be a string URL connecting to a agent-lightning server, or an AgentLightningClient instance connecting to a server (or mock server), or a dataset. If no algorithm is provided when instantiating the trainer, the data must be provided to connecting a server. Otherwise, dataset is also allowed and will be passed to the algorithm.</p> <p>If the algorithm is instantiated and there is no URL/client provided, the algorithm will be responsible for creating a client that will connect to itself. It can also create a mock client if the algorithm does not require a server.</p> Source code in <code>agentlightning/trainer.py</code> <pre><code>def fit(\n    self,\n    agent: LitAgent[T_co],\n    train_data: Union[str, AgentLightningClient, Dataset[T_co]],\n    *,\n    val_data: Union[str, AgentLightningClient, Dataset[T_co], None] = None,\n    dev_data: Union[str, AgentLightningClient, Dataset[T_co], None] = None,\n    dev_backend: Union[str, AgentLightningClient, None] = None,\n):\n    \"\"\"Train the agent using the provided data.\n\n    Each data argument can be a string URL connecting to a agent-lightning server,\n    or an AgentLightningClient instance connecting to a server (or mock server), or a dataset.\n    If no algorithm is provided when instantiating the trainer, the data must be\n    provided to connecting a server. Otherwise, dataset is also allowed and will be\n    passed to the algorithm.\n\n    If the algorithm is instantiated and there is no URL/client provided,\n    the algorithm will be responsible for creating a client that will connect to itself.\n    It can also create a mock client if the algorithm does not require a server.\n    \"\"\"\n\n    if dev_backend is not None:\n        warnings.warn(\"dev_backend is deprecated. Use dev_data instead.\")\n        if dev_data is not None:\n            raise ValueError(\"dev_data and dev_backend cannot be provided at the same time.\")\n        dev_data = dev_backend\n\n    # Extract datasets for algorithm if available\n    train_dataset = self._extract_dataset_from_data(train_data)\n    val_dataset = self._extract_dataset_from_data(val_data) if val_data else None\n    dev_dataset = self._extract_dataset_from_data(dev_data) if dev_data else None\n\n    # Initialize the algorithm with trainer if provided\n    if self.algorithm is not None:\n        self.algorithm.set_trainer(self)\n        # DO NOT RUN TRAINING HERE. Need to spawn the worker first.\n\n    # Determine the backend to use for client-server mode\n    backend = self._determine_backend(train_data, dev_data)\n\n    if self.dev:\n        logger.warning(f\"Running in dev mode. Using dev backend: {backend}\")\n    else:\n        logger.debug(f\"Running in non-dev mode. Using backend: {backend}\")\n\n    self.init(backend)\n\n    processes: List[multiprocessing.Process] = []\n\n    # Determine if the agent is asynchronous\n\n    mode = \"asynchronous\" if agent.is_async else \"synchronous\"\n\n    try:\n        if self.n_workers == 1:\n            logger.info(f\"Running with n_workers=1 ({mode} in main process).\")\n\n            # Warn if algorithm is set with single worker mode\n            if self.algorithm is not None:\n                logger.warning(\n                    \"Algorithm is set but using single worker mode. Algorithm will never get the chance to run.\"\n                )\n                # Ideally the single worker should be run in a separate thread or process.\n\n            num_tasks = self._worker_main_loop(agent, 0, agent.is_async)\n            logger.info(f\"Single worker mode finished. Tasks processed: {num_tasks}\")\n\n            # If algorithm is provided and we have datasets, run algorithm after worker completes\n            if self.algorithm is not None and train_dataset is not None:\n                logger.info(\"Running algorithm training after worker completion.\")\n                self.algorithm.run(\n                    train_dataset=train_dataset,\n                    validation_dataset=val_dataset,\n                    dev_dataset=dev_dataset,\n                )\n        else:\n            logger.info(f\"Running with n_workers={self.n_workers} ({mode} multiprocessing).\")\n            for i in range(self.n_workers):\n                process_name = f\"AgentLightning-Worker-{i}\"\n                p = multiprocessing.Process(\n                    target=self._worker_main_loop,\n                    args=(agent, i, agent.is_async),\n                    daemon=self.daemon,\n                    name=process_name,\n                )\n                processes.append(p)\n                logger.info(f\"Starting worker process {i} (name: {process_name})...\")\n                p.start()\n\n            if self.daemon:\n                # If algorithm is provided and we have datasets, pass them to the algorithm\n                if self.algorithm is not None:\n                    logger.info(\"All workers have been spawned. Running algorithm training with provided datasets.\")\n                    self.algorithm.run(\n                        train_dataset=train_dataset,\n                        validation_dataset=val_dataset,\n                        dev_dataset=dev_dataset,\n                    )\n                    logger.info(\"Algorithm exits. Killing the workers.\")\n                    self._terminate_processes(processes)\n\n                for i, p in enumerate(processes):\n                    p.join()  # Wait for the process to complete\n                    logger.info(\n                        f\"Worker process {i} (name: {p.name}, PID: {p.pid}) joined with exit code {p.exitcode}.\"\n                    )\n                    if p.exitcode != 0:\n                        logger.warning(\n                            f\"Worker process {i} (name: {p.name}, PID: {p.pid}) exited with non-zero code: {p.exitcode}.\"\n                        )\n\n                logger.info(f\"All {self.n_workers} worker processes have completed.\")\n            else:\n                logger.info(\"All worker processes started. Main process will not wait.\")\n\n                # A hack to stop the main process from waiting for child processes to finish.\n                time.sleep(1)  # Give workers time to start\n                import multiprocessing.process as multiprocessing_process\n\n                multiprocessing_process._children.clear()  # type: ignore\n\n                if self.algorithm is not None:\n                    logger.info(\"Main process continues to run algorithm.\")\n                    self.algorithm.run(\n                        train_dataset=train_dataset,\n                        validation_dataset=val_dataset,\n                        dev_dataset=dev_dataset,\n                    )\n                    logger.info(\"Algorithm exits. Killing the workers.\")\n                    self._terminate_processes(processes)\n\n    except KeyboardInterrupt:\n        logger.info(\"KeyboardInterrupt received. Killing the workers.\")\n        self._terminate_processes(processes)\n        logger.info(f\"Workers terminated or single worker interrupted.\")\n        raise\n    except Exception:\n        logger.exception(f\"Unhandled exception in fit method.\")\n        self._terminate_processes(processes)\n        logger.info(f\"Workers terminated or single worker interrupted.\")\n        raise\n    finally:\n        if self.daemon:\n            self.teardown()\n        else:\n            logger.info(\"Main process exiting. Please use Trainer.kill_orphaned_processes() for cleanup.\")\n</code></pre>"},{"location":"reference/core/#agentlightning.trainer.Trainer.kill_orphaned_processes","title":"<code>kill_orphaned_processes()</code>  <code>staticmethod</code>","text":"<p>Kill any orphaned processes that may have been left behind by previous runs. This is useful for cleaning up after crashes or unexpected exits.</p> Source code in <code>agentlightning/trainer.py</code> <pre><code>@staticmethod\ndef kill_orphaned_processes() -&gt; None:\n    \"\"\"\n    Kill any orphaned processes that may have been left behind by previous runs.\n    This is useful for cleaning up after crashes or unexpected exits.\n    \"\"\"\n    import psutil\n\n    for proc in psutil.process_iter():  # type: ignore\n        # check whether the process name matches\n        if proc.name().startswith(\"AgentLightning-\"):\n            proc.kill()\n</code></pre>"},{"location":"reference/core/#agentlightning.tracer","title":"<code>agentlightning.tracer</code>","text":""},{"location":"reference/core/#agentlightning.tracer.AgentOpsTracer","title":"<code>AgentOpsTracer</code>","text":"<p>               Bases: <code>BaseTracer</code></p> <p>Traces agent execution using AgentOps.</p> <p>This tracer provides functionality to capture execution details using the AgentOps library. It manages the AgentOps client initialization, server setup, and integration with the OpenTelemetry tracing ecosystem.</p> <p>Attributes:</p> Name Type Description <code>agentops_managed</code> <p>Whether to automatically manage <code>agentops</code>.               When set to true, tracer calls <code>agentops.init()</code>               automatically and launches an agentops endpoint locally.               If not, you are responsible for calling and using it               before using the tracer.</p> <code>instrument_managed</code> <p>Whether to automatically manage instrumentation.                 When set to false, you will manage the instrumentation                 yourself and the tracer might not work as expected.</p> <code>daemon</code> <p>Whether the AgentOps server runs as a daemon process.     Only applicable if <code>agentops_managed</code> is True.</p> Source code in <code>agentlightning/tracer/agentops.py</code> <pre><code>class AgentOpsTracer(BaseTracer):\n    \"\"\"Traces agent execution using AgentOps.\n\n    This tracer provides functionality to capture execution details using the\n    AgentOps library. It manages the AgentOps client initialization, server setup,\n    and integration with the OpenTelemetry tracing ecosystem.\n\n    Attributes:\n        agentops_managed: Whether to automatically manage `agentops`.\n                          When set to true, tracer calls `agentops.init()`\n                          automatically and launches an agentops endpoint locally.\n                          If not, you are responsible for calling and using it\n                          before using the tracer.\n        instrument_managed: Whether to automatically manage instrumentation.\n                            When set to false, you will manage the instrumentation\n                            yourself and the tracer might not work as expected.\n        daemon: Whether the AgentOps server runs as a daemon process.\n                Only applicable if `agentops_managed` is True.\n    \"\"\"\n\n    def __init__(self, *, agentops_managed: bool = True, instrument_managed: bool = True, daemon: bool = True):\n        super().__init__()\n        self._lightning_span_processor: Optional[LightningSpanProcessor] = None\n        self.agentops_managed = agentops_managed\n        self.instrument_managed = instrument_managed\n        self.daemon = daemon\n\n        self._agentops_server_manager = AgentOpsServerManager(self.daemon)\n        self._agentops_server_port_val: Optional[int] = None\n\n        if not self.agentops_managed:\n            logger.warning(\"agentops_managed=False. You are responsible for AgentOps setup.\")\n        if not self.instrument_managed:\n            logger.warning(\"instrument_managed=False. You are responsible for all instrumentation.\")\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state[\"_agentops_server_manager\"] = None  # Exclude the unpicklable server manager\n        # _agentops_server_port_val (int) is inherently picklable and will be included.\n        logger.debug(f\"Getting state for pickling Trainer (PID {os.getpid()}). _agentops_server_manager excluded.\")\n        return state\n\n    def __setstate__(self, state: Any):\n        self.__dict__.update(state)\n        # In child process, self._agentops_server_manager will be None.\n        logger.debug(f\"Setting state for unpickled Trainer (PID {os.getpid()}). _agentops_server_manager is None.\")\n\n    def init(self, *args: Any, **kwargs: Any):\n        if self.agentops_managed and self._agentops_server_manager:\n            self._agentops_server_manager.start()\n            self._agentops_server_port_val = self._agentops_server_manager.get_port()\n            if self._agentops_server_port_val is None:\n                if (\n                    self._agentops_server_manager.server_process is not None\n                    and self._agentops_server_manager.server_process.is_alive()\n                ):\n                    raise RuntimeError(\"AgentOps server started but port is None. Check server manager logic.\")\n                elif (\n                    self._agentops_server_port_val is None and self._agentops_server_manager.server_process is None\n                ):  # Server failed to start\n                    raise RuntimeError(\"AgentOps server manager indicates server is not running and port is None.\")\n\n    def teardown(self):\n        if self.agentops_managed:\n            self._agentops_server_manager.stop()\n            logger.info(\"AgentOps server stopped.\")\n\n    def instrument(self, worker_id: int):\n        instrument_all()\n\n    def uninstrument(self, worker_id: int):\n        uninstrument_all()\n\n    def init_worker(self, worker_id: int):\n        super().init_worker(worker_id)\n        logger.info(f\"[Worker {worker_id}] Setting up tracer...\")  # worker_id included in process name\n\n        if self.instrument_managed:\n            self.instrument(worker_id)\n            logger.info(f\"[Worker {worker_id}] Instrumentation applied.\")\n\n        if self.agentops_managed:\n            if self._agentops_server_port_val:  # Use the stored, picklable port value\n                base_url = f\"http://localhost:{self._agentops_server_port_val}\"\n                env_vars_to_set = {\n                    \"AGENTOPS_API_KEY\": \"dummy\",\n                    \"AGENTOPS_API_ENDPOINT\": base_url,\n                    \"AGENTOPS_APP_URL\": f\"{base_url}/notavailable\",\n                    \"AGENTOPS_EXPORTER_ENDPOINT\": f\"{base_url}/traces\",\n                }\n                for key, value in env_vars_to_set.items():\n                    os.environ[key] = value\n                    logger.info(f\"[Worker {worker_id}] Env var set: {key}={value}\")\n            else:\n                logger.warning(\n                    f\"[Worker {worker_id}] AgentOps managed, but local server port is not available. Client may not connect as expected.\"\n                )\n\n            if not agentops.get_client().initialized:\n                agentops.init()  # type: ignore\n                logger.info(f\"[Worker {worker_id}] AgentOps client initialized.\")\n            else:\n                logger.warning(f\"[Worker {worker_id}] AgentOps client was already initialized.\")\n\n        self._lightning_span_processor = LightningSpanProcessor()\n\n        try:\n            # new versions\n            instance = agentops.sdk.core.tracer\n            instance.provider.add_span_processor(self._lightning_span_processor)  # type: ignore\n        except AttributeError:\n            # old versions\n            instance = TracingCore.get_instance()  # type: ignore\n            instance._provider.add_span_processor(self._lightning_span_processor)  # type: ignore\n\n    def teardown_worker(self, worker_id: int) -&gt; None:\n        super().teardown_worker(worker_id)\n\n        if self.instrument_managed:\n            self.uninstrument(worker_id)\n            logger.info(f\"[Worker {worker_id}] Instrumentation removed.\")\n\n    @contextmanager\n    def trace_context(self, name: Optional[str] = None) -&gt; Iterator[LightningSpanProcessor]:\n        \"\"\"\n        Starts a new tracing context. This should be used as a context manager.\n\n        Args:\n            name: Optional name for the tracing context.\n\n        Yields:\n            The LightningSpanProcessor instance to collect spans.\n        \"\"\"\n        if not self._lightning_span_processor:\n            raise RuntimeError(\"LightningSpanProcessor is not initialized. Call init_worker() first.\")\n\n        with self._lightning_span_processor:\n            yield self._lightning_span_processor\n\n    def get_last_trace(self) -&gt; List[ReadableSpan]:\n        \"\"\"\n        Retrieves the raw list of captured spans from the most recent trace.\n\n        Returns:\n            A list of OpenTelemetry `ReadableSpan` objects.\n        \"\"\"\n        if not self._lightning_span_processor:\n            raise RuntimeError(\"LightningSpanProcessor is not initialized. Call init_worker() first.\")\n        return self._lightning_span_processor.spans()\n\n    def get_langchain_callback_handler(self, tags: List[str] | None = None) -&gt; LangchainCallbackHandler:\n        \"\"\"\n        Get the Langchain callback handler for integrating with Langchain.\n\n        Args:\n            tags: Optional list of tags to apply to the Langchain callback handler.\n\n        Returns:\n            An instance of the Langchain callback handler.\n        \"\"\"\n        import agentops\n        from agentops.integration.callbacks.langchain import LangchainCallbackHandler\n\n        tags = tags or []\n        client_instance = agentops.get_client()\n        api_key = None\n        if client_instance.initialized:\n            api_key = client_instance.config.api_key\n        else:\n            logger.warning(\n                \"AgentOps client not initialized when creating LangchainCallbackHandler. API key may be missing.\"\n            )\n        return LangchainCallbackHandler(api_key=api_key, tags=tags)\n</code></pre>"},{"location":"reference/core/#agentlightning.tracer.AgentOpsTracer.get_langchain_callback_handler","title":"<code>get_langchain_callback_handler(tags=None)</code>","text":"<p>Get the Langchain callback handler for integrating with Langchain.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>List[str] | None</code> <p>Optional list of tags to apply to the Langchain callback handler.</p> <code>None</code> <p>Returns:</p> Type Description <code>LangchainCallbackHandler</code> <p>An instance of the Langchain callback handler.</p> Source code in <code>agentlightning/tracer/agentops.py</code> <pre><code>def get_langchain_callback_handler(self, tags: List[str] | None = None) -&gt; LangchainCallbackHandler:\n    \"\"\"\n    Get the Langchain callback handler for integrating with Langchain.\n\n    Args:\n        tags: Optional list of tags to apply to the Langchain callback handler.\n\n    Returns:\n        An instance of the Langchain callback handler.\n    \"\"\"\n    import agentops\n    from agentops.integration.callbacks.langchain import LangchainCallbackHandler\n\n    tags = tags or []\n    client_instance = agentops.get_client()\n    api_key = None\n    if client_instance.initialized:\n        api_key = client_instance.config.api_key\n    else:\n        logger.warning(\n            \"AgentOps client not initialized when creating LangchainCallbackHandler. API key may be missing.\"\n        )\n    return LangchainCallbackHandler(api_key=api_key, tags=tags)\n</code></pre>"},{"location":"reference/core/#agentlightning.tracer.AgentOpsTracer.get_last_trace","title":"<code>get_last_trace()</code>","text":"<p>Retrieves the raw list of captured spans from the most recent trace.</p> <p>Returns:</p> Type Description <code>List[ReadableSpan]</code> <p>A list of OpenTelemetry <code>ReadableSpan</code> objects.</p> Source code in <code>agentlightning/tracer/agentops.py</code> <pre><code>def get_last_trace(self) -&gt; List[ReadableSpan]:\n    \"\"\"\n    Retrieves the raw list of captured spans from the most recent trace.\n\n    Returns:\n        A list of OpenTelemetry `ReadableSpan` objects.\n    \"\"\"\n    if not self._lightning_span_processor:\n        raise RuntimeError(\"LightningSpanProcessor is not initialized. Call init_worker() first.\")\n    return self._lightning_span_processor.spans()\n</code></pre>"},{"location":"reference/core/#agentlightning.tracer.AgentOpsTracer.trace_context","title":"<code>trace_context(name=None)</code>","text":"<p>Starts a new tracing context. This should be used as a context manager.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>Optional name for the tracing context.</p> <code>None</code> <p>Yields:</p> Type Description <code>LightningSpanProcessor</code> <p>The LightningSpanProcessor instance to collect spans.</p> Source code in <code>agentlightning/tracer/agentops.py</code> <pre><code>@contextmanager\ndef trace_context(self, name: Optional[str] = None) -&gt; Iterator[LightningSpanProcessor]:\n    \"\"\"\n    Starts a new tracing context. This should be used as a context manager.\n\n    Args:\n        name: Optional name for the tracing context.\n\n    Yields:\n        The LightningSpanProcessor instance to collect spans.\n    \"\"\"\n    if not self._lightning_span_processor:\n        raise RuntimeError(\"LightningSpanProcessor is not initialized. Call init_worker() first.\")\n\n    with self._lightning_span_processor:\n        yield self._lightning_span_processor\n</code></pre>"},{"location":"reference/core/#agentlightning.tracer.BaseTracer","title":"<code>BaseTracer</code>","text":"<p>               Bases: <code>ParallelWorkerBase</code></p> <p>An abstract base class for tracers.</p> <p>This class defines a standard interface for tracing code execution, capturing the resulting spans, and providing them for analysis. It is designed to be backend-agnostic, allowing for different implementations (e.g., for AgentOps, OpenTelemetry, Docker, etc.).</p> <p>The primary interaction pattern is through the <code>trace_context</code> context manager, which ensures that traces are properly started and captured, even in the case of exceptions.</p> <p>A typical workflow:</p> <pre><code>tracer = YourTracerImplementation()\n\ntry:\n    with tracer.trace_context(name=\"my_traced_task\"):\n        # ... code to be traced ...\n        run_my_agent_logic()\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n\n# Retrieve the trace data after the context block\nspans: list[ReadableSpan] = tracer.get_last_trace()\n\n# Process the trace data\nif trace_tree:\n    rl_triplets = TraceTripletAdapter().adapt(spans)\n    # ... do something with the triplets\n</code></pre> Source code in <code>agentlightning/tracer/base.py</code> <pre><code>class BaseTracer(ParallelWorkerBase):\n    \"\"\"\n    An abstract base class for tracers.\n\n    This class defines a standard interface for tracing code execution,\n    capturing the resulting spans, and providing them for analysis. It is\n    designed to be backend-agnostic, allowing for different implementations\n    (e.g., for AgentOps, OpenTelemetry, Docker, etc.).\n\n    The primary interaction pattern is through the `trace_context`\n    context manager, which ensures that traces are properly started and captured,\n    even in the case of exceptions.\n\n    A typical workflow:\n\n    ```python\n    tracer = YourTracerImplementation()\n\n    try:\n        with tracer.trace_context(name=\"my_traced_task\"):\n            # ... code to be traced ...\n            run_my_agent_logic()\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n    # Retrieve the trace data after the context block\n    spans: list[ReadableSpan] = tracer.get_last_trace()\n\n    # Process the trace data\n    if trace_tree:\n        rl_triplets = TraceTripletAdapter().adapt(spans)\n        # ... do something with the triplets\n    ```\n    \"\"\"\n\n    @contextmanager\n    def trace_context(self, name: Optional[str] = None) -&gt; Iterator[Any]:\n        \"\"\"\n        Starts a new tracing context. This should be used as a context manager.\n\n        The implementation should handle the setup and teardown of the tracing\n        for the enclosed code block. It must ensure that any spans generated\n        within the `with` block are collected and made available via\n        `get_last_trace`.\n\n        Args:\n            name: The name for the root span of this trace context.\n        \"\"\"\n        raise NotImplementedError()\n\n    def get_last_trace(self) -&gt; List[ReadableSpan]:\n        \"\"\"\n        Retrieves the raw list of captured spans from the most recent trace.\n\n        Returns:\n            A list of OpenTelemetry `ReadableSpan` objects.\n        \"\"\"\n        raise NotImplementedError()\n\n    def trace_run(self, func: Callable[..., Any], *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"\n        A convenience wrapper to trace the execution of a single synchronous function.\n\n        Args:\n            func: The synchronous function to execute and trace.\n            *args: Positional arguments to pass to the function.\n            **kwargs: Keyword arguments to pass to the function.\n\n        Returns:\n            The return value of the function.\n        \"\"\"\n        with self.trace_context(name=func.__name__):\n            return func(*args, **kwargs)\n\n    async def trace_run_async(self, func: Callable[..., Awaitable[Any]], *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"\n        A convenience wrapper to trace the execution of a single asynchronous function.\n\n        Args:\n            func: The asynchronous function to execute and trace.\n            *args: Positional arguments to pass to the function.\n            **kwargs: Keyword arguments to pass to the function.\n\n        Returns:\n            The return value of the function.\n        \"\"\"\n        with self.trace_context(name=func.__name__):\n            return await func(*args, **kwargs)\n</code></pre>"},{"location":"reference/core/#agentlightning.tracer.BaseTracer.get_last_trace","title":"<code>get_last_trace()</code>","text":"<p>Retrieves the raw list of captured spans from the most recent trace.</p> <p>Returns:</p> Type Description <code>List[ReadableSpan]</code> <p>A list of OpenTelemetry <code>ReadableSpan</code> objects.</p> Source code in <code>agentlightning/tracer/base.py</code> <pre><code>def get_last_trace(self) -&gt; List[ReadableSpan]:\n    \"\"\"\n    Retrieves the raw list of captured spans from the most recent trace.\n\n    Returns:\n        A list of OpenTelemetry `ReadableSpan` objects.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/core/#agentlightning.tracer.BaseTracer.trace_context","title":"<code>trace_context(name=None)</code>","text":"<p>Starts a new tracing context. This should be used as a context manager.</p> <p>The implementation should handle the setup and teardown of the tracing for the enclosed code block. It must ensure that any spans generated within the <code>with</code> block are collected and made available via <code>get_last_trace</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name for the root span of this trace context.</p> <code>None</code> Source code in <code>agentlightning/tracer/base.py</code> <pre><code>@contextmanager\ndef trace_context(self, name: Optional[str] = None) -&gt; Iterator[Any]:\n    \"\"\"\n    Starts a new tracing context. This should be used as a context manager.\n\n    The implementation should handle the setup and teardown of the tracing\n    for the enclosed code block. It must ensure that any spans generated\n    within the `with` block are collected and made available via\n    `get_last_trace`.\n\n    Args:\n        name: The name for the root span of this trace context.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/core/#agentlightning.tracer.BaseTracer.trace_run","title":"<code>trace_run(func, *args, **kwargs)</code>","text":"<p>A convenience wrapper to trace the execution of a single synchronous function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>The synchronous function to execute and trace.</p> required <code>*args</code> <code>Any</code> <p>Positional arguments to pass to the function.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The return value of the function.</p> Source code in <code>agentlightning/tracer/base.py</code> <pre><code>def trace_run(self, func: Callable[..., Any], *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"\n    A convenience wrapper to trace the execution of a single synchronous function.\n\n    Args:\n        func: The synchronous function to execute and trace.\n        *args: Positional arguments to pass to the function.\n        **kwargs: Keyword arguments to pass to the function.\n\n    Returns:\n        The return value of the function.\n    \"\"\"\n    with self.trace_context(name=func.__name__):\n        return func(*args, **kwargs)\n</code></pre>"},{"location":"reference/core/#agentlightning.tracer.BaseTracer.trace_run_async","title":"<code>trace_run_async(func, *args, **kwargs)</code>  <code>async</code>","text":"<p>A convenience wrapper to trace the execution of a single asynchronous function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Awaitable[Any]]</code> <p>The asynchronous function to execute and trace.</p> required <code>*args</code> <code>Any</code> <p>Positional arguments to pass to the function.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The return value of the function.</p> Source code in <code>agentlightning/tracer/base.py</code> <pre><code>async def trace_run_async(self, func: Callable[..., Awaitable[Any]], *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"\n    A convenience wrapper to trace the execution of a single asynchronous function.\n\n    Args:\n        func: The asynchronous function to execute and trace.\n        *args: Positional arguments to pass to the function.\n        **kwargs: Keyword arguments to pass to the function.\n\n    Returns:\n        The return value of the function.\n    \"\"\"\n    with self.trace_context(name=func.__name__):\n        return await func(*args, **kwargs)\n</code></pre>"},{"location":"reference/core/#agentlightning.reward","title":"<code>agentlightning.reward</code>","text":""},{"location":"reference/core/#agentlightning.reward.reward","title":"<code>reward(fn)</code>","text":"<p>A decorator to wrap a function that computes rewards. It will automatically handle the input and output of the function.</p> Source code in <code>agentlightning/reward.py</code> <pre><code>def reward(fn: FnType) -&gt; FnType:\n    \"\"\"\n    A decorator to wrap a function that computes rewards.\n    It will automatically handle the input and output of the function.\n    \"\"\"\n\n    def wrap_result(result: Optional[float]) -&gt; RewardSpanData:\n        \"\"\"\n        Wrap the result of the function in a dict.\n        \"\"\"\n        if result is None:\n            return {\"type\": \"reward\", \"value\": None}\n        if not isinstance(result, (float, int)):  # type: ignore\n            warnings.warn(f\"Reward is ignored because it is not a number: {result}\")\n            return {\"type\": \"reward\", \"value\": None}\n        return {\"type\": \"reward\", \"value\": float(result)}\n\n    # Check if the function is async\n    is_async = asyncio.iscoroutinefunction(fn) or inspect.iscoroutinefunction(fn)\n\n    if is_async:\n\n        async def wrapper_async(*args: Any, **kwargs: Any) -&gt; Any:\n            result: Optional[float] = None\n\n            @operation\n            async def agentops_reward_operation() -&gt; RewardSpanData:\n                # The reward function we are interested in tracing\n                # It takes zero inputs and return a formatted dict\n                nonlocal result\n                result = await fn(*args, **kwargs)\n                return wrap_result(result)\n\n            await agentops_reward_operation()\n            return result\n\n        return wrapper_async  # type: ignore\n\n    else:\n\n        def wrapper(*args: Any, **kwargs: Any) -&gt; Any:\n            result: Optional[float] = None\n\n            @operation\n            def agentops_reward_operation() -&gt; RewardSpanData:\n                nonlocal result\n                result = fn(*args, **kwargs)\n                return wrap_result(result)\n\n            agentops_reward_operation()\n            return result\n\n        return wrapper  # type: ignore\n</code></pre>"},{"location":"reference/core/#server-side","title":"Server Side","text":""},{"location":"reference/core/#agentlightning.server","title":"<code>agentlightning.server</code>","text":""},{"location":"reference/core/#agentlightning.server.AgentLightningServer","title":"<code>AgentLightningServer</code>","text":"<p>The main SDK class for developers to control the Agent Lightning Server.</p> <p>This class manages the server lifecycle, task queueing, resources updates, and retrieval of results, providing a simple interface for the optimization logic.</p> Source code in <code>agentlightning/server.py</code> <pre><code>class AgentLightningServer:\n    \"\"\"\n    The main SDK class for developers to control the Agent Lightning Server.\n\n    This class manages the server lifecycle, task queueing, resources updates,\n    and retrieval of results, providing a simple interface for the optimization logic.\n    \"\"\"\n\n    def __init__(self, host: str = \"127.0.0.1\", port: int = 8000, task_timeout_seconds: float = 300.0):\n        \"\"\"\n        Initializes the server controller.\n\n        Args:\n            host: The host to bind the server to.\n            port: The port to bind the server to.\n            task_timeout_seconds: Time in seconds after which a claimed task is considered stale and requeued.\n        \"\"\"\n        self.host = host\n        self.port = port\n        self.endpoint = f\"http://{host}:{port}\"\n        self._task_timeout_seconds = task_timeout_seconds\n\n        # Defer initialization and use event for cross-thread communication\n        self._store: Optional[ServerDataStore] = None\n        self.loop: Optional[asyncio.AbstractEventLoop] = None\n        self.startup_event = threading.Event()\n\n        # Create FastAPI app instance with a lifespan manager\n        self._app = FastAPI(lifespan=self._lifespan)\n        self._setup_routes()\n\n        self._uvicorn_config = uvicorn.Config(self._app, host=self.host, port=self.port, log_level=\"info\")\n        self._uvicorn_server = uvicorn.Server(self._uvicorn_config)\n\n    # --- ADDED: Lifespan context manager ---\n    @asynccontextmanager\n    async def _lifespan(self, app: FastAPI):\n        \"\"\"\n        Manages server startup and shutdown. This runs inside the server's event loop.\n        \"\"\"\n        logger.info(\"Server is starting up...\")\n        self.loop = asyncio.get_running_loop()\n        self._store = ServerDataStore()  # Initialize data store here\n        self.startup_event.set()  # Signal that the server is ready\n\n        yield\n\n        logger.info(\"Server is shutting down.\")\n        self._store = None\n        self.startup_event.clear()  # Clear the startup event\n        self.loop = None\n\n    async def _check_and_requeue_stale_tasks(self):\n        \"\"\"\n        Check for stale tasks and requeue them. Called reactively during get_next_task.\n        \"\"\"\n        current_time = time.time()\n        # Ensure store is initialized before checking\n        if not self._store:\n            return\n        processing_tasks = self._store.get_processing_tasks()\n\n        for _, task in processing_tasks.items():\n            if task.last_claim_time and current_time - task.last_claim_time &gt; self._task_timeout_seconds:\n                await self._store.requeue_task(task)\n                logger.warning(\n                    f\"Task {task.rollout_id} timed out after {self._task_timeout_seconds}s, requeued (attempt {task.num_claims})\"\n                )\n\n    def _setup_routes(self):\n        \"\"\"Setup FastAPI routes.\"\"\"\n\n        @self._app.get(\"/task\", response_model=TaskIfAny)\n        async def next_task() -&gt; TaskIfAny:  # type: ignore\n            \"\"\"Endpoint for clients to poll for the next available task.\"\"\"\n            await self._check_and_requeue_stale_tasks()\n\n            if not self._store:\n                return TaskIfAny(is_available=False)\n\n            task = await self._store.get_next_task()\n            if task:\n                logger.debug(f\"Serving task {task.rollout_id} to a client.\")\n                return TaskIfAny(is_available=True, task=task)\n            else:\n                logger.debug(\"No task available for client.\")\n                return TaskIfAny(is_available=False)\n\n        @self._app.get(\"/resources/latest\", response_model=ResourcesUpdate)\n        async def fetch_latest_resources() -&gt; ResourcesUpdate:  # type: ignore\n            \"\"\"Endpoint for clients to poll for the latest available resources.\"\"\"\n            if not self._store:\n                raise HTTPException(status_code=503, detail=\"Server not fully initialized.\")\n            resources_update = await self._store.get_latest_resources()\n            if not resources_update:\n                raise HTTPException(status_code=404, detail=\"No resources have been set on the server.\")\n            logger.debug(f\"Serving latest resources '{resources_update.resources_id}' to a client.\")\n            return resources_update\n\n        @self._app.get(\"/resources/{resource_id}\", response_model=ResourcesUpdate)\n        async def fetch_resources_by_id(  # type: ignore\n            resource_id: str = Path(..., description=\"The unique identifier for the resource version.\")\n        ) -&gt; ResourcesUpdate:\n            \"\"\"Endpoint for clients to fetch a specific version of resources.\"\"\"\n            if not self._store:\n                raise HTTPException(status_code=503, detail=\"Server not fully initialized.\")\n            resources_update = await self._store.get_resources_by_id(resource_id)\n            if not resources_update:\n                raise HTTPException(status_code=404, detail=f\"Resource ID '{resource_id}' not found.\")\n            logger.debug(f\"Serving resources for ID '{resource_id}' to a client.\")\n            return resources_update\n\n        @self._app.post(\"/rollout\", response_model=GenericResponse)\n        async def post_rollout(payload: Rollout) -&gt; GenericResponse:  # type: ignore\n            \"\"\"Endpoint for clients to report a completed rollout.\"\"\"\n            if not self._store:\n                raise HTTPException(status_code=503, detail=\"Server not fully initialized.\")\n            await self._store.store_rollout(payload)\n            return GenericResponse(\n                status=\"ok\",\n                message=f\"Rollout {payload.rollout_id} received and stored.\",\n            )\n\n    async def start(self):\n        \"\"\"Starts the FastAPI server in the background.\"\"\"\n        logger.info(f\"Starting server at {self.endpoint}\")\n        asyncio.create_task(self._uvicorn_server.serve())\n        await asyncio.sleep(1)  # Allow time for server to start up.\n\n    async def stop(self):\n        \"\"\"Gracefully stops the running FastAPI server.\"\"\"\n        if self._uvicorn_server.started:\n            logger.info(\"Stopping server...\")\n            self._uvicorn_server.should_exit = True\n            await asyncio.sleep(1)  # Allow time for graceful shutdown.\n            logger.info(\"Server stopped.\")\n\n    async def run_forever(self):\n        \"\"\"\n        Runs the server indefinitely until stopped.\n        This is useful when async start and stop methods do not work.\n        \"\"\"\n        await self._uvicorn_server.serve()\n\n    async def queue_task(\n        self,\n        sample: Any,\n        mode: Literal[\"train\", \"val\", \"test\"] | None = None,\n        resources_id: str | None = None,\n        metadata: Dict[str, Any] | None = None,\n    ) -&gt; str:\n        \"\"\"\n        Adds a task to the queue for a client to process.\n        \"\"\"\n        if not self._store:\n            raise RuntimeError(\"Store not initialized. The server may not be running.\")\n        return await self._store.add_task(sample, mode=mode, resources_id=resources_id, metadata=metadata)\n\n    async def update_resources(self, resources: NamedResources) -&gt; str:\n        \"\"\"\n        Updates the resources, creating a new version and setting it as the latest.\n        \"\"\"\n        if not self._store:\n            raise RuntimeError(\"Store not initialized. The server may not be running.\")\n        resources_id = f\"res-{uuid.uuid4()}\"\n        update = ResourcesUpdate(resources_id=resources_id, resources=resources)\n        await self._store.update_resources(update)\n        return resources_id\n\n    async def get_completed_rollout(self, rollout_id: str) -&gt; Optional[Rollout]:\n        \"\"\"\n        Retrieves a specific completed rollout by its ID.\n        \"\"\"\n        if not self._store:\n            raise RuntimeError(\"Store not initialized. The server may not be running.\")\n        return await self._store.retrieve_rollout(rollout_id)\n\n    async def poll_completed_rollout(self, rollout_id: str, timeout: Optional[float] = None) -&gt; Optional[Rollout]:\n        \"\"\"\n        Polls for a completed rollout by its ID, waiting up to `timeout` seconds.\n        \"\"\"\n        start_time = time.time()\n        while True:\n            rollout = await self.get_completed_rollout(rollout_id)\n            if rollout:\n                return rollout\n            if timeout and (time.time() - start_time) &gt;= timeout:\n                return None\n            await asyncio.sleep(1)\n\n    async def retrieve_completed_rollouts(self) -&gt; List[Rollout]:\n        \"\"\"\n        Retrieves all available completed trajectories and clears the internal store.\n        \"\"\"\n        if not self._store:\n            raise RuntimeError(\"Store not initialized. The server may not be running.\")\n        return await self._store.retrieve_completed_rollouts()\n</code></pre>"},{"location":"reference/core/#agentlightning.server.AgentLightningServer.__init__","title":"<code>__init__(host='127.0.0.1', port=8000, task_timeout_seconds=300.0)</code>","text":"<p>Initializes the server controller.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The host to bind the server to.</p> <code>'127.0.0.1'</code> <code>port</code> <code>int</code> <p>The port to bind the server to.</p> <code>8000</code> <code>task_timeout_seconds</code> <code>float</code> <p>Time in seconds after which a claimed task is considered stale and requeued.</p> <code>300.0</code> Source code in <code>agentlightning/server.py</code> <pre><code>def __init__(self, host: str = \"127.0.0.1\", port: int = 8000, task_timeout_seconds: float = 300.0):\n    \"\"\"\n    Initializes the server controller.\n\n    Args:\n        host: The host to bind the server to.\n        port: The port to bind the server to.\n        task_timeout_seconds: Time in seconds after which a claimed task is considered stale and requeued.\n    \"\"\"\n    self.host = host\n    self.port = port\n    self.endpoint = f\"http://{host}:{port}\"\n    self._task_timeout_seconds = task_timeout_seconds\n\n    # Defer initialization and use event for cross-thread communication\n    self._store: Optional[ServerDataStore] = None\n    self.loop: Optional[asyncio.AbstractEventLoop] = None\n    self.startup_event = threading.Event()\n\n    # Create FastAPI app instance with a lifespan manager\n    self._app = FastAPI(lifespan=self._lifespan)\n    self._setup_routes()\n\n    self._uvicorn_config = uvicorn.Config(self._app, host=self.host, port=self.port, log_level=\"info\")\n    self._uvicorn_server = uvicorn.Server(self._uvicorn_config)\n</code></pre>"},{"location":"reference/core/#agentlightning.server.AgentLightningServer.get_completed_rollout","title":"<code>get_completed_rollout(rollout_id)</code>  <code>async</code>","text":"<p>Retrieves a specific completed rollout by its ID.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def get_completed_rollout(self, rollout_id: str) -&gt; Optional[Rollout]:\n    \"\"\"\n    Retrieves a specific completed rollout by its ID.\n    \"\"\"\n    if not self._store:\n        raise RuntimeError(\"Store not initialized. The server may not be running.\")\n    return await self._store.retrieve_rollout(rollout_id)\n</code></pre>"},{"location":"reference/core/#agentlightning.server.AgentLightningServer.poll_completed_rollout","title":"<code>poll_completed_rollout(rollout_id, timeout=None)</code>  <code>async</code>","text":"<p>Polls for a completed rollout by its ID, waiting up to <code>timeout</code> seconds.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def poll_completed_rollout(self, rollout_id: str, timeout: Optional[float] = None) -&gt; Optional[Rollout]:\n    \"\"\"\n    Polls for a completed rollout by its ID, waiting up to `timeout` seconds.\n    \"\"\"\n    start_time = time.time()\n    while True:\n        rollout = await self.get_completed_rollout(rollout_id)\n        if rollout:\n            return rollout\n        if timeout and (time.time() - start_time) &gt;= timeout:\n            return None\n        await asyncio.sleep(1)\n</code></pre>"},{"location":"reference/core/#agentlightning.server.AgentLightningServer.queue_task","title":"<code>queue_task(sample, mode=None, resources_id=None, metadata=None)</code>  <code>async</code>","text":"<p>Adds a task to the queue for a client to process.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def queue_task(\n    self,\n    sample: Any,\n    mode: Literal[\"train\", \"val\", \"test\"] | None = None,\n    resources_id: str | None = None,\n    metadata: Dict[str, Any] | None = None,\n) -&gt; str:\n    \"\"\"\n    Adds a task to the queue for a client to process.\n    \"\"\"\n    if not self._store:\n        raise RuntimeError(\"Store not initialized. The server may not be running.\")\n    return await self._store.add_task(sample, mode=mode, resources_id=resources_id, metadata=metadata)\n</code></pre>"},{"location":"reference/core/#agentlightning.server.AgentLightningServer.retrieve_completed_rollouts","title":"<code>retrieve_completed_rollouts()</code>  <code>async</code>","text":"<p>Retrieves all available completed trajectories and clears the internal store.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def retrieve_completed_rollouts(self) -&gt; List[Rollout]:\n    \"\"\"\n    Retrieves all available completed trajectories and clears the internal store.\n    \"\"\"\n    if not self._store:\n        raise RuntimeError(\"Store not initialized. The server may not be running.\")\n    return await self._store.retrieve_completed_rollouts()\n</code></pre>"},{"location":"reference/core/#agentlightning.server.AgentLightningServer.run_forever","title":"<code>run_forever()</code>  <code>async</code>","text":"<p>Runs the server indefinitely until stopped. This is useful when async start and stop methods do not work.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def run_forever(self):\n    \"\"\"\n    Runs the server indefinitely until stopped.\n    This is useful when async start and stop methods do not work.\n    \"\"\"\n    await self._uvicorn_server.serve()\n</code></pre>"},{"location":"reference/core/#agentlightning.server.AgentLightningServer.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Starts the FastAPI server in the background.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def start(self):\n    \"\"\"Starts the FastAPI server in the background.\"\"\"\n    logger.info(f\"Starting server at {self.endpoint}\")\n    asyncio.create_task(self._uvicorn_server.serve())\n    await asyncio.sleep(1)  # Allow time for server to start up.\n</code></pre>"},{"location":"reference/core/#agentlightning.server.AgentLightningServer.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Gracefully stops the running FastAPI server.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def stop(self):\n    \"\"\"Gracefully stops the running FastAPI server.\"\"\"\n    if self._uvicorn_server.started:\n        logger.info(\"Stopping server...\")\n        self._uvicorn_server.should_exit = True\n        await asyncio.sleep(1)  # Allow time for graceful shutdown.\n        logger.info(\"Server stopped.\")\n</code></pre>"},{"location":"reference/core/#agentlightning.server.AgentLightningServer.update_resources","title":"<code>update_resources(resources)</code>  <code>async</code>","text":"<p>Updates the resources, creating a new version and setting it as the latest.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def update_resources(self, resources: NamedResources) -&gt; str:\n    \"\"\"\n    Updates the resources, creating a new version and setting it as the latest.\n    \"\"\"\n    if not self._store:\n        raise RuntimeError(\"Store not initialized. The server may not be running.\")\n    resources_id = f\"res-{uuid.uuid4()}\"\n    update = ResourcesUpdate(resources_id=resources_id, resources=resources)\n    await self._store.update_resources(update)\n    return resources_id\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore","title":"<code>ServerDataStore</code>","text":"<p>A centralized, thread-safe, async, in-memory data store for the server's state. This holds the task queue, versioned resources, and completed rollouts.</p> Source code in <code>agentlightning/server.py</code> <pre><code>class ServerDataStore:\n    \"\"\"\n    A centralized, thread-safe, async, in-memory data store for the server's state.\n    This holds the task queue, versioned resources, and completed rollouts.\n    \"\"\"\n\n    def __init__(self):\n        self._task_queue: asyncio.Queue[Task] = asyncio.Queue()\n        self._processing_tasks: Dict[str, Task] = {}  # Currently processing tasks\n        self._completed_rollouts: Dict[str, Rollout] = {}\n\n        # Store for versioned resources\n        self._resource_versions: Dict[str, NamedResources] = {}\n        self._latest_resources_id: Optional[str] = None\n\n        # Locks for thread-safe access\n        self._results_lock = asyncio.Lock()\n        self._resources_lock = asyncio.Lock()\n\n    async def add_task(\n        self,\n        sample: Any,\n        mode: Literal[\"train\", \"val\", \"test\"] | None = None,\n        resources_id: str | None = None,\n        metadata: Dict[str, Any] | None = None,\n    ) -&gt; str:\n        \"\"\"\n        Adds a new task to the queue with specific metadata and returns its unique ID.\n        \"\"\"\n        rollout_id = f\"rollout-{uuid.uuid4()}\"\n        task = Task(\n            rollout_id=rollout_id,\n            input=sample,\n            mode=mode,\n            resources_id=resources_id,\n            create_time=time.time(),\n            num_claims=0,\n            metadata=metadata or {},\n        )\n        await self._task_queue.put(task)\n        logger.info(f\"Task queued: {rollout_id} (mode: {mode}, resources_id: {resources_id})\")\n        return rollout_id\n\n    async def get_next_task(self) -&gt; Optional[Task]:\n        \"\"\"\n        Retrieves the next task from the queue without blocking.\n        Returns None if the queue is empty.\n        \"\"\"\n        try:\n            async with self._results_lock:\n                task = self._task_queue.get_nowait()\n                task = task.model_copy(\n                    update={\n                        \"last_claim_time\": time.time(),\n                        \"num_claims\": (task.num_claims or 0) + 1,\n                    }\n                )\n                self._processing_tasks[task.rollout_id] = task\n                if task.num_claims == 1:\n                    logger.debug(f\"Next task retrieved: {task.rollout_id}\")\n                else:\n                    logger.info(f\"Task {task.rollout_id} re-claimed (attempt {task.num_claims})\")\n                return task\n        except asyncio.QueueEmpty:\n            return None\n\n    async def update_resources(self, update: ResourcesUpdate):\n        \"\"\"\n        Safely stores a new version of named resources and sets it as the latest.\n        \"\"\"\n        # TODO: evict old resources if necessary.\n        async with self._resources_lock:\n            self._resource_versions[update.resources_id] = update.resources\n            self._latest_resources_id = update.resources_id\n            logger.info(f\"Resources updated. New version '{update.resources_id}' is now latest.\")\n\n    async def get_resources_by_id(self, resources_id: str) -&gt; Optional[ResourcesUpdate]:\n        \"\"\"\n        Safely retrieves a specific version of named resources by its ID.\n        \"\"\"\n        async with self._resources_lock:\n            resources = self._resource_versions.get(resources_id)\n            if resources:\n                return ResourcesUpdate(resources_id=resources_id, resources=resources)\n            return None\n\n    async def get_latest_resources(self) -&gt; Optional[ResourcesUpdate]:\n        \"\"\"\n        Safely retrieves the latest version of named resources.\n        \"\"\"\n        if self._latest_resources_id:\n            return await self.get_resources_by_id(self._latest_resources_id)\n        return None\n\n    async def store_rollout(self, rollout: Rollout):\n        \"\"\"\n        Safely stores a completed rollout from a client.\n        \"\"\"\n        async with self._results_lock:\n            self._processing_tasks.pop(rollout.rollout_id, None)\n            self._completed_rollouts[rollout.rollout_id] = rollout\n            logger.info(f\"Rollout received and stored: {rollout.rollout_id}\")\n\n    async def retrieve_rollout(self, rollout_id: str) -&gt; Optional[Rollout]:\n        \"\"\"\n        Safely retrieves a single rollout by its ID, removing it from the store.\n        \"\"\"\n        async with self._results_lock:\n            return self._completed_rollouts.pop(rollout_id, None)\n\n    async def retrieve_completed_rollouts(self) -&gt; List[Rollout]:\n        \"\"\"\n        Retrieves all completed rollouts and clears the store.\n        \"\"\"\n        async with self._results_lock:\n            rollouts = list(self._completed_rollouts.values())\n            self._completed_rollouts.clear()\n            return rollouts\n\n    def get_processing_tasks(self) -&gt; Dict[str, Task]:\n        \"\"\"Returns a copy of currently processing tasks for timeout checking.\"\"\"\n        return self._processing_tasks.copy()\n\n    async def requeue_task(self, task: Task):\n        \"\"\"Requeues a task that has timed out and removes it from processing.\"\"\"\n        logger.warning(f\"Requeuing task {task.rollout_id} after timeout (attempt {task.num_claims})\")\n        async with self._results_lock:\n            # Remove from processing tasks\n            self._processing_tasks.pop(task.rollout_id, None)\n            self._task_queue.put_nowait(task)\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore.add_task","title":"<code>add_task(sample, mode=None, resources_id=None, metadata=None)</code>  <code>async</code>","text":"<p>Adds a new task to the queue with specific metadata and returns its unique ID.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def add_task(\n    self,\n    sample: Any,\n    mode: Literal[\"train\", \"val\", \"test\"] | None = None,\n    resources_id: str | None = None,\n    metadata: Dict[str, Any] | None = None,\n) -&gt; str:\n    \"\"\"\n    Adds a new task to the queue with specific metadata and returns its unique ID.\n    \"\"\"\n    rollout_id = f\"rollout-{uuid.uuid4()}\"\n    task = Task(\n        rollout_id=rollout_id,\n        input=sample,\n        mode=mode,\n        resources_id=resources_id,\n        create_time=time.time(),\n        num_claims=0,\n        metadata=metadata or {},\n    )\n    await self._task_queue.put(task)\n    logger.info(f\"Task queued: {rollout_id} (mode: {mode}, resources_id: {resources_id})\")\n    return rollout_id\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore.get_latest_resources","title":"<code>get_latest_resources()</code>  <code>async</code>","text":"<p>Safely retrieves the latest version of named resources.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def get_latest_resources(self) -&gt; Optional[ResourcesUpdate]:\n    \"\"\"\n    Safely retrieves the latest version of named resources.\n    \"\"\"\n    if self._latest_resources_id:\n        return await self.get_resources_by_id(self._latest_resources_id)\n    return None\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore.get_next_task","title":"<code>get_next_task()</code>  <code>async</code>","text":"<p>Retrieves the next task from the queue without blocking. Returns None if the queue is empty.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def get_next_task(self) -&gt; Optional[Task]:\n    \"\"\"\n    Retrieves the next task from the queue without blocking.\n    Returns None if the queue is empty.\n    \"\"\"\n    try:\n        async with self._results_lock:\n            task = self._task_queue.get_nowait()\n            task = task.model_copy(\n                update={\n                    \"last_claim_time\": time.time(),\n                    \"num_claims\": (task.num_claims or 0) + 1,\n                }\n            )\n            self._processing_tasks[task.rollout_id] = task\n            if task.num_claims == 1:\n                logger.debug(f\"Next task retrieved: {task.rollout_id}\")\n            else:\n                logger.info(f\"Task {task.rollout_id} re-claimed (attempt {task.num_claims})\")\n            return task\n    except asyncio.QueueEmpty:\n        return None\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore.get_processing_tasks","title":"<code>get_processing_tasks()</code>","text":"<p>Returns a copy of currently processing tasks for timeout checking.</p> Source code in <code>agentlightning/server.py</code> <pre><code>def get_processing_tasks(self) -&gt; Dict[str, Task]:\n    \"\"\"Returns a copy of currently processing tasks for timeout checking.\"\"\"\n    return self._processing_tasks.copy()\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore.get_resources_by_id","title":"<code>get_resources_by_id(resources_id)</code>  <code>async</code>","text":"<p>Safely retrieves a specific version of named resources by its ID.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def get_resources_by_id(self, resources_id: str) -&gt; Optional[ResourcesUpdate]:\n    \"\"\"\n    Safely retrieves a specific version of named resources by its ID.\n    \"\"\"\n    async with self._resources_lock:\n        resources = self._resource_versions.get(resources_id)\n        if resources:\n            return ResourcesUpdate(resources_id=resources_id, resources=resources)\n        return None\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore.requeue_task","title":"<code>requeue_task(task)</code>  <code>async</code>","text":"<p>Requeues a task that has timed out and removes it from processing.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def requeue_task(self, task: Task):\n    \"\"\"Requeues a task that has timed out and removes it from processing.\"\"\"\n    logger.warning(f\"Requeuing task {task.rollout_id} after timeout (attempt {task.num_claims})\")\n    async with self._results_lock:\n        # Remove from processing tasks\n        self._processing_tasks.pop(task.rollout_id, None)\n        self._task_queue.put_nowait(task)\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore.retrieve_completed_rollouts","title":"<code>retrieve_completed_rollouts()</code>  <code>async</code>","text":"<p>Retrieves all completed rollouts and clears the store.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def retrieve_completed_rollouts(self) -&gt; List[Rollout]:\n    \"\"\"\n    Retrieves all completed rollouts and clears the store.\n    \"\"\"\n    async with self._results_lock:\n        rollouts = list(self._completed_rollouts.values())\n        self._completed_rollouts.clear()\n        return rollouts\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore.retrieve_rollout","title":"<code>retrieve_rollout(rollout_id)</code>  <code>async</code>","text":"<p>Safely retrieves a single rollout by its ID, removing it from the store.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def retrieve_rollout(self, rollout_id: str) -&gt; Optional[Rollout]:\n    \"\"\"\n    Safely retrieves a single rollout by its ID, removing it from the store.\n    \"\"\"\n    async with self._results_lock:\n        return self._completed_rollouts.pop(rollout_id, None)\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore.store_rollout","title":"<code>store_rollout(rollout)</code>  <code>async</code>","text":"<p>Safely stores a completed rollout from a client.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def store_rollout(self, rollout: Rollout):\n    \"\"\"\n    Safely stores a completed rollout from a client.\n    \"\"\"\n    async with self._results_lock:\n        self._processing_tasks.pop(rollout.rollout_id, None)\n        self._completed_rollouts[rollout.rollout_id] = rollout\n        logger.info(f\"Rollout received and stored: {rollout.rollout_id}\")\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore.update_resources","title":"<code>update_resources(update)</code>  <code>async</code>","text":"<p>Safely stores a new version of named resources and sets it as the latest.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def update_resources(self, update: ResourcesUpdate):\n    \"\"\"\n    Safely stores a new version of named resources and sets it as the latest.\n    \"\"\"\n    # TODO: evict old resources if necessary.\n    async with self._resources_lock:\n        self._resource_versions[update.resources_id] = update.resources\n        self._latest_resources_id = update.resources_id\n        logger.info(f\"Resources updated. New version '{update.resources_id}' is now latest.\")\n</code></pre>"},{"location":"reference/core/#utilities","title":"Utilities","text":""},{"location":"reference/core/#agentlightning.config","title":"<code>agentlightning.config</code>","text":"<p>This file is not carefully reviewed. It might contain unintentional bugs and issues. Please always review the parsed construction arguments before using them.</p>"},{"location":"reference/core/#agentlightning.config.lightning_cli","title":"<code>lightning_cli(*classes)</code>","text":"<pre><code>lightning_cli(cls1: Type[_C1]) -&gt; _C1\n</code></pre><pre><code>lightning_cli(\n    cls1: Type[_C1], cls2: Type[_C2]\n) -&gt; Tuple[_C1, _C2]\n</code></pre><pre><code>lightning_cli(\n    cls1: Type[_C1], cls2: Type[_C2], cls3: Type[_C3]\n) -&gt; Tuple[_C1, _C2, _C3]\n</code></pre><pre><code>lightning_cli(\n    cls1: Type[_C1],\n    cls2: Type[_C2],\n    cls3: Type[_C3],\n    cls4: Type[_C4],\n) -&gt; Tuple[_C1, _C2, _C3, _C4]\n</code></pre><pre><code>lightning_cli(\n    *classes: Type[CliConfigurable],\n) -&gt; Tuple[CliConfigurable, ...]\n</code></pre> <p>Parses command-line arguments to configure and instantiate provided CliConfigurable classes.</p> <p>Parameters:</p> Name Type Description Default <code>*classes</code> <code>Type[CliConfigurable]</code> <p>One or more classes that inherit from CliConfigurable. Each class's       init parameters will be exposed as command-line arguments.</p> <code>()</code> <p>Returns:</p> Type Description <code>CliConfigurable | Tuple[CliConfigurable, ...]</code> <p>A tuple of instantiated objects, corresponding to the input classes in order.</p> Source code in <code>agentlightning/config.py</code> <pre><code>def lightning_cli(*classes: Type[CliConfigurable]) -&gt; CliConfigurable | Tuple[CliConfigurable, ...]:  # type: ignore\n    \"\"\"\n    Parses command-line arguments to configure and instantiate provided CliConfigurable classes.\n\n    Args:\n        *classes: One or more classes that inherit from CliConfigurable. Each class's\n                  __init__ parameters will be exposed as command-line arguments.\n\n    Returns:\n        A tuple of instantiated objects, corresponding to the input classes in order.\n    \"\"\"\n    if not classes:\n        return tuple()  # Return an empty tuple if no classes are provided\n\n    parser = _create_argument_parser()\n\n    # This map will store {cls: {init_param_name: argparse_dest_name}}\n    class_arg_configs_maps: Dict[Type[CliConfigurable], Dict[str, str]] = {}\n\n    for cls in classes:\n        _add_arguments_for_class(parser, cls, class_arg_configs_maps)\n\n    parsed_args = parser.parse_args()  # Uses sys.argv[1:] by default\n\n    # Correctly handle single class case for return type matching overloads\n    instances = _instantiate_classes(parsed_args, classes, class_arg_configs_maps)\n    if len(classes) == 1:\n        return instances[0]\n    return instances\n</code></pre>"},{"location":"reference/core/#agentlightning.config.nullable_float","title":"<code>nullable_float(value)</code>","text":"<p>Converts specific string values (case-insensitive) to None, otherwise returns the float.</p> Source code in <code>agentlightning/config.py</code> <pre><code>def nullable_float(value: str) -&gt; float | None:\n    \"\"\"Converts specific string values (case-insensitive) to None, otherwise returns the float.\"\"\"\n    if value.lower() in [\"none\", \"null\", \"~\", \"nil\"]:  # Define keywords for None\n        return None\n    try:\n        return float(value)\n    except ValueError:\n        raise argparse.ArgumentTypeError(f\"Invalid float value: '{value}'\")\n</code></pre>"},{"location":"reference/core/#agentlightning.config.nullable_int","title":"<code>nullable_int(value)</code>","text":"<p>Converts specific string values (case-insensitive) to None, otherwise returns the integer.</p> Source code in <code>agentlightning/config.py</code> <pre><code>def nullable_int(value: str) -&gt; int | None:\n    \"\"\"Converts specific string values (case-insensitive) to None, otherwise returns the integer.\"\"\"\n    if value.lower() in [\"none\", \"null\", \"~\", \"nil\"]:  # Define keywords for None\n        return None\n    try:\n        return int(value)\n    except ValueError:\n        raise argparse.ArgumentTypeError(f\"Invalid integer value: '{value}'\")\n</code></pre>"},{"location":"reference/core/#agentlightning.config.nullable_str","title":"<code>nullable_str(value)</code>","text":"<p>Converts specific string values (case-insensitive) to None, otherwise returns the string.</p> Source code in <code>agentlightning/config.py</code> <pre><code>def nullable_str(value: str) -&gt; str | None:\n    \"\"\"Converts specific string values (case-insensitive) to None, otherwise returns the string.\"\"\"\n    if value.lower() in [\"none\", \"null\", \"~\", \"nil\"]:  # Define keywords for None\n        return None\n    return value\n</code></pre>"},{"location":"reference/core/#agentlightning.types","title":"<code>agentlightning.types</code>","text":""},{"location":"reference/core/#agentlightning.types.NamedResources","title":"<code>NamedResources = Dict[str, ResourceUnion]</code>  <code>module-attribute</code>","text":"<p>A dictionary-like class to hold named resources.</p> Example <p>resources: NamedResources = {     'main_llm': LLM(         endpoint=\"http://localhost:8080\",         model=\"llama3\",         sampling_parameters={'temperature': 0.7, 'max_tokens': 100}     ),     'system_prompt': PromptTemplate(         template=\"You are a helpful assistant.\",         engine='f-string'     ) }</p>"},{"location":"reference/core/#agentlightning.types.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>Protocol</code>, <code>Generic[T_co]</code></p> <p>The general interface for a dataset.</p> <p>It's currently implemented as a protocol, having a similar interface to torch.utils.data.Dataset. You don't have to inherit from this class; you can use a simple list if you want to.</p> Source code in <code>agentlightning/types.py</code> <pre><code>class Dataset(Protocol, Generic[T_co]):\n    \"\"\"The general interface for a dataset.\n\n    It's currently implemented as a protocol, having a similar interface to torch.utils.data.Dataset.\n    You don't have to inherit from this class; you can use a simple list if you want to.\n    \"\"\"\n\n    def __getitem__(self, index: int) -&gt; T_co: ...\n\n    def __len__(self) -&gt; int: ...\n</code></pre>"},{"location":"reference/core/#agentlightning.types.GenericResponse","title":"<code>GenericResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A generic response message that can be used for various purposes.</p> Source code in <code>agentlightning/types.py</code> <pre><code>class GenericResponse(BaseModel):\n    \"\"\"\n    A generic response message that can be used for various purposes.\n    \"\"\"\n\n    status: str = \"success\"\n    message: Optional[str] = None\n    data: Optional[Dict[str, Any]] = None\n</code></pre>"},{"location":"reference/core/#agentlightning.types.LLM","title":"<code>LLM</code>","text":"<p>               Bases: <code>Resource</code></p> <p>Provide an LLM endpoint and model name as a resource.</p> <p>Attributes:</p> Name Type Description <code>endpoint</code> <code>str</code> <p>The URL of the LLM API endpoint.</p> <code>model</code> <code>str</code> <p>The identifier for the model to be used (e.g., 'gpt-4o').</p> <code>sampling_parameters</code> <code>SamplingParameters</code> <p>A dictionary of hyperparameters for model inference, such as temperature, top_p, etc.</p> Source code in <code>agentlightning/types.py</code> <pre><code>class LLM(Resource):\n    \"\"\"\n    Provide an LLM endpoint and model name as a resource.\n\n    Attributes:\n        endpoint (str): The URL of the LLM API endpoint.\n        model (str): The identifier for the model to be used (e.g., 'gpt-4o').\n        sampling_parameters (SamplingParameters): A dictionary of hyperparameters\n            for model inference, such as temperature, top_p, etc.\n    \"\"\"\n\n    resource_type: Literal[\"llm\"] = \"llm\"\n    endpoint: str\n    model: str\n    sampling_parameters: Dict[str, Any] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/core/#agentlightning.types.ParallelWorkerBase","title":"<code>ParallelWorkerBase</code>","text":"<p>Base class for objects that can be parallelized across multiple worker processes.</p> <p>This class defines the standard lifecycle for parallel processing:</p> Main Process <ol> <li>init() - Initialize the object in the main process</li> <li>spawn workers and call init_worker() in each worker</li> <li>run() - Execute the main workload in parallel across workers</li> <li>teardown_worker() - Clean up resources in each worker</li> <li>teardown() - Final cleanup in the main process</li> </ol> <p>Subclasses should implement the run() method and optionally override the lifecycle methods for custom initialization and cleanup behavior.</p> Source code in <code>agentlightning/types.py</code> <pre><code>class ParallelWorkerBase:\n    \"\"\"Base class for objects that can be parallelized across multiple worker processes.\n\n    This class defines the standard lifecycle for parallel processing:\n\n    Main Process:\n        1. init() - Initialize the object in the main process\n        2. spawn workers and call init_worker() in each worker\n        3. run() - Execute the main workload in parallel across workers\n        4. teardown_worker() - Clean up resources in each worker\n        5. teardown() - Final cleanup in the main process\n\n    Subclasses should implement the run() method and optionally override\n    the lifecycle methods for custom initialization and cleanup behavior.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the base class. This method can be overridden by subclasses.\"\"\"\n        self.worker_id: Optional[int] = None\n\n    def init(self, *args: Any, **kwargs: Any) -&gt; None:\n        pass\n\n    def init_worker(self, worker_id: int, *args: Any, **kwargs: Any) -&gt; None:\n        self.worker_id = worker_id\n\n    def run(self, *args: Any, **kwargs: Any) -&gt; Any:\n        pass\n\n    def teardown_worker(self, worker_id: int, *args: Any, **kwargs: Any) -&gt; None:\n        pass\n\n    def teardown(self, *args: Any, **kwargs: Any) -&gt; None:\n        pass\n</code></pre>"},{"location":"reference/core/#agentlightning.types.ParallelWorkerBase.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the base class. This method can be overridden by subclasses.</p> Source code in <code>agentlightning/types.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the base class. This method can be overridden by subclasses.\"\"\"\n    self.worker_id: Optional[int] = None\n</code></pre>"},{"location":"reference/core/#agentlightning.types.PromptTemplate","title":"<code>PromptTemplate</code>","text":"<p>               Bases: <code>Resource</code></p> <p>A prompt template as a resource.</p> <p>Attributes:</p> Name Type Description <code>template</code> <code>str</code> <p>The template string. The format depends on the engine.</p> <code>engine</code> <code>Literal['jinja', 'f-string', 'poml']</code> <p>The templating engine to use for rendering the prompt. I imagine users can use their own customized engines, but algos can only well operate on a subset of them.</p> Source code in <code>agentlightning/types.py</code> <pre><code>class PromptTemplate(Resource):\n    \"\"\"\n    A prompt template as a resource.\n\n    Attributes:\n        template (str): The template string. The format depends on the engine.\n        engine (Literal['jinja', 'f-string', 'poml']): The templating engine\n            to use for rendering the prompt. I imagine users can use their own\n            customized engines, but algos can only well operate on a subset of them.\n    \"\"\"\n\n    resource_type: Literal[\"prompt_template\"] = \"prompt_template\"\n    template: str\n    engine: Literal[\"jinja\", \"f-string\", \"poml\"]\n</code></pre>"},{"location":"reference/core/#agentlightning.types.Resource","title":"<code>Resource</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for all tunable resources.</p> Source code in <code>agentlightning/types.py</code> <pre><code>class Resource(BaseModel):\n    \"\"\"\n    Base class for all tunable resources.\n    \"\"\"\n\n    resource_type: Any\n</code></pre>"},{"location":"reference/core/#agentlightning.types.ResourcesUpdate","title":"<code>ResourcesUpdate</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A resource update message to be sent from the server to clients.</p> <p>This message contains a dictionary of resources that clients should use for subsequent tasks. It is used to update the resources available to clients dynamically.</p> Source code in <code>agentlightning/types.py</code> <pre><code>class ResourcesUpdate(BaseModel):\n    \"\"\"\n    A resource update message to be sent from the server to clients.\n\n    This message contains a dictionary of resources that clients should use\n    for subsequent tasks. It is used to update the resources available to\n    clients dynamically.\n    \"\"\"\n\n    resources_id: str\n    resources: NamedResources\n</code></pre>"},{"location":"reference/core/#agentlightning.types.Rollout","title":"<code>Rollout</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The standard reporting object from client to server.</p> Source code in <code>agentlightning/types.py</code> <pre><code>class Rollout(BaseModel):\n    \"\"\"The standard reporting object from client to server.\"\"\"\n\n    rollout_id: str\n\n    # Echoing the input task\n    task: Optional[Task] = None\n\n    # Primary, high-level feedback\n    final_reward: Optional[float] = None\n\n    # Structured, sequential feedback for RL-style optimization\n    triplets: Optional[List[Triplet]] = None\n\n    # Optional, rich-context data for deep analysis\n    trace: Optional[List[Dict[str, Any]]] = Field(\n        default=None,\n        description=\"A list of spans that conform to the OpenTelemetry JSON format. \"\n        \"Users of the opentelemetry-sdk can generate this by calling \"\n        \"json.loads(readable_span.to_json()).\",\n    )\n    logs: Optional[List[str]] = None\n\n    # A bucket for any other relevant information\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/core/#agentlightning.types.Task","title":"<code>Task</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A task (rollout request) to be processed by the client agent.</p> Source code in <code>agentlightning/types.py</code> <pre><code>class Task(BaseModel):\n    \"\"\"A task (rollout request) to be processed by the client agent.\"\"\"\n\n    rollout_id: str\n    input: TaskInput\n\n    mode: Optional[Literal[\"train\", \"val\", \"test\"]] = None\n    resources_id: Optional[str] = None\n\n    # Optional fields for tracking task lifecycle\n    create_time: Optional[float] = None\n    last_claim_time: Optional[float] = None\n    num_claims: Optional[int] = None\n\n    # Allow additional metadata fields\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/core/#agentlightning.types.Triplet","title":"<code>Triplet</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A standard structure for a single turn in a trajectory.</p> Source code in <code>agentlightning/types.py</code> <pre><code>class Triplet(BaseModel):\n    \"\"\"A standard structure for a single turn in a trajectory.\"\"\"\n\n    prompt: Any\n    response: Any\n    reward: Optional[float] = None\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/core/#agentlightning.logging","title":"<code>agentlightning.logging</code>","text":""},{"location":"reference/core/#agentlightning.instrumentation","title":"<code>agentlightning.instrumentation</code>","text":""},{"location":"reference/rl/","title":"Reinforcement Learning API","text":""},{"location":"reference/rl/#agentlightning.verl","title":"<code>agentlightning.verl</code>","text":""},{"location":"reference/rl/#agentlightning.verl.NamedResources","title":"<code>NamedResources = Dict[str, ResourceUnion]</code>  <code>module-attribute</code>","text":"<p>A dictionary-like class to hold named resources.</p> Example <p>resources: NamedResources = {     'main_llm': LLM(         endpoint=\"http://localhost:8080\",         model=\"llama3\",         sampling_parameters={'temperature': 0.7, 'max_tokens': 100}     ),     'system_prompt': PromptTemplate(         template=\"You are a helpful assistant.\",         engine='f-string'     ) }</p>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningServer","title":"<code>AgentLightningServer</code>","text":"<p>The main SDK class for developers to control the Agent Lightning Server.</p> <p>This class manages the server lifecycle, task queueing, resources updates, and retrieval of results, providing a simple interface for the optimization logic.</p> Source code in <code>agentlightning/server.py</code> <pre><code>class AgentLightningServer:\n    \"\"\"\n    The main SDK class for developers to control the Agent Lightning Server.\n\n    This class manages the server lifecycle, task queueing, resources updates,\n    and retrieval of results, providing a simple interface for the optimization logic.\n    \"\"\"\n\n    def __init__(self, host: str = \"127.0.0.1\", port: int = 8000, task_timeout_seconds: float = 300.0):\n        \"\"\"\n        Initializes the server controller.\n\n        Args:\n            host: The host to bind the server to.\n            port: The port to bind the server to.\n            task_timeout_seconds: Time in seconds after which a claimed task is considered stale and requeued.\n        \"\"\"\n        self.host = host\n        self.port = port\n        self.endpoint = f\"http://{host}:{port}\"\n        self._task_timeout_seconds = task_timeout_seconds\n\n        # Defer initialization and use event for cross-thread communication\n        self._store: Optional[ServerDataStore] = None\n        self.loop: Optional[asyncio.AbstractEventLoop] = None\n        self.startup_event = threading.Event()\n\n        # Create FastAPI app instance with a lifespan manager\n        self._app = FastAPI(lifespan=self._lifespan)\n        self._setup_routes()\n\n        self._uvicorn_config = uvicorn.Config(self._app, host=self.host, port=self.port, log_level=\"info\")\n        self._uvicorn_server = uvicorn.Server(self._uvicorn_config)\n\n    # --- ADDED: Lifespan context manager ---\n    @asynccontextmanager\n    async def _lifespan(self, app: FastAPI):\n        \"\"\"\n        Manages server startup and shutdown. This runs inside the server's event loop.\n        \"\"\"\n        logger.info(\"Server is starting up...\")\n        self.loop = asyncio.get_running_loop()\n        self._store = ServerDataStore()  # Initialize data store here\n        self.startup_event.set()  # Signal that the server is ready\n\n        yield\n\n        logger.info(\"Server is shutting down.\")\n        self._store = None\n        self.startup_event.clear()  # Clear the startup event\n        self.loop = None\n\n    async def _check_and_requeue_stale_tasks(self):\n        \"\"\"\n        Check for stale tasks and requeue them. Called reactively during get_next_task.\n        \"\"\"\n        current_time = time.time()\n        # Ensure store is initialized before checking\n        if not self._store:\n            return\n        processing_tasks = self._store.get_processing_tasks()\n\n        for _, task in processing_tasks.items():\n            if task.last_claim_time and current_time - task.last_claim_time &gt; self._task_timeout_seconds:\n                await self._store.requeue_task(task)\n                logger.warning(\n                    f\"Task {task.rollout_id} timed out after {self._task_timeout_seconds}s, requeued (attempt {task.num_claims})\"\n                )\n\n    def _setup_routes(self):\n        \"\"\"Setup FastAPI routes.\"\"\"\n\n        @self._app.get(\"/task\", response_model=TaskIfAny)\n        async def next_task() -&gt; TaskIfAny:  # type: ignore\n            \"\"\"Endpoint for clients to poll for the next available task.\"\"\"\n            await self._check_and_requeue_stale_tasks()\n\n            if not self._store:\n                return TaskIfAny(is_available=False)\n\n            task = await self._store.get_next_task()\n            if task:\n                logger.debug(f\"Serving task {task.rollout_id} to a client.\")\n                return TaskIfAny(is_available=True, task=task)\n            else:\n                logger.debug(\"No task available for client.\")\n                return TaskIfAny(is_available=False)\n\n        @self._app.get(\"/resources/latest\", response_model=ResourcesUpdate)\n        async def fetch_latest_resources() -&gt; ResourcesUpdate:  # type: ignore\n            \"\"\"Endpoint for clients to poll for the latest available resources.\"\"\"\n            if not self._store:\n                raise HTTPException(status_code=503, detail=\"Server not fully initialized.\")\n            resources_update = await self._store.get_latest_resources()\n            if not resources_update:\n                raise HTTPException(status_code=404, detail=\"No resources have been set on the server.\")\n            logger.debug(f\"Serving latest resources '{resources_update.resources_id}' to a client.\")\n            return resources_update\n\n        @self._app.get(\"/resources/{resource_id}\", response_model=ResourcesUpdate)\n        async def fetch_resources_by_id(  # type: ignore\n            resource_id: str = Path(..., description=\"The unique identifier for the resource version.\")\n        ) -&gt; ResourcesUpdate:\n            \"\"\"Endpoint for clients to fetch a specific version of resources.\"\"\"\n            if not self._store:\n                raise HTTPException(status_code=503, detail=\"Server not fully initialized.\")\n            resources_update = await self._store.get_resources_by_id(resource_id)\n            if not resources_update:\n                raise HTTPException(status_code=404, detail=f\"Resource ID '{resource_id}' not found.\")\n            logger.debug(f\"Serving resources for ID '{resource_id}' to a client.\")\n            return resources_update\n\n        @self._app.post(\"/rollout\", response_model=GenericResponse)\n        async def post_rollout(payload: Rollout) -&gt; GenericResponse:  # type: ignore\n            \"\"\"Endpoint for clients to report a completed rollout.\"\"\"\n            if not self._store:\n                raise HTTPException(status_code=503, detail=\"Server not fully initialized.\")\n            await self._store.store_rollout(payload)\n            return GenericResponse(\n                status=\"ok\",\n                message=f\"Rollout {payload.rollout_id} received and stored.\",\n            )\n\n    async def start(self):\n        \"\"\"Starts the FastAPI server in the background.\"\"\"\n        logger.info(f\"Starting server at {self.endpoint}\")\n        asyncio.create_task(self._uvicorn_server.serve())\n        await asyncio.sleep(1)  # Allow time for server to start up.\n\n    async def stop(self):\n        \"\"\"Gracefully stops the running FastAPI server.\"\"\"\n        if self._uvicorn_server.started:\n            logger.info(\"Stopping server...\")\n            self._uvicorn_server.should_exit = True\n            await asyncio.sleep(1)  # Allow time for graceful shutdown.\n            logger.info(\"Server stopped.\")\n\n    async def run_forever(self):\n        \"\"\"\n        Runs the server indefinitely until stopped.\n        This is useful when async start and stop methods do not work.\n        \"\"\"\n        await self._uvicorn_server.serve()\n\n    async def queue_task(\n        self,\n        sample: Any,\n        mode: Literal[\"train\", \"val\", \"test\"] | None = None,\n        resources_id: str | None = None,\n        metadata: Dict[str, Any] | None = None,\n    ) -&gt; str:\n        \"\"\"\n        Adds a task to the queue for a client to process.\n        \"\"\"\n        if not self._store:\n            raise RuntimeError(\"Store not initialized. The server may not be running.\")\n        return await self._store.add_task(sample, mode=mode, resources_id=resources_id, metadata=metadata)\n\n    async def update_resources(self, resources: NamedResources) -&gt; str:\n        \"\"\"\n        Updates the resources, creating a new version and setting it as the latest.\n        \"\"\"\n        if not self._store:\n            raise RuntimeError(\"Store not initialized. The server may not be running.\")\n        resources_id = f\"res-{uuid.uuid4()}\"\n        update = ResourcesUpdate(resources_id=resources_id, resources=resources)\n        await self._store.update_resources(update)\n        return resources_id\n\n    async def get_completed_rollout(self, rollout_id: str) -&gt; Optional[Rollout]:\n        \"\"\"\n        Retrieves a specific completed rollout by its ID.\n        \"\"\"\n        if not self._store:\n            raise RuntimeError(\"Store not initialized. The server may not be running.\")\n        return await self._store.retrieve_rollout(rollout_id)\n\n    async def poll_completed_rollout(self, rollout_id: str, timeout: Optional[float] = None) -&gt; Optional[Rollout]:\n        \"\"\"\n        Polls for a completed rollout by its ID, waiting up to `timeout` seconds.\n        \"\"\"\n        start_time = time.time()\n        while True:\n            rollout = await self.get_completed_rollout(rollout_id)\n            if rollout:\n                return rollout\n            if timeout and (time.time() - start_time) &gt;= timeout:\n                return None\n            await asyncio.sleep(1)\n\n    async def retrieve_completed_rollouts(self) -&gt; List[Rollout]:\n        \"\"\"\n        Retrieves all available completed trajectories and clears the internal store.\n        \"\"\"\n        if not self._store:\n            raise RuntimeError(\"Store not initialized. The server may not be running.\")\n        return await self._store.retrieve_completed_rollouts()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningServer.__init__","title":"<code>__init__(host='127.0.0.1', port=8000, task_timeout_seconds=300.0)</code>","text":"<p>Initializes the server controller.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The host to bind the server to.</p> <code>'127.0.0.1'</code> <code>port</code> <code>int</code> <p>The port to bind the server to.</p> <code>8000</code> <code>task_timeout_seconds</code> <code>float</code> <p>Time in seconds after which a claimed task is considered stale and requeued.</p> <code>300.0</code> Source code in <code>agentlightning/server.py</code> <pre><code>def __init__(self, host: str = \"127.0.0.1\", port: int = 8000, task_timeout_seconds: float = 300.0):\n    \"\"\"\n    Initializes the server controller.\n\n    Args:\n        host: The host to bind the server to.\n        port: The port to bind the server to.\n        task_timeout_seconds: Time in seconds after which a claimed task is considered stale and requeued.\n    \"\"\"\n    self.host = host\n    self.port = port\n    self.endpoint = f\"http://{host}:{port}\"\n    self._task_timeout_seconds = task_timeout_seconds\n\n    # Defer initialization and use event for cross-thread communication\n    self._store: Optional[ServerDataStore] = None\n    self.loop: Optional[asyncio.AbstractEventLoop] = None\n    self.startup_event = threading.Event()\n\n    # Create FastAPI app instance with a lifespan manager\n    self._app = FastAPI(lifespan=self._lifespan)\n    self._setup_routes()\n\n    self._uvicorn_config = uvicorn.Config(self._app, host=self.host, port=self.port, log_level=\"info\")\n    self._uvicorn_server = uvicorn.Server(self._uvicorn_config)\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningServer.get_completed_rollout","title":"<code>get_completed_rollout(rollout_id)</code>  <code>async</code>","text":"<p>Retrieves a specific completed rollout by its ID.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def get_completed_rollout(self, rollout_id: str) -&gt; Optional[Rollout]:\n    \"\"\"\n    Retrieves a specific completed rollout by its ID.\n    \"\"\"\n    if not self._store:\n        raise RuntimeError(\"Store not initialized. The server may not be running.\")\n    return await self._store.retrieve_rollout(rollout_id)\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningServer.poll_completed_rollout","title":"<code>poll_completed_rollout(rollout_id, timeout=None)</code>  <code>async</code>","text":"<p>Polls for a completed rollout by its ID, waiting up to <code>timeout</code> seconds.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def poll_completed_rollout(self, rollout_id: str, timeout: Optional[float] = None) -&gt; Optional[Rollout]:\n    \"\"\"\n    Polls for a completed rollout by its ID, waiting up to `timeout` seconds.\n    \"\"\"\n    start_time = time.time()\n    while True:\n        rollout = await self.get_completed_rollout(rollout_id)\n        if rollout:\n            return rollout\n        if timeout and (time.time() - start_time) &gt;= timeout:\n            return None\n        await asyncio.sleep(1)\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningServer.queue_task","title":"<code>queue_task(sample, mode=None, resources_id=None, metadata=None)</code>  <code>async</code>","text":"<p>Adds a task to the queue for a client to process.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def queue_task(\n    self,\n    sample: Any,\n    mode: Literal[\"train\", \"val\", \"test\"] | None = None,\n    resources_id: str | None = None,\n    metadata: Dict[str, Any] | None = None,\n) -&gt; str:\n    \"\"\"\n    Adds a task to the queue for a client to process.\n    \"\"\"\n    if not self._store:\n        raise RuntimeError(\"Store not initialized. The server may not be running.\")\n    return await self._store.add_task(sample, mode=mode, resources_id=resources_id, metadata=metadata)\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningServer.retrieve_completed_rollouts","title":"<code>retrieve_completed_rollouts()</code>  <code>async</code>","text":"<p>Retrieves all available completed trajectories and clears the internal store.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def retrieve_completed_rollouts(self) -&gt; List[Rollout]:\n    \"\"\"\n    Retrieves all available completed trajectories and clears the internal store.\n    \"\"\"\n    if not self._store:\n        raise RuntimeError(\"Store not initialized. The server may not be running.\")\n    return await self._store.retrieve_completed_rollouts()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningServer.run_forever","title":"<code>run_forever()</code>  <code>async</code>","text":"<p>Runs the server indefinitely until stopped. This is useful when async start and stop methods do not work.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def run_forever(self):\n    \"\"\"\n    Runs the server indefinitely until stopped.\n    This is useful when async start and stop methods do not work.\n    \"\"\"\n    await self._uvicorn_server.serve()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningServer.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Starts the FastAPI server in the background.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def start(self):\n    \"\"\"Starts the FastAPI server in the background.\"\"\"\n    logger.info(f\"Starting server at {self.endpoint}\")\n    asyncio.create_task(self._uvicorn_server.serve())\n    await asyncio.sleep(1)  # Allow time for server to start up.\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningServer.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Gracefully stops the running FastAPI server.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def stop(self):\n    \"\"\"Gracefully stops the running FastAPI server.\"\"\"\n    if self._uvicorn_server.started:\n        logger.info(\"Stopping server...\")\n        self._uvicorn_server.should_exit = True\n        await asyncio.sleep(1)  # Allow time for graceful shutdown.\n        logger.info(\"Server stopped.\")\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningServer.update_resources","title":"<code>update_resources(resources)</code>  <code>async</code>","text":"<p>Updates the resources, creating a new version and setting it as the latest.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def update_resources(self, resources: NamedResources) -&gt; str:\n    \"\"\"\n    Updates the resources, creating a new version and setting it as the latest.\n    \"\"\"\n    if not self._store:\n        raise RuntimeError(\"Store not initialized. The server may not be running.\")\n    resources_id = f\"res-{uuid.uuid4()}\"\n    update = ResourcesUpdate(resources_id=resources_id, resources=resources)\n    await self._store.update_resources(update)\n    return resources_id\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningTrainer","title":"<code>AgentLightningTrainer</code>","text":"<p>               Bases: <code>RayPPOTrainer</code></p> <p>Specialized PPO trainer for agent-based reinforcement learning.</p> <p>This trainer is designed specifically for scenarios where the model interacts with external environments, tools, or APIs through an AgentLightningServer. It simplifies the training loop by removing the complex conditional logic present in the original RayPPOTrainer and focusing on the agent mode workflow.</p> <p>Key differences from RayPPOTrainer: 1. Uses AgentModeDaemon for server communication 2. Simplified data flow without pop/union operations 3. Direct batch processing through agent daemon 4. Streamlined validation using agent_mode validation</p> Source code in <code>agentlightning/verl/trainer.py</code> <pre><code>class AgentLightningTrainer(RayPPOTrainer):\n    \"\"\"\n    Specialized PPO trainer for agent-based reinforcement learning.\n\n    This trainer is designed specifically for scenarios where the model interacts with\n    external environments, tools, or APIs through an AgentLightningServer. It simplifies\n    the training loop by removing the complex conditional logic present in the original\n    RayPPOTrainer and focusing on the agent mode workflow.\n\n    Key differences from RayPPOTrainer:\n    1. Uses AgentModeDaemon for server communication\n    2. Simplified data flow without pop/union operations\n    3. Direct batch processing through agent daemon\n    4. Streamlined validation using agent_mode validation\n    \"\"\"\n\n    def _validate(self):\n        assert len(self.val_dataloader) == 1, \"Please set val_batch_size to None for better throughput.\"\n\n        test_data = next(iter(self.val_dataloader))\n        test_batch = DataProto.from_single_dict(test_data)\n\n        self.async_rollout_manager.wake_up()\n        self.agent_mode_daemon.set_up_data_and_server(\n            test_batch.non_tensor_batch,\n            self.async_rollout_manager.server_addresses,\n            is_train=False,\n        )\n        self.agent_mode_daemon.run_until_all_finished()\n        test_metrics = self.agent_mode_daemon.get_test_metrics()\n        self.agent_mode_daemon.clear_data_and_server()\n        self.async_rollout_manager.sleep()\n        return test_metrics\n\n    def _train_step(self, batch_dict: dict) -&gt; dict:\n        # Isolate in a separate method to automatically recycle the variables before validation.\n        batch: DataProto = DataProto.from_single_dict(batch_dict)\n        metrics = {}\n        timing_raw = {}\n\n        with _timer(\"step\", timing_raw):\n\n            # When agent mode is enabled, we read the batch as it is.\n            gen_batch = batch\n\n            # generate a batch\n            with _timer(\"gen\", timing_raw):\n                self.async_rollout_manager.wake_up()\n                self.agent_mode_daemon.set_up_data_and_server(\n                    gen_batch.non_tensor_batch, self.async_rollout_manager.server_addresses\n                )\n                self.agent_mode_daemon.run_until_all_finished()\n                batch, agent_metrics = self.agent_mode_daemon.get_train_data_batch(\n                    max_prompt_length=self.config.data.max_prompt_length,\n                    max_response_length=self.config.data.max_response_length,\n                    device=gen_batch.batch[\"fake_ids\"].device,\n                )\n                metrics.update(agent_metrics)\n                self.agent_mode_daemon.clear_data_and_server()\n                self.async_rollout_manager.sleep()\n\n            if self.config.algorithm.adv_estimator == AdvantageEstimator.REMAX:\n                with _timer(\"gen_max\", timing_raw):\n                    gen_baseline_batch = deepcopy(gen_batch)\n                    gen_baseline_batch.meta_info[\"do_sample\"] = False\n                    gen_baseline_output = self.actor_rollout_wg.generate_sequences(gen_baseline_batch)\n\n                    batch = batch.union(gen_baseline_output)\n                    reward_baseline_tensor = self.reward_fn(batch)\n                    reward_baseline_tensor = reward_baseline_tensor.sum(dim=-1)\n\n                    batch.pop(batch_keys=list(gen_baseline_output.batch.keys()))\n\n                    batch.batch[\"reward_baselines\"] = reward_baseline_tensor\n\n                    del gen_baseline_batch, gen_baseline_output\n\n            # uid is used for algorithm like GRPO, should be aligned to data id\n            batch.non_tensor_batch[\"uid\"] = batch.non_tensor_batch[\"data_id_list\"]\n\n            batch.batch[\"response_mask\"] = compute_response_mask(batch)\n\n            # compute global_valid tokens\n            batch.meta_info[\"global_token_num\"] = torch.sum(batch.batch[\"attention_mask\"], dim=-1).tolist()\n\n            with _timer(\"reward\", timing_raw):\n                # compute reward model score\n                if self.use_rm:\n                    reward_tensor = self.rm_wg.compute_rm_score(batch)\n                    batch = batch.union(reward_tensor)\n\n                reward_extra_infos_dict = {}\n\n            # for agent mode, pad the lengths to calculate old log prob, ref, and values\n            batch, pad_size = pad_dataproto_to_divisor(batch, self.actor_rollout_wg.world_size)\n\n            # recompute old_log_probs\n            with _timer(\"old_log_prob\", timing_raw):\n                old_log_prob = self.actor_rollout_wg.compute_log_prob(batch)\n                entropys = old_log_prob.batch[\"entropys\"]\n                response_masks = batch.batch[\"response_mask\"]\n                loss_agg_mode = self.config.actor_rollout_ref.actor.loss_agg_mode\n                entropy_loss = agg_loss(loss_mat=entropys, loss_mask=response_masks, loss_agg_mode=loss_agg_mode)\n                old_log_prob_metrics = {\"actor/entropy_loss\": entropy_loss.detach().item()}\n                metrics.update(old_log_prob_metrics)\n                old_log_prob.batch.pop(\"entropys\")\n                batch = batch.union(old_log_prob)\n\n            if self.use_reference_policy:\n                # compute reference log_prob\n                with _timer(\"ref\", timing_raw):\n                    ref_log_prob = self.ref_policy_wg.compute_ref_log_prob(batch)\n                    batch = batch.union(ref_log_prob)\n\n            # compute values\n            if self.use_critic:\n                with _timer(\"values\", timing_raw):\n                    values = self.critic_wg.compute_values(batch)\n                    batch = batch.union(values)\n\n            # for agent mode, unpad to calculate adv\n            # it is important, as adv should be based on the raw traces\n            batch = unpad_dataproto(batch, pad_size=pad_size)\n\n            with _timer(\"adv\", timing_raw):\n                # if agent_mode is enabled, there is already token_level_scores\n                # token_level_scores is not needed to compute here\n\n                # compute rewards. apply_kl_penalty if available\n                if self.config.algorithm.use_kl_in_reward:\n                    batch, kl_metrics = apply_kl_penalty(\n                        batch, kl_ctrl=self.kl_ctrl_in_reward, kl_penalty=self.config.algorithm.kl_penalty\n                    )\n                    metrics.update(kl_metrics)\n                else:\n                    batch.batch[\"token_level_rewards\"] = batch.batch[\"token_level_scores\"]\n\n                # compute advantages, executed on the driver process\n\n                norm_adv_by_std_in_grpo = self.config.algorithm.get(\n                    \"norm_adv_by_std_in_grpo\", True\n                )  # GRPO adv normalization factor\n\n                batch = compute_advantage(\n                    batch,\n                    adv_estimator=self.config.algorithm.adv_estimator,\n                    gamma=self.config.algorithm.gamma,\n                    lam=self.config.algorithm.lam,\n                    num_repeat=self.config.actor_rollout_ref.rollout.n,\n                    norm_adv_by_std_in_grpo=norm_adv_by_std_in_grpo,\n                    config=self.config.algorithm,\n                )\n\n            # after advantages are assinged, we begin to drop (1) long prompt (2) floor to ppo minisize\n            keep_indices = (~batch.batch[\"is_drop_mask\"]).nonzero(as_tuple=True)[0]\n            metrics[\"training/n_triplets_prompt_too_long\"] = (\n                batch.batch[\"is_drop_mask\"].shape[0] - keep_indices.shape[0]\n            )\n            batch = batch[keep_indices]\n            # next, round to minibatch size\n            mini_batch_size = self.config.actor_rollout_ref.actor.ppo_mini_batch_size\n            n_transition = len(batch)\n            random_indices = list(range(n_transition))\n            random.shuffle(random_indices)\n            batch.reorder(torch.tensor(random_indices).type(torch.int32))\n            n_remained_transition = n_transition // mini_batch_size * mini_batch_size\n            batch = batch[list(range(n_remained_transition))]\n            metrics[\"training/n_triplets_dropped_remainder\"] = n_transition - n_remained_transition\n\n            # Agent mode note: Change the order of balance batch;\n            #     1. first calculate advantage\n            #     2. then drop the samples (too long prompt &amp; floor to ppo minisize)\n            #     3. balance\n            # balance the number of valid tokens on each dp rank.\n            # Note that this breaks the order of data inside the batch.\n            # Please take care when you implement group based adv computation such as GRPO and rloo\n            if self.config.trainer.balance_batch:\n                self._balance_batch(batch, metrics=metrics)\n\n            # update critic\n            if self.use_critic:\n                with _timer(\"update_critic\", timing_raw):\n                    critic_output = self.critic_wg.update_critic(batch)\n                critic_output_metrics = reduce_metrics(critic_output.meta_info[\"metrics\"])\n                metrics.update(critic_output_metrics)\n\n            # implement critic warmup\n            if self.config.trainer.critic_warmup &lt;= self.global_steps:\n                # update actor\n                with _timer(\"update_actor\", timing_raw):\n                    batch.meta_info[\"multi_turn\"] = self.config.actor_rollout_ref.rollout.multi_turn.enable\n                    actor_output = self.actor_rollout_wg.update_actor(batch)\n                actor_output_metrics = reduce_metrics(actor_output.meta_info[\"metrics\"])\n                metrics.update(actor_output_metrics)\n\n            # Log rollout generations if enabled\n            rollout_data_dir = self.config.trainer.get(\"rollout_data_dir\", None)\n            if rollout_data_dir:\n                with _timer(\"dump_rollout_generations\", timing_raw):\n                    print(batch.batch.keys())\n                    inputs = self.tokenizer.batch_decode(batch.batch[\"prompts\"], skip_special_tokens=True)\n                    outputs = self.tokenizer.batch_decode(batch.batch[\"responses\"], skip_special_tokens=True)\n                    scores = batch.batch[\"token_level_scores\"].sum(-1).cpu().tolist()\n                    self._dump_generations(\n                        inputs=inputs,\n                        outputs=outputs,\n                        scores=scores,\n                        reward_extra_infos_dict=reward_extra_infos_dict,\n                        dump_path=rollout_data_dir,\n                    )\n\n        # compute training metrics\n        metrics.update(compute_data_metrics(batch=batch, use_critic=self.use_critic))\n        metrics.update(compute_timing_metrics(batch=batch, timing_raw=timing_raw))\n        # TODO: implement actual tflpo and theoretical tflpo\n        n_gpus = self.resource_pool_manager.get_n_gpus()\n        metrics.update(compute_throughout_metrics(batch=batch, timing_raw=timing_raw, n_gpus=n_gpus))\n\n        return metrics\n\n    def fit(self):\n        logger = Tracking(\n            project_name=self.config.trainer.project_name,\n            experiment_name=self.config.trainer.experiment_name,\n            default_backend=self.config.trainer.logger,\n            config=OmegaConf.to_container(self.config, resolve=True),\n        )\n\n        self.global_steps = 0\n\n        # load checkpoint before doing anything\n        self._load_checkpoint()\n\n        assert self.async_rollout_mode, \"If agent mode is enabled, async server must be enabled\"\n        self.agent_mode_daemon = AgentModeDaemon(\n            self.config.agentlightning.port,\n            self.config.actor_rollout_ref.rollout.n,\n            train_information={\n                # Note (Zhiyuan): To avoid further patch into vllm async server, using the same sentence to get the naming here.\n                # However, it is possible that verl updates the naming and causes incompatibility.\n                # Reference: https://github.com/volcengine/verl/blob/5b5e09d9cc20625e436d01f69d9cc739ff681c54/verl/workers/rollout/vllm_rollout/vllm_async_server.py#L217\n                \"model\": \"/\".join(self.config.actor_rollout_ref.model.path.split(\"/\")[-2:]),\n                \"temperature\": self.config.actor_rollout_ref.rollout.temperature,\n            },\n            tokenizer=self.tokenizer,\n            mini_batch_size=self.config.actor_rollout_ref.actor.ppo_mini_batch_size,\n            pad_token_id=self.tokenizer.pad_token_id,\n        )\n        self.agent_mode_daemon.start()\n\n        # perform validation before training\n        # currently, we only support validation using the reward_function.\n        if self.val_reward_fn is not None and self.config.trainer.get(\"val_before_train\", True):\n            val_metrics = self._validate()\n            assert val_metrics, f\"{val_metrics=}\"\n            pprint(f\"Initial validation metrics: {val_metrics}\")\n            logger.log(data=val_metrics, step=self.global_steps)\n            if self.config.trainer.get(\"val_only\", False):\n                return\n\n        # add tqdm\n        progress_bar = tqdm(total=self.total_training_steps, initial=self.global_steps, desc=\"Training Progress\")\n\n        # we start from step 1\n        self.global_steps += 1\n        last_val_metrics = None\n\n        for epoch in range(self.config.trainer.total_epochs):\n            for batch_dict in self.train_dataloader:\n                metrics = {}\n                timing_raw = {}\n                is_last_step = self.global_steps &gt;= self.total_training_steps\n\n                # train step\n                metrics = self._train_step(batch_dict)\n\n                # validate\n                if (\n                    self.val_reward_fn is not None\n                    and self.config.trainer.test_freq &gt; 0\n                    and (is_last_step or self.global_steps % self.config.trainer.test_freq == 0)\n                ):\n                    with _timer(\"validate\", timing_raw):\n                        val_metrics: dict = self._validate()\n                        if is_last_step:\n                            last_val_metrics = val_metrics\n                    metrics.update(val_metrics)\n\n                if self.config.trainer.save_freq &gt; 0 and (\n                    is_last_step or self.global_steps % self.config.trainer.save_freq == 0\n                ):\n                    with _timer(\"save_checkpoint\", timing_raw):\n                        self._save_checkpoint()\n\n                # step metrics\n                metrics.update(\n                    {\n                        \"training/global_step\": self.global_steps,\n                        \"training/epoch\": epoch,\n                    }\n                )\n\n                # TODO: make a canonical logger that supports various backend\n                logger.log(data=metrics, step=self.global_steps)\n\n                if is_last_step:\n                    pprint(f\"Final validation metrics: {last_val_metrics}\")\n                    progress_bar.close()\n\n                    # This exit logic is to ensure a robust CI.\n                    pprint(f\"Flush the logger...\")\n                    del logger  # Make sure the loggers are flushed and closed properly\n                    pprint(f\"Training finished at step {self.global_steps}.\")\n                    return\n\n                progress_bar.update(1)\n                self.global_steps += 1\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentModeDaemon","title":"<code>AgentModeDaemon</code>","text":"<p>AgentModeDaemon using the AgentLightningServer SDK.</p> <p>This class manages the server lifecycle, task queueing, and results retrieval, while also running a proxy server for LLM requests. It maintains the original interface for compatibility with the RayPPOTrainer.</p> Source code in <code>agentlightning/verl/daemon.py</code> <pre><code>class AgentModeDaemon:\n    \"\"\"\n    AgentModeDaemon using the AgentLightningServer SDK.\n\n    This class manages the server lifecycle, task queueing, and results\n    retrieval, while also running a proxy server for LLM requests. It maintains\n    the original interface for compatibility with the RayPPOTrainer.\n    \"\"\"\n\n    def __init__(\n        self,\n        port: int,\n        train_rollout_n: int,\n        train_information: Dict[str, Any],\n        tokenizer: Any,\n        mini_batch_size: int,\n        pad_token_id: int,\n        reward_fillna_value: float = 0.0,\n        llm_timeout_seconds: float = 1200.0,\n    ):\n        # Server and Task Configuration\n        self.server_port = port\n        self.llm_timeout_seconds = llm_timeout_seconds\n        self.server = AgentLightningServer(\n            host=\"0.0.0.0\", port=self.server_port, task_timeout_seconds=self.llm_timeout_seconds\n        )\n        self.proxy_port = _find_available_port()  # Run proxy on a different port\n\n        # Training and Data Configuration\n        self.train_rollout_n = train_rollout_n\n        self.train_information = train_information\n        self.mini_batch_size = mini_batch_size\n        self.pad_token_id = pad_token_id\n        self.tokenizer = tokenizer\n        self.reward_fillna_value = reward_fillna_value\n\n        # Internal State\n        self.backend_llm_server_addresses: List[str] = []\n        self._total_tasks_queued = 0\n        self._completed_rollouts: Dict[str, Rollout] = {}\n        self._task_id_to_original_sample: Dict[str, Dict[str, Any]] = {}\n        self._server_thread: Optional[threading.Thread] = None\n        self._proxy_thread: Optional[threading.Thread] = None\n        self.is_train = True\n\n    def _start_proxy_server(self):\n        \"\"\"\n        Initializes and runs a Flask-based proxy server in a separate thread.\n        This proxy load-balances requests to the actual backend LLM servers.\n        \"\"\"\n        app = Flask(__name__)\n\n        num_requests = 0\n        last_request_time = 0\n\n        @app.route(\"/v1/&lt;path:path&gt;\", methods=[\"GET\", \"POST\", \"PUT\", \"DELETE\", \"PATCH\", \"OPTIONS\", \"HEAD\"])\n        def proxy(path: str):  # type: ignore\n            if not self.backend_llm_server_addresses:\n                abort(503, description=\"No backend LLM servers available.\")\n\n            # Randomly choose a backend server for load balancing\n            target_server = random.choice(self.backend_llm_server_addresses)\n            target_url = f\"http://{target_server}/v1/{path}\"\n\n            # Copy client request headers, removing the Host header\n            headers = {key: value for key, value in request.headers if key.lower() != \"host\"}\n\n            # Log the request for debugging\n            nonlocal num_requests, last_request_time\n            current_time = time.time()\n            num_requests += 1\n            if current_time - last_request_time &gt; 60 or num_requests == 1 or num_requests % 100 == 0:\n                print(f\"Proxying {request.method} request to {target_server}. Request data: {request.get_data()}\")\n            last_request_time = current_time\n\n            try:\n                # Forward the request to the target backend\n                resp = requests.request(\n                    method=request.method,\n                    url=target_url,\n                    headers=headers,\n                    params=request.args,  # type: ignore\n                    data=request.get_data(),\n                    cookies=request.cookies,\n                    allow_redirects=False,\n                    timeout=self.llm_timeout_seconds,\n                )\n                # Filter out hop-by-hop headers before returning the response\n                excluded_headers = [\n                    \"content-encoding\",\n                    \"content-length\",\n                    \"transfer-encoding\",\n                    \"connection\",\n                    \"keep-alive\",\n                    \"proxy-authenticate\",\n                    \"proxy-authorization\",\n                    \"te\",\n                    \"trailers\",\n                    \"upgrade\",\n                ]\n                response_headers = [\n                    (name, value) for name, value in resp.raw.headers.items() if name.lower() not in excluded_headers\n                ]\n                if resp.status_code == 200:\n                    # NOTE: from Zhiyuan's code.\n                    # https://github.com/hzy46/verl_agent_mode/blob/2db65ea9858f645a914120357412a7540f8bd82d/verl/trainer/ppo/ray_trainer.py#L692-L711\n                    # request_json = json.loads(request.get_data().decode(\"utf-8\"))\n                    response_json = json.loads(resp.content.decode(\"utf-8\"))\n                    # response_message = ChatCompletion(**response_json).choices[0].message.model_dump(exclude_unset=True, exclude_none=True)\n                    # tool_schemas = request_json.get(\"tools\", None)\n                    # prompt_ids = self.tokenizer.apply_chat_template(request_json[\"messages\"], tools=tool_schemas, add_generation_prompt=True, tokenize=True)\n                    # full_ids = self.tokenizer.apply_chat_template(request_json[\"messages\"] + [response_message], tools=tool_schemas, add_generation_prompt=False, tokenize=True)\n                    # TBD: response_ids sometimes ends with \"&lt;eos_id&gt;\\n\", shall we keep the extra \"\\n\"?\n                    # sometimes it has some differences with the hacky method in the end, but this should align with ToolCompletionCallback\n                    # response_ids = full_ids[len(prompt_ids):]\n\n                    # NOTE (yuge): They are different. Don't know why.\n                    # assert response_json['prompt_token_ids'] == prompt_ids\n                    # patched_response_ids = response_json['response_token_ids'][0]\n                    # assert patched_response_ids == response_ids[:len(patched_response_ids)], f\"{patched_response_ids} != {response_ids[:len(patched_response_ids)]}\"\n                    # response_json['prompt_token_ids'] = prompt_ids\n                    # response_json['response_token_ids'] = [response_ids]\n                    replaced_return_content = json.dumps(response_json).encode(\"utf-8\")\n                    return Response(replaced_return_content, status=resp.status_code, headers=response_headers)\n                return Response(resp.content, resp.status_code, response_headers)\n            except requests.exceptions.RequestException as e:\n                abort(500, description=f\"Error proxying request: {e}\")\n\n        def run_app():\n            app.run(host=\"0.0.0.0\", port=self.proxy_port, threaded=True, debug=False)\n\n        self._proxy_thread = threading.Thread(target=run_app, daemon=True)\n        self._proxy_thread.start()\n        print(f\"Proxy server running on port {self.proxy_port}\")\n\n    def start(self):\n        \"\"\"Starts the main AgentLightningServer and the proxy server.\"\"\"\n\n        def run_server():\n            \"\"\"Run the AgentLightningServer in a separate thread.\"\"\"\n            asyncio.run(self.server.run_forever())\n\n        self._server_thread = threading.Thread(target=run_server, daemon=True)\n        self._server_thread.start()\n\n        # Wait for the server's internal startup event to be set.\n        print(\"Waiting for AgentLightningServer to start...\")\n        is_ready = self.server.startup_event.wait(timeout=20.0)  # Wait up to 20s\n        if not is_ready:\n            raise RuntimeError(\"AgentLightningServer failed to start within the timeout period.\")\n\n        print(f\"AgentLightningServer control plane running on port {self.server_port}\")\n\n        self._start_proxy_server()\n\n    async def _async_set_up(self, data: Dict[str, Any], server_addresses: List[str], is_train: bool = True):\n        \"\"\"Async helper to set up data and resources on the server.\"\"\"\n        self.clear_data_and_server()\n        self.backend_llm_server_addresses = server_addresses\n        self.is_train = is_train\n\n        # 1. Update resources on the server for clients to use\n        llm_resource = LLM(\n            endpoint=f\"http://127.0.0.1:{self.proxy_port}/v1\",\n            model=self.train_information.get(\"model\", \"default-model\"),\n            sampling_parameters={\"temperature\": self.train_information.get(\"temperature\", 0.7 if is_train else 0.0)},\n        )\n        resources: NamedResources = {\"main_llm\": llm_resource}\n        resources_id = await self.server.update_resources(resources)\n\n        # 2. Queue tasks for agents to process\n        keys = list(data.keys())\n        num_samples = len(data[keys[0]])\n        rollouts_per_sample = self.train_rollout_n if is_train else 1\n\n        for i in range(num_samples):\n            data_id = str(uuid.uuid4())\n            original_sample = {key: data[key][i] for key in keys}\n            original_sample[\"data_id\"] = data_id\n\n            # For training, each sample is rolled out multiple times\n            for _ in range(rollouts_per_sample):\n                task_metadata = {\"data_id\": data_id, \"is_train\": is_train}\n\n                # Data ID is different from Rollout ID, as one data can have multiple rollouts.\n                rollout_id = await self.server.queue_task(\n                    sample=_to_native(original_sample),\n                    mode=\"train\" if is_train else \"val\",\n                    resources_id=resources_id,\n                    metadata=task_metadata,\n                )\n                # Store original sample data to reconstruct batch information later\n                self._task_id_to_original_sample[rollout_id] = original_sample\n                self._total_tasks_queued += 1\n\n    def set_up_data_and_server(self, data: Dict[str, Any], server_addresses: List[str], is_train: bool = True):\n        \"\"\"Synchronous wrapper for setting up data and server resources.\"\"\"\n        if not self.server.loop or not self.server.startup_event.is_set():\n            raise RuntimeError(\"Server is not running or ready.\")\n\n        coro = self._async_set_up(data, server_addresses, is_train)\n        future = asyncio.run_coroutine_threadsafe(coro, self.server.loop)\n        try:\n            future.result(timeout=60)  # Wait for completion with a timeout\n        except Exception as e:\n            print(f\"Failed to set up data on server: {e}\")\n            raise\n\n    def _validate_data(self, rollout: Rollout):\n        if rollout.final_reward is None:\n            print(\n                f\"Warning: Reward is None for rollout {rollout.rollout_id}, will be auto-set to {self.reward_fillna_value}.\"\n            )\n        if rollout.triplets is None:\n            print(f\"Warning: Triplet is None for rollout {rollout.rollout_id}.\")\n        elif len(rollout.triplets) == 0:\n            print(f\"Warning: Length of triplets is 0 for rollout {rollout.rollout_id}.\")\n        elif any(not r.response.get(\"token_ids\", []) for r in rollout.triplets):\n            print(f\"Warning: Rollout {rollout.rollout_id} contains empty response: {rollout.triplets}\")\n        elif any(not r.prompt.get(\"token_ids\", []) for r in rollout.triplets):\n            print(f\"Warning: Rollout {rollout.rollout_id} contains empty prompt: {rollout.triplets}\")\n\n    async def _async_run_until_finished(self, verbose: bool = True):\n        \"\"\"Async helper to wait for all tasks to complete.\"\"\"\n        while len(self._completed_rollouts) &lt; self._total_tasks_queued:\n            completed_batch = await self.server.retrieve_completed_rollouts()\n            for rollout in completed_batch:\n                self._validate_data(rollout)\n                if rollout.rollout_id not in self._task_id_to_original_sample:\n                    print(f\"Warning: Received unknown rollout ID {rollout.rollout_id}, skipping.\")\n                else:\n                    self._completed_rollouts[rollout.rollout_id] = rollout\n            if verbose:\n                print(f\"Completed {len(self._completed_rollouts)}/{self._total_tasks_queued} tasks...\")\n            await asyncio.sleep(5)\n        print(\"All tasks finished.\")\n\n    def run_until_all_finished(self, verbose: bool = True):\n        \"\"\"Synchronously waits for all queued tasks to be completed and reported.\"\"\"\n        if self._total_tasks_queued == 0:\n            print(\"Warning: No tasks were queued.\")\n            return\n\n        if not self.server.loop or not self.server.startup_event.is_set():\n            raise RuntimeError(\"Server is not running or ready.\")\n\n        coro = self._async_run_until_finished(verbose)\n        future = asyncio.run_coroutine_threadsafe(coro, self.server.loop)\n        try:\n            future.result()  # Wait indefinitely for all tasks to complete\n        except Exception as e:\n            print(f\"Error while waiting for tasks to finish: {e}\")\n            raise\n\n    def get_test_metrics(self):\n        \"\"\"Calculates and returns metrics for a validation run.\"\"\"\n        assert not self.is_train, \"This method should only be called during validation.\"\n        assert len(self._completed_rollouts) == self._total_tasks_queued\n\n        sample_stat_list: List[Dict[str, Any]] = []\n        for _, rollout in self._completed_rollouts.items():\n            final_reward = self._fillna_reward(rollout)\n            if not rollout.triplets:\n                print(f\"Warning: No triplets found for test rollout {rollout.rollout_id}.\")\n                sample_stat_list.append({\"reward\": final_reward})\n                continue\n            response_length_list = [len(triplet.response.get(\"token_ids\", [])) for triplet in rollout.triplets]\n            sample_stat_list.append(\n                {\n                    \"sum_response_length\": np.sum(response_length_list),\n                    \"mean_response_length\": np.mean(response_length_list) if response_length_list else 0,\n                    \"turn_count\": len(rollout.triplets),\n                    \"reward\": final_reward,\n                }\n            )\n\n        stats_w_trace = [stat for stat in sample_stat_list if \"sum_response_length\" in stat]\n        return {\n            \"val/n_rollouts\": len(sample_stat_list),\n            \"val/n_rollouts_w_trace\": len(stats_w_trace),\n            \"val/reward\": np.mean(\n                [stat[\"reward\"] for stat in sample_stat_list]\n            ),  # each rollout must have a reward (fillna if missing)\n            \"val/mean_response_length\": np.mean([stat[\"mean_response_length\"] for stat in stats_w_trace]),\n            \"val/sum_response_length\": np.mean([stat[\"sum_response_length\"] for stat in stats_w_trace]),\n            \"val/turn_count\": np.mean([stat[\"turn_count\"] for stat in stats_w_trace]),\n        }\n\n    def get_train_data_batch(self, max_prompt_length: int, max_response_length: int, device: torch.device):\n        \"\"\"\n        Processes completed rollouts to generate a training data batch.\n\n        This function reconstructs the logic from the original AgentModeDaemon,\n        using data retrieved from the new server architecture. It handles padding,\n        truncation, and tensor creation for the PPO training loop.\n        \"\"\"\n        assert self.is_train, \"This method should only be called during training.\"\n        assert len(self._completed_rollouts) == self._total_tasks_queued\n\n        # 1. Reconstruct the `finished_id_to_sample_info` structure from completed rollouts\n        finished_id_to_sample_info: Dict[str, Dict[str, Any]] = {}\n        finished_id_to_final_reward: Dict[str, float] = {}\n        for rollout_id, rollout in self._completed_rollouts.items():\n            original_sample = self._task_id_to_original_sample[rollout_id]\n\n            final_reward = self._fillna_reward(rollout)\n\n            if not rollout.triplets:\n                finished_id_to_final_reward[rollout_id] = final_reward\n                print(f\"Warning: No triplets found for training rollout {rollout.rollout_id}, skipping.\")\n                continue\n\n            # The client should report triplets that contain prompt_ids and response_ids.\n            # Example triplet.prompt: {\"token_ids\": [...]}\n            # Example triplet.response: {\"token_ids\": [...]}\n            trace_list = [\n                {\"prompt_ids\": t.prompt.get(\"token_ids\", []), \"response_ids\": t.response.get(\"token_ids\", [])}\n                for t in rollout.triplets\n            ]\n            info = {\n                \"reward\": final_reward,\n                \"trace_list\": trace_list,\n                \"data_id\": original_sample[\"data_id\"],\n            }\n            finished_id_to_sample_info[rollout_id] = info\n            finished_id_to_final_reward[rollout_id] = final_reward\n        #\n        # --- Data processing and tensor creation logic ---\n        # Get all the reported data.\n        # prompt_ids are left-padded.\n        # response_ids are right-padded.\n        # They are concatenated in the middle.\n        # Discard handling:\n        #   - Those exceeding max_prompt_length will be marked for discard, but not\n        #     discarded here. They are only truncated and marked, to be discarded later.\n        #     This is for the correctness of the advantage calculation.\n        #   - The discard for the PPO mini-batch should also be handled this way.\n        input_ids_list: List[List[int]] = []\n        input_attention_mask_list: List[List[int]] = []\n        response_ids_list: List[List[int]] = []\n        response_attention_mask_list: List[List[int]] = []\n        reward_list: List[float] = []\n        data_id_list: List[str] = []\n        rollout_id_list: List[str] = []\n        turn_index_list: List[int] = []\n        is_drop_list: List[bool] = []\n        n_trunc_sample_because_of_response = 0\n\n        for rollout_id, sample_info in finished_id_to_sample_info.items():\n            for turn_index, trace in enumerate(sample_info[\"trace_list\"]):\n\n                reward_list.append(sample_info[\"reward\"])\n                prompt_ids, response_ids = trace[\"prompt_ids\"], trace[\"response_ids\"]\n\n                # Mark samples with prompts exceeding max_prompt_length to be dropped later\n                if len(prompt_ids) &gt; max_prompt_length:\n                    prompt_ids = prompt_ids[:max_prompt_length]\n                    is_drop_list.append(True)\n                else:\n                    is_drop_list.append(False)\n\n                # Truncate responses that exceed max_response_length\n                if len(response_ids) &gt; max_response_length:\n                    response_ids = response_ids[:max_response_length]\n                    n_trunc_sample_because_of_response += 1\n\n                # Pad prompts to the left and responses to the right\n                one_input_ids, one_input_attention_mask = get_left_padded_ids_and_attention_mask(\n                    prompt_ids, max_prompt_length, self.pad_token_id\n                )\n                one_response_ids, one_response_attention_mask = get_right_padded_ids_and_attention_mask(\n                    response_ids, max_response_length, self.pad_token_id\n                )\n\n                input_ids_list.append(one_input_ids)\n                input_attention_mask_list.append(one_input_attention_mask)\n                response_ids_list.append(one_response_ids)\n                response_attention_mask_list.append(one_response_attention_mask)\n                data_id_list.append(sample_info[\"data_id\"])\n                rollout_id_list.append(rollout_id)\n                turn_index_list.append(turn_index)\n\n        n_transition = len(input_ids_list)\n        batch_input_ids = torch.LongTensor(input_ids_list).to(device)\n        input_attention_mask = torch.LongTensor(input_attention_mask_list).to(device)\n        batch_response_ids = torch.LongTensor(response_ids_list).to(device)\n        response_attention_mask = torch.LongTensor(response_attention_mask_list).to(device)\n\n        # Concatenate prompts and responses to form the full sequence\n        batch_seq = torch.cat([batch_input_ids, batch_response_ids], dim=-1)\n        attention_mask = torch.cat([input_attention_mask, response_attention_mask], dim=-1)\n        position_ids = torch.clamp(torch.cumsum(attention_mask, dim=-1) - 1, min=0)\n        is_drop_mask = torch.BoolTensor(is_drop_list).to(device)\n        scores = torch.tensor(reward_list, dtype=torch.bfloat16).to(device)\n\n        # Create token-level scores by placing the final reward at the last token position\n        token_level_scores = torch.zeros_like(attention_mask, dtype=scores.dtype)\n        # At the eos_mask_idx position of each sample, fill in the corresponding scores.\n        # torch.arange(n_transition) generates [0,1,2,...,bsz-1] as indices for the batch dimension.\n        eos_mask_idx = torch.argmax(position_ids * attention_mask, dim=-1)  # (bsz,)\n        token_level_scores[torch.arange(n_transition), eos_mask_idx] = scores\n        # Only take the last response_length part of the sequence to get the token-level scores for the model's response part.\n        token_level_scores = token_level_scores[:, -max_response_length:]\n\n        # Form the final batch using TensorDict\n        batch = TensorDict(\n            {\n                \"prompts\": batch_input_ids,\n                \"responses\": batch_response_ids,\n                \"input_ids\": batch_seq,  # here input_ids become the whole sentences\n                \"attention_mask\": attention_mask,\n                \"position_ids\": position_ids,\n                \"is_drop_mask\": is_drop_mask,\n                \"token_level_scores\": token_level_scores.contiguous(),\n            },\n            batch_size=n_transition,\n        )\n        data_proto = DataProto(batch=batch)\n\n        data_metrics = {\n            \"training/reward\": np.mean(list(finished_id_to_final_reward.values())),\n            \"training/n_rollouts\": len(finished_id_to_final_reward),\n            \"training/n_rollouts_w_trace\": len(finished_id_to_sample_info),\n            \"training/n_truncated_triplets\": n_trunc_sample_because_of_response,\n            \"training/n_triplets\": n_transition,\n        }\n\n        # Add non-tensor data for advantage calculation and logging\n        data_proto.non_tensor_batch[\"data_id_list\"] = np.array(data_id_list)  # type: ignore\n        data_proto.non_tensor_batch[\"rollout_id_list\"] = np.array(rollout_id_list)  # type: ignore\n        data_proto.non_tensor_batch[\"turn_index_list\"] = np.array(turn_index_list)  # type: ignore\n\n        return data_proto, data_metrics\n\n    def clear_data_and_server(self):\n        \"\"\"Resets the internal state of the daemon for the next run.\"\"\"\n        self.backend_llm_server_addresses = []\n        self._completed_rollouts.clear()\n        self._task_id_to_original_sample.clear()\n        self._total_tasks_queued = 0\n        # For a true reset, the server's internal queues would also need clearing.\n        # This implementation assumes that `set_up_data_and_server` is called\n        # for each new run, effectively starting a fresh batch.\n\n    def _fillna_reward(self, rollout: Rollout):\n        if rollout.final_reward is None:\n            if self.reward_fillna_value is not None:  # type: ignore\n                final_reward = self.reward_fillna_value\n            else:\n                raise ValueError(f\"Reward is None for rollout {rollout.rollout_id}, please check the reward function.\")\n        else:\n            final_reward = rollout.final_reward\n        return final_reward\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentModeDaemon.clear_data_and_server","title":"<code>clear_data_and_server()</code>","text":"<p>Resets the internal state of the daemon for the next run.</p> Source code in <code>agentlightning/verl/daemon.py</code> <pre><code>def clear_data_and_server(self):\n    \"\"\"Resets the internal state of the daemon for the next run.\"\"\"\n    self.backend_llm_server_addresses = []\n    self._completed_rollouts.clear()\n    self._task_id_to_original_sample.clear()\n    self._total_tasks_queued = 0\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentModeDaemon.get_test_metrics","title":"<code>get_test_metrics()</code>","text":"<p>Calculates and returns metrics for a validation run.</p> Source code in <code>agentlightning/verl/daemon.py</code> <pre><code>def get_test_metrics(self):\n    \"\"\"Calculates and returns metrics for a validation run.\"\"\"\n    assert not self.is_train, \"This method should only be called during validation.\"\n    assert len(self._completed_rollouts) == self._total_tasks_queued\n\n    sample_stat_list: List[Dict[str, Any]] = []\n    for _, rollout in self._completed_rollouts.items():\n        final_reward = self._fillna_reward(rollout)\n        if not rollout.triplets:\n            print(f\"Warning: No triplets found for test rollout {rollout.rollout_id}.\")\n            sample_stat_list.append({\"reward\": final_reward})\n            continue\n        response_length_list = [len(triplet.response.get(\"token_ids\", [])) for triplet in rollout.triplets]\n        sample_stat_list.append(\n            {\n                \"sum_response_length\": np.sum(response_length_list),\n                \"mean_response_length\": np.mean(response_length_list) if response_length_list else 0,\n                \"turn_count\": len(rollout.triplets),\n                \"reward\": final_reward,\n            }\n        )\n\n    stats_w_trace = [stat for stat in sample_stat_list if \"sum_response_length\" in stat]\n    return {\n        \"val/n_rollouts\": len(sample_stat_list),\n        \"val/n_rollouts_w_trace\": len(stats_w_trace),\n        \"val/reward\": np.mean(\n            [stat[\"reward\"] for stat in sample_stat_list]\n        ),  # each rollout must have a reward (fillna if missing)\n        \"val/mean_response_length\": np.mean([stat[\"mean_response_length\"] for stat in stats_w_trace]),\n        \"val/sum_response_length\": np.mean([stat[\"sum_response_length\"] for stat in stats_w_trace]),\n        \"val/turn_count\": np.mean([stat[\"turn_count\"] for stat in stats_w_trace]),\n    }\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentModeDaemon.get_train_data_batch","title":"<code>get_train_data_batch(max_prompt_length, max_response_length, device)</code>","text":"<p>Processes completed rollouts to generate a training data batch.</p> <p>This function reconstructs the logic from the original AgentModeDaemon, using data retrieved from the new server architecture. It handles padding, truncation, and tensor creation for the PPO training loop.</p> Source code in <code>agentlightning/verl/daemon.py</code> <pre><code>def get_train_data_batch(self, max_prompt_length: int, max_response_length: int, device: torch.device):\n    \"\"\"\n    Processes completed rollouts to generate a training data batch.\n\n    This function reconstructs the logic from the original AgentModeDaemon,\n    using data retrieved from the new server architecture. It handles padding,\n    truncation, and tensor creation for the PPO training loop.\n    \"\"\"\n    assert self.is_train, \"This method should only be called during training.\"\n    assert len(self._completed_rollouts) == self._total_tasks_queued\n\n    # 1. Reconstruct the `finished_id_to_sample_info` structure from completed rollouts\n    finished_id_to_sample_info: Dict[str, Dict[str, Any]] = {}\n    finished_id_to_final_reward: Dict[str, float] = {}\n    for rollout_id, rollout in self._completed_rollouts.items():\n        original_sample = self._task_id_to_original_sample[rollout_id]\n\n        final_reward = self._fillna_reward(rollout)\n\n        if not rollout.triplets:\n            finished_id_to_final_reward[rollout_id] = final_reward\n            print(f\"Warning: No triplets found for training rollout {rollout.rollout_id}, skipping.\")\n            continue\n\n        # The client should report triplets that contain prompt_ids and response_ids.\n        # Example triplet.prompt: {\"token_ids\": [...]}\n        # Example triplet.response: {\"token_ids\": [...]}\n        trace_list = [\n            {\"prompt_ids\": t.prompt.get(\"token_ids\", []), \"response_ids\": t.response.get(\"token_ids\", [])}\n            for t in rollout.triplets\n        ]\n        info = {\n            \"reward\": final_reward,\n            \"trace_list\": trace_list,\n            \"data_id\": original_sample[\"data_id\"],\n        }\n        finished_id_to_sample_info[rollout_id] = info\n        finished_id_to_final_reward[rollout_id] = final_reward\n    #\n    # --- Data processing and tensor creation logic ---\n    # Get all the reported data.\n    # prompt_ids are left-padded.\n    # response_ids are right-padded.\n    # They are concatenated in the middle.\n    # Discard handling:\n    #   - Those exceeding max_prompt_length will be marked for discard, but not\n    #     discarded here. They are only truncated and marked, to be discarded later.\n    #     This is for the correctness of the advantage calculation.\n    #   - The discard for the PPO mini-batch should also be handled this way.\n    input_ids_list: List[List[int]] = []\n    input_attention_mask_list: List[List[int]] = []\n    response_ids_list: List[List[int]] = []\n    response_attention_mask_list: List[List[int]] = []\n    reward_list: List[float] = []\n    data_id_list: List[str] = []\n    rollout_id_list: List[str] = []\n    turn_index_list: List[int] = []\n    is_drop_list: List[bool] = []\n    n_trunc_sample_because_of_response = 0\n\n    for rollout_id, sample_info in finished_id_to_sample_info.items():\n        for turn_index, trace in enumerate(sample_info[\"trace_list\"]):\n\n            reward_list.append(sample_info[\"reward\"])\n            prompt_ids, response_ids = trace[\"prompt_ids\"], trace[\"response_ids\"]\n\n            # Mark samples with prompts exceeding max_prompt_length to be dropped later\n            if len(prompt_ids) &gt; max_prompt_length:\n                prompt_ids = prompt_ids[:max_prompt_length]\n                is_drop_list.append(True)\n            else:\n                is_drop_list.append(False)\n\n            # Truncate responses that exceed max_response_length\n            if len(response_ids) &gt; max_response_length:\n                response_ids = response_ids[:max_response_length]\n                n_trunc_sample_because_of_response += 1\n\n            # Pad prompts to the left and responses to the right\n            one_input_ids, one_input_attention_mask = get_left_padded_ids_and_attention_mask(\n                prompt_ids, max_prompt_length, self.pad_token_id\n            )\n            one_response_ids, one_response_attention_mask = get_right_padded_ids_and_attention_mask(\n                response_ids, max_response_length, self.pad_token_id\n            )\n\n            input_ids_list.append(one_input_ids)\n            input_attention_mask_list.append(one_input_attention_mask)\n            response_ids_list.append(one_response_ids)\n            response_attention_mask_list.append(one_response_attention_mask)\n            data_id_list.append(sample_info[\"data_id\"])\n            rollout_id_list.append(rollout_id)\n            turn_index_list.append(turn_index)\n\n    n_transition = len(input_ids_list)\n    batch_input_ids = torch.LongTensor(input_ids_list).to(device)\n    input_attention_mask = torch.LongTensor(input_attention_mask_list).to(device)\n    batch_response_ids = torch.LongTensor(response_ids_list).to(device)\n    response_attention_mask = torch.LongTensor(response_attention_mask_list).to(device)\n\n    # Concatenate prompts and responses to form the full sequence\n    batch_seq = torch.cat([batch_input_ids, batch_response_ids], dim=-1)\n    attention_mask = torch.cat([input_attention_mask, response_attention_mask], dim=-1)\n    position_ids = torch.clamp(torch.cumsum(attention_mask, dim=-1) - 1, min=0)\n    is_drop_mask = torch.BoolTensor(is_drop_list).to(device)\n    scores = torch.tensor(reward_list, dtype=torch.bfloat16).to(device)\n\n    # Create token-level scores by placing the final reward at the last token position\n    token_level_scores = torch.zeros_like(attention_mask, dtype=scores.dtype)\n    # At the eos_mask_idx position of each sample, fill in the corresponding scores.\n    # torch.arange(n_transition) generates [0,1,2,...,bsz-1] as indices for the batch dimension.\n    eos_mask_idx = torch.argmax(position_ids * attention_mask, dim=-1)  # (bsz,)\n    token_level_scores[torch.arange(n_transition), eos_mask_idx] = scores\n    # Only take the last response_length part of the sequence to get the token-level scores for the model's response part.\n    token_level_scores = token_level_scores[:, -max_response_length:]\n\n    # Form the final batch using TensorDict\n    batch = TensorDict(\n        {\n            \"prompts\": batch_input_ids,\n            \"responses\": batch_response_ids,\n            \"input_ids\": batch_seq,  # here input_ids become the whole sentences\n            \"attention_mask\": attention_mask,\n            \"position_ids\": position_ids,\n            \"is_drop_mask\": is_drop_mask,\n            \"token_level_scores\": token_level_scores.contiguous(),\n        },\n        batch_size=n_transition,\n    )\n    data_proto = DataProto(batch=batch)\n\n    data_metrics = {\n        \"training/reward\": np.mean(list(finished_id_to_final_reward.values())),\n        \"training/n_rollouts\": len(finished_id_to_final_reward),\n        \"training/n_rollouts_w_trace\": len(finished_id_to_sample_info),\n        \"training/n_truncated_triplets\": n_trunc_sample_because_of_response,\n        \"training/n_triplets\": n_transition,\n    }\n\n    # Add non-tensor data for advantage calculation and logging\n    data_proto.non_tensor_batch[\"data_id_list\"] = np.array(data_id_list)  # type: ignore\n    data_proto.non_tensor_batch[\"rollout_id_list\"] = np.array(rollout_id_list)  # type: ignore\n    data_proto.non_tensor_batch[\"turn_index_list\"] = np.array(turn_index_list)  # type: ignore\n\n    return data_proto, data_metrics\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentModeDaemon.run_until_all_finished","title":"<code>run_until_all_finished(verbose=True)</code>","text":"<p>Synchronously waits for all queued tasks to be completed and reported.</p> Source code in <code>agentlightning/verl/daemon.py</code> <pre><code>def run_until_all_finished(self, verbose: bool = True):\n    \"\"\"Synchronously waits for all queued tasks to be completed and reported.\"\"\"\n    if self._total_tasks_queued == 0:\n        print(\"Warning: No tasks were queued.\")\n        return\n\n    if not self.server.loop or not self.server.startup_event.is_set():\n        raise RuntimeError(\"Server is not running or ready.\")\n\n    coro = self._async_run_until_finished(verbose)\n    future = asyncio.run_coroutine_threadsafe(coro, self.server.loop)\n    try:\n        future.result()  # Wait indefinitely for all tasks to complete\n    except Exception as e:\n        print(f\"Error while waiting for tasks to finish: {e}\")\n        raise\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentModeDaemon.set_up_data_and_server","title":"<code>set_up_data_and_server(data, server_addresses, is_train=True)</code>","text":"<p>Synchronous wrapper for setting up data and server resources.</p> Source code in <code>agentlightning/verl/daemon.py</code> <pre><code>def set_up_data_and_server(self, data: Dict[str, Any], server_addresses: List[str], is_train: bool = True):\n    \"\"\"Synchronous wrapper for setting up data and server resources.\"\"\"\n    if not self.server.loop or not self.server.startup_event.is_set():\n        raise RuntimeError(\"Server is not running or ready.\")\n\n    coro = self._async_set_up(data, server_addresses, is_train)\n    future = asyncio.run_coroutine_threadsafe(coro, self.server.loop)\n    try:\n        future.result(timeout=60)  # Wait for completion with a timeout\n    except Exception as e:\n        print(f\"Failed to set up data on server: {e}\")\n        raise\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentModeDaemon.start","title":"<code>start()</code>","text":"<p>Starts the main AgentLightningServer and the proxy server.</p> Source code in <code>agentlightning/verl/daemon.py</code> <pre><code>def start(self):\n    \"\"\"Starts the main AgentLightningServer and the proxy server.\"\"\"\n\n    def run_server():\n        \"\"\"Run the AgentLightningServer in a separate thread.\"\"\"\n        asyncio.run(self.server.run_forever())\n\n    self._server_thread = threading.Thread(target=run_server, daemon=True)\n    self._server_thread.start()\n\n    # Wait for the server's internal startup event to be set.\n    print(\"Waiting for AgentLightningServer to start...\")\n    is_ready = self.server.startup_event.wait(timeout=20.0)  # Wait up to 20s\n    if not is_ready:\n        raise RuntimeError(\"AgentLightningServer failed to start within the timeout period.\")\n\n    print(f\"AgentLightningServer control plane running on port {self.server_port}\")\n\n    self._start_proxy_server()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>Protocol</code>, <code>Generic[T_co]</code></p> <p>The general interface for a dataset.</p> <p>It's currently implemented as a protocol, having a similar interface to torch.utils.data.Dataset. You don't have to inherit from this class; you can use a simple list if you want to.</p> Source code in <code>agentlightning/types.py</code> <pre><code>class Dataset(Protocol, Generic[T_co]):\n    \"\"\"The general interface for a dataset.\n\n    It's currently implemented as a protocol, having a similar interface to torch.utils.data.Dataset.\n    You don't have to inherit from this class; you can use a simple list if you want to.\n    \"\"\"\n\n    def __getitem__(self, index: int) -&gt; T_co: ...\n\n    def __len__(self) -&gt; int: ...\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LLM","title":"<code>LLM</code>","text":"<p>               Bases: <code>Resource</code></p> <p>Provide an LLM endpoint and model name as a resource.</p> <p>Attributes:</p> Name Type Description <code>endpoint</code> <code>str</code> <p>The URL of the LLM API endpoint.</p> <code>model</code> <code>str</code> <p>The identifier for the model to be used (e.g., 'gpt-4o').</p> <code>sampling_parameters</code> <code>SamplingParameters</code> <p>A dictionary of hyperparameters for model inference, such as temperature, top_p, etc.</p> Source code in <code>agentlightning/types.py</code> <pre><code>class LLM(Resource):\n    \"\"\"\n    Provide an LLM endpoint and model name as a resource.\n\n    Attributes:\n        endpoint (str): The URL of the LLM API endpoint.\n        model (str): The identifier for the model to be used (e.g., 'gpt-4o').\n        sampling_parameters (SamplingParameters): A dictionary of hyperparameters\n            for model inference, such as temperature, top_p, etc.\n    \"\"\"\n\n    resource_type: Literal[\"llm\"] = \"llm\"\n    endpoint: str\n    model: str\n    sampling_parameters: Dict[str, Any] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.Rollout","title":"<code>Rollout</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The standard reporting object from client to server.</p> Source code in <code>agentlightning/types.py</code> <pre><code>class Rollout(BaseModel):\n    \"\"\"The standard reporting object from client to server.\"\"\"\n\n    rollout_id: str\n\n    # Echoing the input task\n    task: Optional[Task] = None\n\n    # Primary, high-level feedback\n    final_reward: Optional[float] = None\n\n    # Structured, sequential feedback for RL-style optimization\n    triplets: Optional[List[Triplet]] = None\n\n    # Optional, rich-context data for deep analysis\n    trace: Optional[List[Dict[str, Any]]] = Field(\n        default=None,\n        description=\"A list of spans that conform to the OpenTelemetry JSON format. \"\n        \"Users of the opentelemetry-sdk can generate this by calling \"\n        \"json.loads(readable_span.to_json()).\",\n    )\n    logs: Optional[List[str]] = None\n\n    # A bucket for any other relevant information\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.get_left_padded_ids_and_attention_mask","title":"<code>get_left_padded_ids_and_attention_mask(ids, max_length, pad_token_id)</code>","text":"<p>Left-pad (or truncate) a sequence of token IDs to a fixed length, and build the corresponding attention mask.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>List[int]</code> <p>the original list of token IDs.</p> required <code>max_length</code> <code>int</code> <p>desired total length after padding/truncation.</p> required <code>pad_token_id</code> <code>int</code> <p>ID to use for padding.</p> required <p>Returns:</p> Name Type Description <code>padded_ids</code> <code>any</code> <p>list of length == max_length.</p> <code>attention_mask</code> <code>any</code> <p>list of same length: 1 for non-pad tokens, 0 for pads.</p> Source code in <code>agentlightning/verl/daemon.py</code> <pre><code>def get_left_padded_ids_and_attention_mask(\n    ids: List[int], max_length: int, pad_token_id: int\n) -&gt; Tuple[List[int], List[int]]:\n    \"\"\"\n    Left-pad (or truncate) a sequence of token IDs to a fixed length,\n    and build the corresponding attention mask.\n\n    Args:\n        ids:             the original list of token IDs.\n        max_length:      desired total length after padding/truncation.\n        pad_token_id:    ID to use for padding.\n\n    Returns:\n        padded_ids (any):      list of length == max_length.\n        attention_mask (any):  list of same length: 1 for non-pad tokens, 0 for pads.\n    \"\"\"\n    seq_len = len(ids)\n\n    if seq_len &gt;= max_length:\n        # too long \u2192 truncate from the left, keep the last max_length tokens\n        trimmed = ids[-max_length:]\n        attention_mask = [1] * max_length\n        return trimmed, attention_mask\n\n    # too short \u2192 pad on the left\n    pad_len = max_length - seq_len\n    padded_ids = [pad_token_id] * pad_len + ids\n    attention_mask = [0] * pad_len + [1] * seq_len\n    return padded_ids, attention_mask\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.get_right_padded_ids_and_attention_mask","title":"<code>get_right_padded_ids_and_attention_mask(ids, max_length, pad_token_id)</code>","text":"<p>Right-pad (or truncate) a sequence of token IDs to a fixed length, and build the corresponding attention mask.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>List[int]</code> <p>the original list of token IDs.</p> required <code>max_length</code> <code>int</code> <p>desired total length after padding/truncation.</p> required <code>pad_token_id</code> <code>int</code> <p>ID to use for padding.</p> required <p>Returns:</p> Name Type Description <code>padded_ids</code> <code>any</code> <p>list of length == max_length.</p> <code>attention_mask</code> <code>any</code> <p>list of same length: 1 for non-pad tokens, 0 for pads.</p> Source code in <code>agentlightning/verl/daemon.py</code> <pre><code>def get_right_padded_ids_and_attention_mask(\n    ids: List[int], max_length: int, pad_token_id: int\n) -&gt; Tuple[List[int], List[int]]:\n    \"\"\"\n    Right-pad (or truncate) a sequence of token IDs to a fixed length,\n    and build the corresponding attention mask.\n\n    Args:\n        ids:            the original list of token IDs.\n        max_length:     desired total length after padding/truncation.\n        pad_token_id:   ID to use for padding.\n\n    Returns:\n        padded_ids (any):     list of length == max_length.\n        attention_mask (any): list of same length: 1 for non-pad tokens, 0 for pads.\n    \"\"\"\n    seq_len = len(ids)\n\n    if seq_len &gt;= max_length:\n        # too long \u2192 truncate to the first max_length tokens\n        trimmed = ids[:max_length]\n        attention_mask = [1] * max_length\n        return trimmed, attention_mask\n\n    # too short \u2192 pad on the right\n    pad_len = max_length - seq_len\n    padded_ids = ids + [pad_token_id] * pad_len\n    attention_mask = [1] * seq_len + [0] * pad_len\n    return padded_ids, attention_mask\n</code></pre>"}]}